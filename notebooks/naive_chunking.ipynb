{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T16:08:51.441472Z",
     "start_time": "2025-01-02T16:08:45.678240Z"
    }
   },
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from nlp_chat_bot.model.minilm import MiniLM\n",
    "from nlp_chat_bot.rag import RAG\n",
    "from nlp_chat_bot.vector_store.chroma_vector_store_builder import ChromaVectorStoreBuilder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T16:08:51.457097Z",
     "start_time": "2025-01-02T16:08:51.441472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T16:08:56.509480Z",
     "start_time": "2025-01-02T16:08:51.850965Z"
    }
   },
   "source": [
    "dataset_path = \"../data\"\n",
    "vector_store_path = \"../chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "rag = RAG(dataset_path, embedding_function, vector_store_path, splitter, llm=llm_gemini)\n",
    "print(\"LENGTH\", rag.get_num_docs())\n",
    "docs_retrieved = rag.retrieve(state = {\"question\": \"What is my conclusion in my project report on image inpainting?\", \"context\": []})\n",
    "\n",
    "print(\"Num docs:\", len(docs_retrieved[\"context\"]))\n",
    "\n",
    "for i in range(len(docs_retrieved[\"context\"])):\n",
    "    doc = docs_retrieved[\"context\"][i]\n",
    "    print(\"\\n\\n\", \"#\"*30,\"\\n\")\n",
    "    print(f\"doc {i}: (score: {doc.metadata['score']})\")\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.79it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and storing 35 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 87.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH 468\n",
      "Num docs: 3\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 0: (score: 0.9039752809391811)\n",
      "Image Inpainting with Basic Convolutional Networks\n",
      "Robin Meneust, Ethan Pinto\n",
      "December 2024\n",
      "1 Introduction\n",
      "In the context of our ”AI-Based Image Processing”\n",
      "course, we worked on this project, in which we repro-\n",
      "duced and tested a specific image inpainting approach,\n",
      "defined by the paper ”Context Encoders: Feature Learn-\n",
      "ing by Inpainting”(Pathak et al., 2016)[1].\n",
      "Image inpainting consists of filling hole(s) in an im-\n",
      "age. There exist different methods to do so (e.g. they\n",
      "compared their results with Photoshop). In this paper,\n",
      "they used a context encoder trained in an adversarial\n",
      "way. Basically there is a generator, this is our context\n",
      "encoder (here an encoder and a decoder) that given an\n",
      "image of size 128x128 with a dropout region (a ”hole”,\n",
      "with values set to 0) tries to predict what should be inside\n",
      "the hole. We focused on the simplest case here for the\n",
      "dropout region: a square in the center of size 64x64 (i.e.\n",
      "half of the image). This is a large section of the image,\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 1: (score: 1.0136410144681987)\n",
      "realistic, not just the missing regions”. So it will be\n",
      "sharper than with the reconstruction loss, as we can see\n",
      "in their paper in the figure 1. It’s basically BCE (Bi-\n",
      "nary Cross Entropy) loss. Note here that the encoder\n",
      "only considers the masked region (dropout) and not the\n",
      "whole image. This isn’t really clear in the loss section of\n",
      "the paper, but it is on their model architecture image,\n",
      "here in figure 2, since the discriminator input is 64x64\n",
      "while the full image is 128x128.\n",
      "In this report, we will first explain how we imple-\n",
      "mented the model as it’s defined in the paper in section 2.\n",
      "1https://github.com/Hanabi-TheFox/\n",
      "Image-Inpainting-with-Basic-Convolutional-Networks\n",
      "Figure 1: Comparison of Inpainting Results Using Rec,\n",
      "Adv or Joint Loss (Results From the Paper)\n",
      "Then we will describe our experiments setup (datasets,\n",
      "hyper-parameters...)3. Finally, we will present and in-\n",
      "terpret our results for the tests that we conducted in the\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 2: (score: 1.310561670975423)\n",
      "is missing. However, it’s too blurry, so we tried increas-\n",
      "ing the weight of the adversarial loss. The results are in\n",
      "figure 4. Here there is almost no blur and the results are\n",
      "very similar to the ones in the paper, even though it’s\n",
      "not perfect.\n",
      "The two others are on Tiny ImageNet. We tried set-\n",
      "ting the same weight for adversarial and reconstruction\n",
      "Figure 3: Experiment on ImageNet 128x128 with Stan-\n",
      "dard Parameters (×1000 Ratio)\n",
      "Figure 4: Experiment on ImageNet 128x128 with ×200\n",
      "Ratio\n",
      "loss for once in figure 5. The results were to be expected,\n",
      "and are aligned with the original paper results (on adver-\n",
      "sarial loss only). The context doesn’t seem to be taken\n",
      "into account, the results are not blurry but are totally\n",
      "off compared to what we want. Note that compared to\n",
      "ImageNet-1k, we obtained quite good results with the\n",
      "parameters of the paper in figure 6.\n",
      "No matter the experiment, we always observed some\n",
      "errors on especially difficult images. That is for example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T16:08:57.906748Z",
     "start_time": "2025-01-02T16:08:56.640295Z"
    }
   },
   "cell_type": "code",
   "source": "rag.invoke(query={\"question\":\"What is my conclusion in my project report on image inpainting?\"})",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my conclusion in my project report on image inpainting?',\n",
       " 'context': (Document(metadata={'score': 0.9039752809391811}, page_content='Image Inpainting with Basic Convolutional Networks\\nRobin Meneust, Ethan Pinto\\nDecember 2024\\n1 Introduction\\nIn the context of our ”AI-Based Image Processing”\\ncourse, we worked on this project, in which we repro-\\nduced and tested a specific image inpainting approach,\\ndefined by the paper ”Context Encoders: Feature Learn-\\ning by Inpainting”(Pathak et al., 2016)[1].\\nImage inpainting consists of filling hole(s) in an im-\\nage. There exist different methods to do so (e.g. they\\ncompared their results with Photoshop). In this paper,\\nthey used a context encoder trained in an adversarial\\nway. Basically there is a generator, this is our context\\nencoder (here an encoder and a decoder) that given an\\nimage of size 128x128 with a dropout region (a ”hole”,\\nwith values set to 0) tries to predict what should be inside\\nthe hole. We focused on the simplest case here for the\\ndropout region: a square in the center of size 64x64 (i.e.\\nhalf of the image). This is a large section of the image,'),\n",
       "  Document(metadata={'score': 1.0136410144681987}, page_content='realistic, not just the missing regions”. So it will be\\nsharper than with the reconstruction loss, as we can see\\nin their paper in the figure 1. It’s basically BCE (Bi-\\nnary Cross Entropy) loss. Note here that the encoder\\nonly considers the masked region (dropout) and not the\\nwhole image. This isn’t really clear in the loss section of\\nthe paper, but it is on their model architecture image,\\nhere in figure 2, since the discriminator input is 64x64\\nwhile the full image is 128x128.\\nIn this report, we will first explain how we imple-\\nmented the model as it’s defined in the paper in section 2.\\n1https://github.com/Hanabi-TheFox/\\nImage-Inpainting-with-Basic-Convolutional-Networks\\nFigure 1: Comparison of Inpainting Results Using Rec,\\nAdv or Joint Loss (Results From the Paper)\\nThen we will describe our experiments setup (datasets,\\nhyper-parameters...)3. Finally, we will present and in-\\nterpret our results for the tests that we conducted in the'),\n",
       "  Document(metadata={'score': 1.310561670975423}, page_content='is missing. However, it’s too blurry, so we tried increas-\\ning the weight of the adversarial loss. The results are in\\nfigure 4. Here there is almost no blur and the results are\\nvery similar to the ones in the paper, even though it’s\\nnot perfect.\\nThe two others are on Tiny ImageNet. We tried set-\\nting the same weight for adversarial and reconstruction\\nFigure 3: Experiment on ImageNet 128x128 with Stan-\\ndard Parameters (×1000 Ratio)\\nFigure 4: Experiment on ImageNet 128x128 with ×200\\nRatio\\nloss for once in figure 5. The results were to be expected,\\nand are aligned with the original paper results (on adver-\\nsarial loss only). The context doesn’t seem to be taken\\ninto account, the results are not blurry but are totally\\noff compared to what we want. Note that compared to\\nImageNet-1k, we obtained quite good results with the\\nparameters of the paper in figure 6.\\nNo matter the experiment, we always observed some\\nerrors on especially difficult images. That is for example')),\n",
       " 'answer': 'The provided text describes the implementation and experimental results of an image inpainting model.  The experiments involved varying the weights of adversarial and reconstruction losses, and testing on different datasets.  The conclusion is not explicitly stated in this excerpt.\\n'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
