{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook used to test our RAG chatbot package on the `rag-mini-bioasq` dataset\n",
    "\n",
    "## Setup and dataset download"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:51.352203Z",
     "start_time": "2025-01-13T12:41:45.589182Z"
    }
   },
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.doc_loader.test_data_csv_loader import TestDataCSVLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.model.embedding.minilm import MiniLM\n",
    "from nlp_chat_bot.model.llm.gemma import Gemma\n",
    "from nlp_chat_bot.model.llm.mistral import Mistral\n",
    "from nlp_chat_bot.rag.classic_rag import ClassicRAG\n",
    "from nlp_chat_bot.rag.query_translation_rag_decomposition import QueryTranslationRAGDecomposition\n",
    "from nlp_chat_bot.rag.query_translation_rag_fusion import QueryTranslationRAGFusion\n",
    "from nlp_chat_bot.vector_store.late_chunking_chroma_vector_store_builder import LateChunkingChromaVectorStoreBuilder\n",
    "from nlp_chat_bot.vector_store.naive_chunking_chroma_vector_store_builder import NaiveChunkingChromaVectorStoreBuilder\n",
    "from datasets import load_dataset, tqdm\n",
    "from nlp_chat_bot.model.embedding.late_chunking_embedding import LateChunkingEmbedding\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:56.166574Z",
     "start_time": "2025-01-13T12:41:51.355210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_qa = load_dataset(\"enelpol/rag-mini-bioasq\", \"question-answer-passages\")[\"test\"]\n",
    "ds_corpus = load_dataset(\"enelpol/rag-mini-bioasq\", \"text-corpus\")[\"test\"]\n",
    "\n",
    "# only keep ds_qa rows with 3 items or more because we use 3 by default in our case\n",
    "ds_qa = ds_qa.filter(lambda x: len(x[\"relevant_passage_ids\"]) >= 3)\n",
    "\n",
    "# we remove extra \\n from passages \n",
    "ds_corpus = ds_corpus.map(lambda row: {\"passage\": row[\"passage\"].replace(\"\\n\", \" \")})\n",
    "\n",
    "ds_qa.to_csv(\"../test_datasets/rag-mini-bioasq/qa/qa.csv\")\n",
    "ds_corpus.to_csv(\"../test_datasets/rag-mini-bioasq/corpus/corpus.csv\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 43.45ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 41/41 [00:00<00:00, 50.85ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60209211"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:56.407077Z",
     "start_time": "2025-01-13T12:41:56.393702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Datasets sizes:\")\n",
    "print(f\"ds_qa: {len(ds_qa)}\")\n",
    "print(f\"ds_corpus: {len(ds_corpus)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets sizes:\n",
      "ds_qa: 497\n",
      "ds_corpus: 40181\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:56.966812Z",
     "start_time": "2025-01-13T12:41:56.419905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(ds_corpus)\n",
    "df.head(1)\n",
    "\n",
    "df = pd.DataFrame(ds_qa)\n",
    "df.head(1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          question  \\\n",
       "0  Describe the mechanism of action of ibalizumab.   \n",
       "\n",
       "                                              answer    id  \\\n",
       "0  Ibalizumab is a humanized monoclonal antibody ...  2835   \n",
       "\n",
       "                                relevant_passage_ids  \n",
       "0  [29675744, 24853313, 29689540, 21289125, 20698...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>relevant_passage_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Describe the mechanism of action of ibalizumab.</td>\n",
       "      <td>Ibalizumab is a humanized monoclonal antibody ...</td>\n",
       "      <td>2835</td>\n",
       "      <td>[29675744, 24853313, 29689540, 21289125, 20698...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:57.020836Z",
     "start_time": "2025-01-13T12:41:57.006908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test if our Llama installation supports GPU\n",
    "\n",
    "# import os\n",
    "# from llama_cpp.llama_cpp import load_shared_library\n",
    "# import llama_cpp\n",
    "# \n",
    "# llama_root_path_module = os.path.dirname(llama_cpp.__file__)\n",
    "# import pathlib\n",
    "# \n",
    "# def is_gpu_available_v3() -> bool:\n",
    "# \n",
    "#     lib = load_shared_library('llama',pathlib.Path(llama_root_path_module+'/lib'))\n",
    "#     return bool(lib.llama_supports_gpu_offload())\n",
    "# \n",
    "# print(is_gpu_available_v3())"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:57.058952Z",
     "start_time": "2025-01-13T12:41:57.052829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_retrieval(ds_qa, retrieve_function):\n",
    "    total_num_documents_considered = 0\n",
    "    num_valid_docs = 0\n",
    "    for test_item in tqdm(ds_qa):\n",
    "        question = test_item[\"question\"]\n",
    "        expected_documents_ids = test_item[\"relevant_passage_ids\"]\n",
    "        \n",
    "        response = retrieve_function(state = {\"question\": question, \"context\": []})\n",
    "        docs_retrieved = response[\"context\"]        \n",
    "        \n",
    "        # print(\"Question:\",question)\n",
    "        # print(\"Retrieved:\",docs_retrieved)\n",
    "        # print(\"#########################\\n\\n\")\n",
    "        # if it's a dict of docs (e.g. with QueryTranslationRAGDecomposition)\n",
    "        if isinstance(docs_retrieved, dict):\n",
    "            num_docs_retrieved = 0\n",
    "            for _, docs in docs_retrieved.items():\n",
    "                num_docs_retrieved += len(docs)\n",
    "                for doc in docs:\n",
    "                    if int(doc.metadata[\"id\"]) in expected_documents_ids:\n",
    "                        num_valid_docs += 1\n",
    "            total_num_documents_considered += min(len(expected_documents_ids), num_docs_retrieved)\n",
    "        else:\n",
    "            num_documents_considered = min(len(expected_documents_ids), len(docs_retrieved))\n",
    "            total_num_documents_considered += num_documents_considered\n",
    "            # print(\"Expected:\",expected_documents_ids,\"Got:\",[doc.metadata[\"id\"] for doc in docs_retrieved])\n",
    "            # print(\"Expected:\",expected_documents_ids)\n",
    "            for doc in docs_retrieved:\n",
    "                # print(\"Got:\",doc.metadata[\"id\"])\n",
    "                if int(doc.metadata[\"id\"]) in expected_documents_ids:\n",
    "                    num_valid_docs += 1\n",
    "        \n",
    "        \n",
    "    return num_valid_docs / total_num_documents_considered"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tests"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:57.072369Z",
     "start_time": "2025-01-13T12:41:57.062018Z"
    }
   },
   "cell_type": "code",
   "source": "scores = {}",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:41:58.005742Z",
     "start_time": "2025-01-13T12:41:57.088627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_path = \"../test_datasets/rag-mini-bioasq/corpus\"\n",
    "vector_store_path = \"../test_chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "reload_vector_store = True # Add non existing documents\n",
    "reset_vector_store = False # Remove previous documents\n",
    "\n",
    "# splitter = None\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    ")\n",
    "\n",
    "llm = Gemma(model_download_path=model_download_path)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\n",
      "llama_model_load_from_file: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11247 MiB free\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from ../models\\gemma-2-2b-it-GGUF\\models--MaziyarPanahi--gemma-2-2b-it-GGUF\\snapshots\\bd652eddf75b473fe86639b6b927e06972277d1a\\gemma-2-2b-it.IQ1_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models Google Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = models-google-gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 31\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = ./gemma-2-2b-it-GGUF_imatrix.dat\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 65\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q2_K:   29 tensors\n",
      "llama_model_loader: - type q5_K:    1 tensors\n",
      "llama_model_loader: - type iq2_xxs:   26 tensors\n",
      "llama_model_loader: - type iq1_m:  127 tensors\n",
      "llm_load_vocab: control token:      1 '<eos>' is not marked as EOG\n",
      "llm_load_vocab: control token:      0 '<pad>' is not marked as EOG\n",
      "llm_load_vocab: control token:     47 '<unused40>' is not marked as EOG\n",
      "llm_load_vocab: control token:      3 '<unk>' is not marked as EOG\n",
      "llm_load_vocab: control token:     55 '<unused48>' is not marked as EOG\n",
      "llm_load_vocab: control token:     24 '<unused17>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '<bos>' is not marked as EOG\n",
      "llm_load_vocab: control token:      5 '<2mass>' is not marked as EOG\n",
      "llm_load_vocab: control token:     58 '<unused51>' is not marked as EOG\n",
      "llm_load_vocab: control token:      4 '<mask>' is not marked as EOG\n",
      "llm_load_vocab: control token:     42 '<unused35>' is not marked as EOG\n",
      "llm_load_vocab: control token:      9 '<unused2>' is not marked as EOG\n",
      "llm_load_vocab: control token:      6 '[@BOS@]' is not marked as EOG\n",
      "llm_load_vocab: control token:     44 '<unused37>' is not marked as EOG\n",
      "llm_load_vocab: control token:     35 '<unused28>' is not marked as EOG\n",
      "llm_load_vocab: control token:      7 '<unused0>' is not marked as EOG\n",
      "llm_load_vocab: control token:     43 '<unused36>' is not marked as EOG\n",
      "llm_load_vocab: control token:     36 '<unused29>' is not marked as EOG\n",
      "llm_load_vocab: control token:      8 '<unused1>' is not marked as EOG\n",
      "llm_load_vocab: control token:     41 '<unused34>' is not marked as EOG\n",
      "llm_load_vocab: control token:     10 '<unused3>' is not marked as EOG\n",
      "llm_load_vocab: control token:     40 '<unused33>' is not marked as EOG\n",
      "llm_load_vocab: control token:     11 '<unused4>' is not marked as EOG\n",
      "llm_load_vocab: control token:     39 '<unused32>' is not marked as EOG\n",
      "llm_load_vocab: control token:     12 '<unused5>' is not marked as EOG\n",
      "llm_load_vocab: control token:     38 '<unused31>' is not marked as EOG\n",
      "llm_load_vocab: control token:     13 '<unused6>' is not marked as EOG\n",
      "llm_load_vocab: control token:     37 '<unused30>' is not marked as EOG\n",
      "llm_load_vocab: control token:     14 '<unused7>' is not marked as EOG\n",
      "llm_load_vocab: control token:     27 '<unused20>' is not marked as EOG\n",
      "llm_load_vocab: control token:     15 '<unused8>' is not marked as EOG\n",
      "llm_load_vocab: control token:     28 '<unused21>' is not marked as EOG\n",
      "llm_load_vocab: control token:     16 '<unused9>' is not marked as EOG\n",
      "llm_load_vocab: control token:     17 '<unused10>' is not marked as EOG\n",
      "llm_load_vocab: control token:     18 '<unused11>' is not marked as EOG\n",
      "llm_load_vocab: control token:     19 '<unused12>' is not marked as EOG\n",
      "llm_load_vocab: control token:     20 '<unused13>' is not marked as EOG\n",
      "llm_load_vocab: control token:     21 '<unused14>' is not marked as EOG\n",
      "llm_load_vocab: control token:     22 '<unused15>' is not marked as EOG\n",
      "llm_load_vocab: control token:     56 '<unused49>' is not marked as EOG\n",
      "llm_load_vocab: control token:     23 '<unused16>' is not marked as EOG\n",
      "llm_load_vocab: control token:     54 '<unused47>' is not marked as EOG\n",
      "llm_load_vocab: control token:     25 '<unused18>' is not marked as EOG\n",
      "llm_load_vocab: control token:     53 '<unused46>' is not marked as EOG\n",
      "llm_load_vocab: control token:     26 '<unused19>' is not marked as EOG\n",
      "llm_load_vocab: control token:     29 '<unused22>' is not marked as EOG\n",
      "llm_load_vocab: control token:     30 '<unused23>' is not marked as EOG\n",
      "llm_load_vocab: control token:     31 '<unused24>' is not marked as EOG\n",
      "llm_load_vocab: control token:     32 '<unused25>' is not marked as EOG\n",
      "llm_load_vocab: control token:     46 '<unused39>' is not marked as EOG\n",
      "llm_load_vocab: control token:     33 '<unused26>' is not marked as EOG\n",
      "llm_load_vocab: control token:     45 '<unused38>' is not marked as EOG\n",
      "llm_load_vocab: control token:     34 '<unused27>' is not marked as EOG\n",
      "llm_load_vocab: control token:     48 '<unused41>' is not marked as EOG\n",
      "llm_load_vocab: control token:     49 '<unused42>' is not marked as EOG\n",
      "llm_load_vocab: control token:     50 '<unused43>' is not marked as EOG\n",
      "llm_load_vocab: control token:     51 '<unused44>' is not marked as EOG\n",
      "llm_load_vocab: control token:     52 '<unused45>' is not marked as EOG\n",
      "llm_load_vocab: control token:     57 '<unused50>' is not marked as EOG\n",
      "llm_load_vocab: control token:     59 '<unused52>' is not marked as EOG\n",
      "llm_load_vocab: control token:     60 '<unused53>' is not marked as EOG\n",
      "llm_load_vocab: control token:     61 '<unused54>' is not marked as EOG\n",
      "llm_load_vocab: control token:     62 '<unused55>' is not marked as EOG\n",
      "llm_load_vocab: control token:     63 '<unused56>' is not marked as EOG\n",
      "llm_load_vocab: control token:     64 '<unused57>' is not marked as EOG\n",
      "llm_load_vocab: control token:     65 '<unused58>' is not marked as EOG\n",
      "llm_load_vocab: control token:     66 '<unused59>' is not marked as EOG\n",
      "llm_load_vocab: control token:     67 '<unused60>' is not marked as EOG\n",
      "llm_load_vocab: control token:     68 '<unused61>' is not marked as EOG\n",
      "llm_load_vocab: control token:     69 '<unused62>' is not marked as EOG\n",
      "llm_load_vocab: control token:     70 '<unused63>' is not marked as EOG\n",
      "llm_load_vocab: control token:     71 '<unused64>' is not marked as EOG\n",
      "llm_load_vocab: control token:     72 '<unused65>' is not marked as EOG\n",
      "llm_load_vocab: control token:     73 '<unused66>' is not marked as EOG\n",
      "llm_load_vocab: control token:     74 '<unused67>' is not marked as EOG\n",
      "llm_load_vocab: control token:     75 '<unused68>' is not marked as EOG\n",
      "llm_load_vocab: control token:     76 '<unused69>' is not marked as EOG\n",
      "llm_load_vocab: control token:     77 '<unused70>' is not marked as EOG\n",
      "llm_load_vocab: control token:     78 '<unused71>' is not marked as EOG\n",
      "llm_load_vocab: control token:     79 '<unused72>' is not marked as EOG\n",
      "llm_load_vocab: control token:     80 '<unused73>' is not marked as EOG\n",
      "llm_load_vocab: control token:     81 '<unused74>' is not marked as EOG\n",
      "llm_load_vocab: control token:     82 '<unused75>' is not marked as EOG\n",
      "llm_load_vocab: control token:     83 '<unused76>' is not marked as EOG\n",
      "llm_load_vocab: control token:     84 '<unused77>' is not marked as EOG\n",
      "llm_load_vocab: control token:     85 '<unused78>' is not marked as EOG\n",
      "llm_load_vocab: control token:     86 '<unused79>' is not marked as EOG\n",
      "llm_load_vocab: control token:     87 '<unused80>' is not marked as EOG\n",
      "llm_load_vocab: control token:     88 '<unused81>' is not marked as EOG\n",
      "llm_load_vocab: control token:     89 '<unused82>' is not marked as EOG\n",
      "llm_load_vocab: control token:     90 '<unused83>' is not marked as EOG\n",
      "llm_load_vocab: control token:     91 '<unused84>' is not marked as EOG\n",
      "llm_load_vocab: control token:     92 '<unused85>' is not marked as EOG\n",
      "llm_load_vocab: control token:     93 '<unused86>' is not marked as EOG\n",
      "llm_load_vocab: control token:     94 '<unused87>' is not marked as EOG\n",
      "llm_load_vocab: control token:     95 '<unused88>' is not marked as EOG\n",
      "llm_load_vocab: control token:     96 '<unused89>' is not marked as EOG\n",
      "llm_load_vocab: control token:     97 '<unused90>' is not marked as EOG\n",
      "llm_load_vocab: control token:     98 '<unused91>' is not marked as EOG\n",
      "llm_load_vocab: control token:     99 '<unused92>' is not marked as EOG\n",
      "llm_load_vocab: control token:    100 '<unused93>' is not marked as EOG\n",
      "llm_load_vocab: control token:    101 '<unused94>' is not marked as EOG\n",
      "llm_load_vocab: control token:    102 '<unused95>' is not marked as EOG\n",
      "llm_load_vocab: control token:    103 '<unused96>' is not marked as EOG\n",
      "llm_load_vocab: control token:    104 '<unused97>' is not marked as EOG\n",
      "llm_load_vocab: control token:    105 '<unused98>' is not marked as EOG\n",
      "llm_load_vocab: control token:    106 '<start_of_turn>' is not marked as EOG\n",
      "llm_load_vocab: control token: 255999 '<unused99>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 249\n",
      "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = gemma2\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 256000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 2304\n",
      "llm_load_print_meta: n_layer          = 26\n",
      "llm_load_print_meta: n_head           = 8\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 256\n",
      "llm_load_print_meta: n_swa            = 4096\n",
      "llm_load_print_meta: n_embd_head_k    = 256\n",
      "llm_load_print_meta: n_embd_head_v    = 256\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 9216\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 2B\n",
      "llm_load_print_meta: model ftype      = IQ1_M - 1.75 bpw\n",
      "llm_load_print_meta: model params     = 2.61 B\n",
      "llm_load_print_meta: model size       = 827.55 MiB (2.66 BPW) \n",
      "llm_load_print_meta: general.name     = Models Google Gemma 2 2b It\n",
      "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
      "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
      "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
      "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 26 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 27/27 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =   827.57 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   386.72 MiB\n",
      ".......................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 8192\n",
      "llama_new_context_with_model: n_ctx_per_seq = 8192\n",
      "llama_new_context_with_model: n_batch       = 32\n",
      "llama_new_context_with_model: n_ubatch      = 8\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 26, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   832.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  832.00 MiB, K (f16):  416.00 MiB, V (f16):  416.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =     7.88 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     2.07 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1050\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Models Google Gemma 2 2b It', 'general.architecture': 'gemma2', 'general.type': 'model', 'general.basename': 'models-google-gemma-2', 'general.finetune': 'it', 'general.size_label': '2B', 'gemma2.context_length': '8192', 'general.license': 'gemma', 'gemma2.embedding_length': '2304', 'gemma2.block_count': '26', 'gemma2.feed_forward_length': '9216', 'gemma2.attention.head_count': '8', 'gemma2.attention.head_count_kv': '4', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attn_logit_softcapping': '50.000000', 'general.file_type': '31', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '65', 'quantize.imatrix.file': './gemma-2-2b-it-GGUF_imatrix.dat', 'quantize.imatrix.dataset': 'group_40.txt', 'quantize.imatrix.entries_count': '182'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MiniLM, Naive Chunking"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:42:12.424406Z",
     "start_time": "2025-01-13T12:41:58.013498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Storing documents embeddings (batch size is 1000): 0it [00:00, ?it/s]\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 497/497 [00:03<00:00, 154.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5030181086519114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Jina, Late Chunking\n",
    "\n",
    "Here we should note that we use Late Chunking on small documents, which is not the best use case for Late Chunking, so we can't really evaluate the performance of Late Chunking here."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:49:39.851020Z",
     "start_time": "2025-01-13T12:42:12.436016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = LateChunkingEmbedding(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = LateChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm=llm)\n",
    "\n",
    "scores[\"Jina-LateChunking-Gemma\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"Jina-LateChunking-Gemma\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Storing 40182 documents embeddings (batch size is 100): 40200it [07:07, 94.14it/s]                            \n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 497/497 [00:03<00:00, 150.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3983903420523139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Decomposition\n",
    "\n",
    "The score is worse here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:14:09.516117Z",
     "start_time": "2025-01-13T12:49:39.975725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGDecomposition(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma-Decomposition\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma-Decomposition\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Storing documents embeddings (batch size is 1000): 0it [00:00, ?it/s]\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/497 [00:00<?, ?it/s]C:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\model\\llm\\gemma.py:31: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return AIMessage(self._llm(prompt.messages[0].content))\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3762.43 ms /   393 tokens\n",
      "  0%|          | 1/497 [00:04<34:12,  4.14s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2871.17 ms /   319 tokens\n",
      "  0%|          | 2/497 [00:07<28:28,  3.45s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2808.09 ms /   307 tokens\n",
      "  1%|          | 3/497 [00:09<26:11,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2812.18 ms /   309 tokens\n",
      "  1%|          | 4/497 [00:12<25:06,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2908.17 ms /   311 tokens\n",
      "  1%|          | 5/497 [00:15<24:44,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2907.30 ms /   310 tokens\n",
      "  1%|          | 6/497 [00:18<24:35,  3.00s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2918.68 ms /   312 tokens\n",
      "  1%|▏         | 7/497 [00:21<24:25,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.69 ms /   310 tokens\n",
      "  2%|▏         | 8/497 [00:24<24:12,  2.97s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2840.74 ms /   311 tokens\n",
      "  2%|▏         | 9/497 [00:27<23:56,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2813.59 ms /   309 tokens\n",
      "  2%|▏         | 10/497 [00:30<23:40,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2916.08 ms /   311 tokens\n",
      "  2%|▏         | 11/497 [00:33<23:44,  2.93s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2809.61 ms /   307 tokens\n",
      "  2%|▏         | 12/497 [00:36<23:29,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2864.87 ms /   311 tokens\n",
      "  3%|▎         | 13/497 [00:39<23:25,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2819.92 ms /   305 tokens\n",
      "  3%|▎         | 14/497 [00:41<23:14,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2174.74 ms /   235 tokens\n",
      "  3%|▎         | 15/497 [00:44<21:33,  2.68s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.85 ms /   309 tokens\n",
      "  3%|▎         | 16/497 [00:47<22:08,  2.76s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.57 ms /   309 tokens\n",
      "  3%|▎         | 17/497 [00:50<22:25,  2.80s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2911.77 ms /   318 tokens\n",
      "  4%|▎         | 18/497 [00:52<22:42,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2856.64 ms /   314 tokens\n",
      "  4%|▍         | 19/497 [00:55<22:47,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2826.98 ms /   310 tokens\n",
      "  4%|▍         | 20/497 [00:58<22:45,  2.86s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.92 ms /   310 tokens\n",
      "  4%|▍         | 21/497 [01:01<22:45,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2836.52 ms /   306 tokens\n",
      "  4%|▍         | 22/497 [01:04<22:43,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2874.41 ms /   323 tokens\n",
      "  5%|▍         | 23/497 [01:07<22:46,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2843.01 ms /   316 tokens\n",
      "  5%|▍         | 24/497 [01:10<22:44,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2902.91 ms /   310 tokens\n",
      "  5%|▌         | 25/497 [01:13<22:49,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.64 ms /   312 tokens\n",
      "  5%|▌         | 26/497 [01:16<22:45,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2861.01 ms /   308 tokens\n",
      "  5%|▌         | 27/497 [01:19<22:42,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.20 ms /   313 tokens\n",
      "  6%|▌         | 28/497 [01:21<22:42,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.88 ms /   316 tokens\n",
      "  6%|▌         | 29/497 [01:24<23:02,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.87 ms /   312 tokens\n",
      "  6%|▌         | 30/497 [01:27<22:49,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2864.56 ms /   310 tokens\n",
      "  6%|▌         | 31/497 [01:30<22:40,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2875.30 ms /   315 tokens\n",
      "  6%|▋         | 32/497 [01:33<22:35,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2891.69 ms /   316 tokens\n",
      "  7%|▋         | 33/497 [01:36<22:33,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2441.23 ms /   268 tokens\n",
      "  7%|▋         | 34/497 [01:39<21:29,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2825.08 ms /   304 tokens\n",
      "  7%|▋         | 35/497 [01:41<21:36,  2.81s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.03 ms /   310 tokens\n",
      "  7%|▋         | 36/497 [01:44<21:44,  2.83s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2896.43 ms /   308 tokens\n",
      "  7%|▋         | 37/497 [01:47<21:54,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2820.70 ms /   309 tokens\n",
      "  8%|▊         | 38/497 [01:50<21:49,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.93 ms /   310 tokens\n",
      "  8%|▊         | 39/497 [01:53<21:51,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.62 ms /   315 tokens\n",
      "  8%|▊         | 40/497 [01:56<21:59,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2818.84 ms /   307 tokens\n",
      "  8%|▊         | 41/497 [01:59<21:50,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.33 ms /   309 tokens\n",
      "  8%|▊         | 42/497 [02:02<21:45,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2874.03 ms /   322 tokens\n",
      "  9%|▊         | 43/497 [02:05<21:47,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.15 ms /   309 tokens\n",
      "  9%|▉         | 44/497 [02:07<21:38,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.53 ms /   309 tokens\n",
      "  9%|▉         | 45/497 [02:10<21:31,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2739.26 ms /   302 tokens\n",
      "  9%|▉         | 46/497 [02:13<21:17,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     999.99 ms /   112 tokens\n",
      "  9%|▉         | 47/497 [02:14<17:12,  2.30s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2855.71 ms /   312 tokens\n",
      " 10%|▉         | 48/497 [02:17<18:28,  2.47s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2845.36 ms /   315 tokens\n",
      " 10%|▉         | 49/497 [02:20<19:22,  2.59s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2849.97 ms /   305 tokens\n",
      " 10%|█         | 50/497 [02:23<19:57,  2.68s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2847.71 ms /   305 tokens\n",
      " 10%|█         | 51/497 [02:26<20:22,  2.74s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2956.80 ms /   314 tokens\n",
      " 10%|█         | 52/497 [02:29<20:52,  2.81s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2919.81 ms /   310 tokens\n",
      " 11%|█         | 53/497 [02:31<21:07,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2842.19 ms /   315 tokens\n",
      " 11%|█         | 54/497 [02:34<21:07,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.77 ms /   316 tokens\n",
      " 11%|█         | 55/497 [02:37<21:08,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2846.12 ms /   306 tokens\n",
      " 11%|█▏        | 56/497 [02:40<21:08,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2900.56 ms /   313 tokens\n",
      " 11%|█▏        | 57/497 [02:43<21:11,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.28 ms /   314 tokens\n",
      " 12%|█▏        | 58/497 [02:46<21:08,  2.89s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2853.78 ms /   313 tokens\n",
      " 12%|█▏        | 59/497 [02:49<21:04,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.36 ms /   305 tokens\n",
      " 12%|█▏        | 60/497 [02:52<20:57,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2853.56 ms /   315 tokens\n",
      " 12%|█▏        | 61/497 [02:55<20:56,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.98 ms /   307 tokens\n",
      " 12%|█▏        | 62/497 [02:57<20:53,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.56 ms /   312 tokens\n",
      " 13%|█▎        | 63/497 [03:00<20:48,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.24 ms /   308 tokens\n",
      " 13%|█▎        | 64/497 [03:03<20:46,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.14 ms /   314 tokens\n",
      " 13%|█▎        | 65/497 [03:06<20:45,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.92 ms /   304 tokens\n",
      " 13%|█▎        | 66/497 [03:09<20:38,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2834.30 ms /   310 tokens\n",
      " 13%|█▎        | 67/497 [03:12<20:35,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2827.54 ms /   311 tokens\n",
      " 14%|█▎        | 68/497 [03:15<20:30,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2805.56 ms /   305 tokens\n",
      " 14%|█▍        | 69/497 [03:17<20:23,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.22 ms /   308 tokens\n",
      " 14%|█▍        | 70/497 [03:20<20:27,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2862.24 ms /   311 tokens\n",
      " 14%|█▍        | 71/497 [03:23<20:26,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2822.92 ms /   313 tokens\n",
      " 14%|█▍        | 72/497 [03:26<20:19,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2829.86 ms /   310 tokens\n",
      " 15%|█▍        | 73/497 [03:29<20:16,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2827.96 ms /   310 tokens\n",
      " 15%|█▍        | 74/497 [03:32<20:14,  2.87s/it]Llama.generate: 84 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2838.07 ms /   305 tokens\n",
      " 15%|█▌        | 75/497 [03:35<20:11,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2940.21 ms /   319 tokens\n",
      " 15%|█▌        | 76/497 [03:38<20:20,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.01 ms /   315 tokens\n",
      " 15%|█▌        | 77/497 [03:41<20:17,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2872.57 ms /   316 tokens\n",
      " 16%|█▌        | 78/497 [03:44<20:15,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2872.49 ms /   307 tokens\n",
      " 16%|█▌        | 79/497 [03:46<20:12,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.25 ms /   309 tokens\n",
      " 16%|█▌        | 80/497 [03:49<20:07,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2809.34 ms /   305 tokens\n",
      " 16%|█▋        | 81/497 [03:52<19:56,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2861.30 ms /   316 tokens\n",
      " 16%|█▋        | 82/497 [03:55<19:57,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2826.01 ms /   313 tokens\n",
      " 17%|█▋        | 83/497 [03:58<19:51,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2818.51 ms /   308 tokens\n",
      " 17%|█▋        | 84/497 [04:01<19:45,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2813.28 ms /   312 tokens\n",
      " 17%|█▋        | 85/497 [04:04<19:38,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2825.43 ms /   312 tokens\n",
      " 17%|█▋        | 86/497 [04:06<19:34,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2875.53 ms /   319 tokens\n",
      " 18%|█▊        | 87/497 [04:09<19:38,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2855.16 ms /   309 tokens\n",
      " 18%|█▊        | 88/497 [04:12<19:37,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2860.28 ms /   307 tokens\n",
      " 18%|█▊        | 89/497 [04:15<19:37,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.59 ms /   307 tokens\n",
      " 18%|█▊        | 90/497 [04:18<19:31,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.84 ms /   306 tokens\n",
      " 18%|█▊        | 91/497 [04:21<19:25,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2953.44 ms /   311 tokens\n",
      " 19%|█▊        | 92/497 [04:24<19:37,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2869.52 ms /   315 tokens\n",
      " 19%|█▊        | 93/497 [04:27<19:34,  2.91s/it]Llama.generate: 85 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.78 ms /   306 tokens\n",
      " 19%|█▉        | 94/497 [04:30<19:23,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.62 ms /   313 tokens\n",
      " 19%|█▉        | 95/497 [04:32<19:18,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.84 ms /   308 tokens\n",
      " 19%|█▉        | 96/497 [04:35<19:13,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.71 ms /   315 tokens\n",
      " 20%|█▉        | 97/497 [04:38<19:15,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.35 ms /   317 tokens\n",
      " 20%|█▉        | 98/497 [04:41<19:11,  2.89s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.92 ms /   309 tokens\n",
      " 20%|█▉        | 99/497 [04:44<19:06,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.96 ms /   311 tokens\n",
      " 20%|██        | 100/497 [04:47<19:08,  2.89s/it]Llama.generate: 88 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2855.06 ms /   305 tokens\n",
      " 20%|██        | 101/497 [04:50<19:03,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2859.57 ms /   312 tokens\n",
      " 21%|██        | 102/497 [04:53<19:01,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2821.89 ms /   308 tokens\n",
      " 21%|██        | 103/497 [04:56<18:54,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2833.89 ms /   312 tokens\n",
      " 21%|██        | 104/497 [04:58<18:49,  2.87s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2831.60 ms /   310 tokens\n",
      " 21%|██        | 105/497 [05:01<18:45,  2.87s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2849.14 ms /   308 tokens\n",
      " 21%|██▏       | 106/497 [05:04<18:43,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.93 ms /   307 tokens\n",
      " 22%|██▏       | 107/497 [05:07<18:41,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2838.98 ms /   316 tokens\n",
      " 22%|██▏       | 108/497 [05:10<18:38,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2862.73 ms /   310 tokens\n",
      " 22%|██▏       | 109/497 [05:13<18:36,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.25 ms /   320 tokens\n",
      " 22%|██▏       | 110/497 [05:16<18:36,  2.88s/it]Llama.generate: 85 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.83 ms /   310 tokens\n",
      " 22%|██▏       | 111/497 [05:19<18:32,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2816.12 ms /   315 tokens\n",
      " 23%|██▎       | 112/497 [05:21<18:26,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.97 ms /   316 tokens\n",
      " 23%|██▎       | 113/497 [05:24<18:25,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2845.07 ms /   308 tokens\n",
      " 23%|██▎       | 114/497 [05:27<18:23,  2.88s/it]Llama.generate: 84 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.42 ms /   325 tokens\n",
      " 23%|██▎       | 115/497 [05:30<18:26,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2611.38 ms /   285 tokens\n",
      " 23%|██▎       | 116/497 [05:33<17:54,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2822.85 ms /   312 tokens\n",
      " 24%|██▎       | 117/497 [05:36<17:56,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2828.68 ms /   309 tokens\n",
      " 24%|██▎       | 118/497 [05:38<17:54,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2826.74 ms /   311 tokens\n",
      " 24%|██▍       | 119/497 [05:41<17:54,  2.84s/it]Llama.generate: 90 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.83 ms /   304 tokens\n",
      " 24%|██▍       | 120/497 [05:44<17:57,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2954.49 ms /   309 tokens\n",
      " 24%|██▍       | 121/497 [05:47<18:08,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.62 ms /   307 tokens\n",
      " 25%|██▍       | 122/497 [05:50<18:17,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2979.04 ms /   314 tokens\n",
      " 25%|██▍       | 123/497 [05:53<18:24,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2899.43 ms /   311 tokens\n",
      " 25%|██▍       | 124/497 [05:56<18:19,  2.95s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.28 ms /   311 tokens\n",
      " 25%|██▌       | 125/497 [05:59<18:04,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2454.65 ms /   268 tokens\n",
      " 25%|██▌       | 126/497 [06:01<17:13,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2836.79 ms /   309 tokens\n",
      " 26%|██▌       | 127/497 [06:04<17:20,  2.81s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2807.23 ms /   310 tokens\n",
      " 26%|██▌       | 128/497 [06:07<17:19,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2790.23 ms /   304 tokens\n",
      " 26%|██▌       | 129/497 [06:10<17:16,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1093.05 ms /   126 tokens\n",
      " 26%|██▌       | 130/497 [06:11<14:08,  2.31s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2829.14 ms /   319 tokens\n",
      " 26%|██▋       | 131/497 [06:14<15:07,  2.48s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1218.18 ms /   143 tokens\n",
      " 27%|██▋       | 132/497 [06:15<12:50,  2.11s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.13 ms /   308 tokens\n",
      " 27%|██▋       | 133/497 [06:18<14:10,  2.34s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.98 ms /   311 tokens\n",
      " 27%|██▋       | 134/497 [06:21<15:12,  2.51s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2893.71 ms /   311 tokens\n",
      " 27%|██▋       | 135/497 [06:24<15:56,  2.64s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.25 ms /   308 tokens\n",
      " 27%|██▋       | 136/497 [06:27<16:17,  2.71s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2804.93 ms /   307 tokens\n",
      " 28%|██▊       | 137/497 [06:30<16:26,  2.74s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2816.36 ms /   312 tokens\n",
      " 28%|██▊       | 138/497 [06:33<16:35,  2.77s/it]Llama.generate: 83 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2842.77 ms /   323 tokens\n",
      " 28%|██▊       | 139/497 [06:35<16:44,  2.81s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2807.38 ms /   309 tokens\n",
      " 28%|██▊       | 140/497 [06:38<16:45,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2822.29 ms /   311 tokens\n",
      " 28%|██▊       | 141/497 [06:41<16:44,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2809.64 ms /   308 tokens\n",
      " 29%|██▊       | 142/497 [06:44<16:43,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.23 ms /   326 tokens\n",
      " 29%|██▉       | 143/497 [06:47<16:48,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2820.98 ms /   310 tokens\n",
      " 29%|██▉       | 144/497 [06:50<16:46,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2804.22 ms /   308 tokens\n",
      " 29%|██▉       | 145/497 [06:53<16:42,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.95 ms /   317 tokens\n",
      " 29%|██▉       | 146/497 [06:55<16:40,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2808.17 ms /   308 tokens\n",
      " 30%|██▉       | 147/497 [06:58<16:36,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2812.75 ms /   309 tokens\n",
      " 30%|██▉       | 148/497 [07:01<16:32,  2.84s/it]Llama.generate: 84 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2834.79 ms /   318 tokens\n",
      " 30%|██▉       | 149/497 [07:04<16:32,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.50 ms /   311 tokens\n",
      " 30%|███       | 150/497 [07:07<16:34,  2.87s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.20 ms /   315 tokens\n",
      " 30%|███       | 151/497 [07:10<16:33,  2.87s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2838.69 ms /   311 tokens\n",
      " 31%|███       | 152/497 [07:13<16:32,  2.88s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2835.96 ms /   313 tokens\n",
      " 31%|███       | 153/497 [07:15<16:27,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    40 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2902.47 ms /   339 tokens\n",
      " 31%|███       | 154/497 [07:18<16:31,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2843.42 ms /   311 tokens\n",
      " 31%|███       | 155/497 [07:21<16:26,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.21 ms /   317 tokens\n",
      " 31%|███▏      | 156/497 [07:24<16:22,  2.88s/it]Llama.generate: 84 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2811.58 ms /   302 tokens\n",
      " 32%|███▏      | 157/497 [07:27<16:16,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2843.96 ms /   309 tokens\n",
      " 32%|███▏      | 158/497 [07:30<16:14,  2.87s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2940.15 ms /   309 tokens\n",
      " 32%|███▏      | 159/497 [07:33<16:22,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.04 ms /   310 tokens\n",
      " 32%|███▏      | 160/497 [07:36<16:20,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2864.89 ms /   315 tokens\n",
      " 32%|███▏      | 161/497 [07:39<16:16,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2849.12 ms /   312 tokens\n",
      " 33%|███▎      | 162/497 [07:42<16:10,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2885.03 ms /   318 tokens\n",
      " 33%|███▎      | 163/497 [07:44<16:10,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2838.76 ms /   308 tokens\n",
      " 33%|███▎      | 164/497 [07:47<16:03,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.19 ms /   308 tokens\n",
      " 33%|███▎      | 165/497 [07:50<16:00,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.34 ms /   311 tokens\n",
      " 33%|███▎      | 166/497 [07:53<16:04,  2.91s/it]Llama.generate: 85 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     711.95 ms /    79 tokens\n",
      " 34%|███▎      | 167/497 [07:54<12:27,  2.26s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2921.62 ms /   312 tokens\n",
      " 34%|███▍      | 168/497 [07:57<13:32,  2.47s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.10 ms /   318 tokens\n",
      " 34%|███▍      | 169/497 [08:00<14:18,  2.62s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2893.85 ms /   311 tokens\n",
      " 34%|███▍      | 170/497 [08:03<14:45,  2.71s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2934.38 ms /   308 tokens\n",
      " 34%|███▍      | 171/497 [08:06<15:08,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2887.24 ms /   309 tokens\n",
      " 35%|███▍      | 172/497 [08:09<15:18,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2862.43 ms /   306 tokens\n",
      " 35%|███▍      | 173/497 [08:12<15:22,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    48 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.31 ms /   347 tokens\n",
      " 35%|███▌      | 174/497 [08:15<15:38,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2945.46 ms /   311 tokens\n",
      " 35%|███▌      | 175/497 [08:18<15:41,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.86 ms /   312 tokens\n",
      " 35%|███▌      | 176/497 [08:20<15:36,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2833.06 ms /   305 tokens\n",
      " 36%|███▌      | 177/497 [08:23<15:29,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2814.66 ms /   304 tokens\n",
      " 36%|███▌      | 178/497 [08:26<15:20,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.65 ms /   304 tokens\n",
      " 36%|███▌      | 179/497 [08:29<15:19,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.81 ms /   308 tokens\n",
      " 36%|███▌      | 180/497 [08:32<15:16,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2882.31 ms /   309 tokens\n",
      " 36%|███▋      | 181/497 [08:35<15:16,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2869.78 ms /   310 tokens\n",
      " 37%|███▋      | 182/497 [08:38<15:13,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.23 ms /   311 tokens\n",
      " 37%|███▋      | 183/497 [08:41<15:13,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2882.98 ms /   315 tokens\n",
      " 37%|███▋      | 184/497 [08:44<15:09,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2864.00 ms /   305 tokens\n",
      " 37%|███▋      | 185/497 [08:47<15:06,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.91 ms /   309 tokens\n",
      " 37%|███▋      | 186/497 [08:49<15:08,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.24 ms /   306 tokens\n",
      " 38%|███▊      | 187/497 [08:52<14:59,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.01 ms /   310 tokens\n",
      " 38%|███▊      | 188/497 [08:55<15:05,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2914.45 ms /   309 tokens\n",
      " 38%|███▊      | 189/497 [08:58<15:03,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2897.60 ms /   313 tokens\n",
      " 38%|███▊      | 190/497 [09:01<15:00,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2887.98 ms /   308 tokens\n",
      " 38%|███▊      | 191/497 [09:04<14:56,  2.93s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2876.46 ms /   315 tokens\n",
      " 39%|███▊      | 192/497 [09:07<14:50,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.56 ms /   316 tokens\n",
      " 39%|███▉      | 193/497 [09:10<14:46,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2877.87 ms /   310 tokens\n",
      " 39%|███▉      | 194/497 [09:13<14:41,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2891.13 ms /   317 tokens\n",
      " 39%|███▉      | 195/497 [09:16<14:41,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2898.33 ms /   310 tokens\n",
      " 39%|███▉      | 196/497 [09:19<14:39,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.78 ms /   316 tokens\n",
      " 40%|███▉      | 197/497 [09:22<14:40,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     479.36 ms /    61 tokens\n",
      " 40%|███▉      | 198/497 [09:22<11:01,  2.21s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1963.77 ms /   216 tokens\n",
      " 40%|████      | 199/497 [09:24<10:39,  2.15s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2966.73 ms /   311 tokens\n",
      " 40%|████      | 200/497 [09:27<11:52,  2.40s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.56 ms /   314 tokens\n",
      " 40%|████      | 201/497 [09:30<12:36,  2.56s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2916.28 ms /   310 tokens\n",
      " 41%|████      | 202/497 [09:33<13:07,  2.67s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3015.42 ms /   312 tokens\n",
      " 41%|████      | 203/497 [09:36<13:39,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2986.79 ms /   305 tokens\n",
      " 41%|████      | 204/497 [09:39<13:57,  2.86s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2901.18 ms /   309 tokens\n",
      " 41%|████      | 205/497 [09:42<14:01,  2.88s/it]Llama.generate: 85 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.14 ms /   305 tokens\n",
      " 41%|████▏     | 206/497 [09:45<14:02,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2924.39 ms /   313 tokens\n",
      " 42%|████▏     | 207/497 [09:48<14:05,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.72 ms /   316 tokens\n",
      " 42%|████▏     | 208/497 [09:51<14:07,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.96 ms /   305 tokens\n",
      " 42%|████▏     | 209/497 [09:54<13:58,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2878.04 ms /   314 tokens\n",
      " 42%|████▏     | 210/497 [09:57<13:55,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2883.52 ms /   308 tokens\n",
      " 42%|████▏     | 211/497 [10:00<13:51,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2853.02 ms /   307 tokens\n",
      " 43%|████▎     | 212/497 [10:02<13:47,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2867.41 ms /   310 tokens\n",
      " 43%|████▎     | 213/497 [10:05<13:44,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.48 ms /   118 tokens\n",
      " 43%|████▎     | 214/497 [10:06<11:08,  2.36s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2976.01 ms /   314 tokens\n",
      " 43%|████▎     | 215/497 [10:09<12:00,  2.55s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2918.68 ms /   311 tokens\n",
      " 43%|████▎     | 216/497 [10:12<12:30,  2.67s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2874.28 ms /   315 tokens\n",
      " 44%|████▎     | 217/497 [10:15<12:47,  2.74s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2872.59 ms /   313 tokens\n",
      " 44%|████▍     | 218/497 [10:18<12:58,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2905.40 ms /   326 tokens\n",
      " 44%|████▍     | 219/497 [10:21<13:07,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2618.72 ms /   280 tokens\n",
      " 44%|████▍     | 220/497 [10:24<12:50,  2.78s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2870.62 ms /   308 tokens\n",
      " 44%|████▍     | 221/497 [10:27<12:57,  2.82s/it]Llama.generate: 84 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2904.26 ms /   317 tokens\n",
      " 45%|████▍     | 222/497 [10:30<13:03,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2884.51 ms /   312 tokens\n",
      " 45%|████▍     | 223/497 [10:33<13:06,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2832.37 ms /   310 tokens\n",
      " 45%|████▌     | 224/497 [10:35<13:02,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2889.76 ms /   310 tokens\n",
      " 45%|████▌     | 225/497 [10:38<13:04,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2930.69 ms /   316 tokens\n",
      " 45%|████▌     | 226/497 [10:41<13:07,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.45 ms /   308 tokens\n",
      " 46%|████▌     | 227/497 [10:44<13:05,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2847.78 ms /   308 tokens\n",
      " 46%|████▌     | 228/497 [10:47<12:59,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2878.31 ms /   310 tokens\n",
      " 46%|████▌     | 229/497 [10:50<12:57,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2832.56 ms /   311 tokens\n",
      " 46%|████▋     | 230/497 [10:53<12:52,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2858.83 ms /   316 tokens\n",
      " 46%|████▋     | 231/497 [10:56<12:50,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2904.74 ms /   306 tokens\n",
      " 47%|████▋     | 232/497 [10:59<12:51,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.84 ms /   314 tokens\n",
      " 47%|████▋     | 233/497 [11:02<12:45,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2847.32 ms /   307 tokens\n",
      " 47%|████▋     | 234/497 [11:04<12:41,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.68 ms /   312 tokens\n",
      " 47%|████▋     | 235/497 [11:07<12:36,  2.89s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2892.98 ms /   316 tokens\n",
      " 47%|████▋     | 236/497 [11:10<12:37,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2882.39 ms /   307 tokens\n",
      " 48%|████▊     | 237/497 [11:13<12:35,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2952.77 ms /   315 tokens\n",
      " 48%|████▊     | 238/497 [11:16<12:39,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2912.74 ms /   307 tokens\n",
      " 48%|████▊     | 239/497 [11:19<12:38,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2950.04 ms /   319 tokens\n",
      " 48%|████▊     | 240/497 [11:22<12:38,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2831.54 ms /   307 tokens\n",
      " 48%|████▊     | 241/497 [11:25<12:29,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2877.56 ms /   312 tokens\n",
      " 49%|████▊     | 242/497 [11:28<12:25,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2921.29 ms /   309 tokens\n",
      " 49%|████▉     | 243/497 [11:31<12:24,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2844.42 ms /   316 tokens\n",
      " 49%|████▉     | 244/497 [11:34<12:16,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1312.17 ms /   144 tokens\n",
      " 49%|████▉     | 245/497 [11:35<10:15,  2.44s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2870.88 ms /   309 tokens\n",
      " 49%|████▉     | 246/497 [11:38<10:47,  2.58s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2861.77 ms /   306 tokens\n",
      " 50%|████▉     | 247/497 [11:41<11:08,  2.67s/it]Llama.generate: 85 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2869.13 ms /   314 tokens\n",
      " 50%|████▉     | 248/497 [11:44<11:22,  2.74s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2872.34 ms /   307 tokens\n",
      " 50%|█████     | 249/497 [11:47<11:31,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2930.60 ms /   318 tokens\n",
      " 50%|█████     | 250/497 [11:50<11:41,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2817.56 ms /   304 tokens\n",
      " 51%|█████     | 251/497 [11:52<11:38,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.58 ms /   326 tokens\n",
      " 51%|█████     | 252/497 [11:55<11:45,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2920.28 ms /   316 tokens\n",
      " 51%|█████     | 253/497 [11:58<11:46,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2883.77 ms /   308 tokens\n",
      " 51%|█████     | 254/497 [12:01<11:45,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.10 ms /   310 tokens\n",
      " 51%|█████▏    | 255/497 [12:04<11:42,  2.90s/it]Llama.generate: 84 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2967.65 ms /   305 tokens\n",
      " 52%|█████▏    | 256/497 [12:07<11:46,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2882.68 ms /   307 tokens\n",
      " 52%|█████▏    | 257/497 [12:10<11:42,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2942.65 ms /   311 tokens\n",
      " 52%|█████▏    | 258/497 [12:13<11:42,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2880.17 ms /   310 tokens\n",
      " 52%|█████▏    | 259/497 [12:16<11:37,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1641.57 ms /   180 tokens\n",
      " 52%|█████▏    | 260/497 [12:18<10:05,  2.55s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.91 ms /   309 tokens\n",
      " 53%|█████▎    | 261/497 [12:21<10:31,  2.68s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.45 ms /   310 tokens\n",
      " 53%|█████▎    | 262/497 [12:24<10:58,  2.80s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1555.79 ms /   166 tokens\n",
      " 53%|█████▎    | 263/497 [12:25<09:28,  2.43s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.41 ms /   305 tokens\n",
      " 53%|█████▎    | 264/497 [12:28<09:58,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3060.79 ms /   311 tokens\n",
      " 53%|█████▎    | 265/497 [12:31<10:33,  2.73s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2937.50 ms /   308 tokens\n",
      " 54%|█████▎    | 266/497 [12:34<10:47,  2.80s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3003.70 ms /   314 tokens\n",
      " 54%|█████▎    | 267/497 [12:37<11:00,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.13 ms /   316 tokens\n",
      " 54%|█████▍    | 268/497 [12:40<11:01,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2987.61 ms /   308 tokens\n",
      " 54%|█████▍    | 269/497 [12:43<11:07,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.27 ms /   312 tokens\n",
      " 54%|█████▍    | 270/497 [12:46<11:12,  2.96s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2989.28 ms /   310 tokens\n",
      " 55%|█████▍    | 271/497 [12:49<11:13,  2.98s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2964.77 ms /   308 tokens\n",
      " 55%|█████▍    | 272/497 [12:52<11:12,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2936.69 ms /   307 tokens\n",
      " 55%|█████▍    | 273/497 [12:55<11:08,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2850.96 ms /   311 tokens\n",
      " 55%|█████▌    | 274/497 [12:58<10:58,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2903.53 ms /   313 tokens\n",
      " 55%|█████▌    | 275/497 [13:01<10:53,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.85 ms /   306 tokens\n",
      " 56%|█████▌    | 276/497 [13:04<10:45,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2855.86 ms /   309 tokens\n",
      " 56%|█████▌    | 277/497 [13:07<10:40,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2945.72 ms /   311 tokens\n",
      " 56%|█████▌    | 278/497 [13:10<10:42,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2957.68 ms /   310 tokens\n",
      " 56%|█████▌    | 279/497 [13:13<10:43,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.27 ms /   119 tokens\n",
      " 56%|█████▋    | 280/497 [13:14<08:38,  2.39s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2846.91 ms /   308 tokens\n",
      " 57%|█████▋    | 281/497 [13:17<09:07,  2.53s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2814.26 ms /   310 tokens\n",
      " 57%|█████▋    | 282/497 [13:20<09:23,  2.62s/it]Llama.generate: 84 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2809.13 ms /   304 tokens\n",
      " 57%|█████▋    | 283/497 [13:22<09:34,  2.69s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2820.26 ms /   309 tokens\n",
      " 57%|█████▋    | 284/497 [13:25<09:43,  2.74s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.28 ms /   310 tokens\n",
      " 57%|█████▋    | 285/497 [13:28<09:50,  2.79s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2942.77 ms /   316 tokens\n",
      " 58%|█████▊    | 286/497 [13:31<09:59,  2.84s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2822.72 ms /   309 tokens\n",
      " 58%|█████▊    | 287/497 [13:34<09:56,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2951.15 ms /   316 tokens\n",
      " 58%|█████▊    | 288/497 [13:37<10:03,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2860.65 ms /   305 tokens\n",
      " 58%|█████▊    | 289/497 [13:40<10:00,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.57 ms /   310 tokens\n",
      " 58%|█████▊    | 290/497 [13:43<09:58,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2471.85 ms /   269 tokens\n",
      " 59%|█████▊    | 291/497 [13:45<09:31,  2.78s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2911.26 ms /   309 tokens\n",
      " 59%|█████▉    | 292/497 [13:48<09:38,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2881.72 ms /   313 tokens\n",
      " 59%|█████▉    | 293/497 [13:51<09:41,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2875.07 ms /   311 tokens\n",
      " 59%|█████▉    | 294/497 [13:54<09:42,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.62 ms /   307 tokens\n",
      " 59%|█████▉    | 295/497 [13:57<09:43,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1760.12 ms /   190 tokens\n",
      " 60%|█████▉    | 296/497 [13:59<08:34,  2.56s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2950.95 ms /   306 tokens\n",
      " 60%|█████▉    | 297/497 [14:02<08:57,  2.69s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2912.94 ms /   313 tokens\n",
      " 60%|█████▉    | 298/497 [14:05<09:09,  2.76s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2948.17 ms /   313 tokens\n",
      " 60%|██████    | 299/497 [14:08<09:20,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.52 ms /   321 tokens\n",
      " 60%|██████    | 300/497 [14:11<09:24,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2907.39 ms /   308 tokens\n",
      " 61%|██████    | 301/497 [14:14<09:26,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2940.56 ms /   312 tokens\n",
      " 61%|██████    | 302/497 [14:17<09:28,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2939.00 ms /   321 tokens\n",
      " 61%|██████    | 303/497 [14:20<09:29,  2.93s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.00 ms /   307 tokens\n",
      " 61%|██████    | 304/497 [14:22<09:22,  2.92s/it]Llama.generate: 84 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2889.63 ms /   312 tokens\n",
      " 61%|██████▏   | 305/497 [14:25<09:19,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.15 ms /   311 tokens\n",
      " 62%|██████▏   | 306/497 [14:28<09:15,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2876.39 ms /   308 tokens\n",
      " 62%|██████▏   | 307/497 [14:31<09:12,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2477.86 ms /   258 tokens\n",
      " 62%|██████▏   | 308/497 [14:34<08:47,  2.79s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2881.52 ms /   308 tokens\n",
      " 62%|██████▏   | 309/497 [14:37<08:51,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.42 ms /   312 tokens\n",
      " 62%|██████▏   | 310/497 [14:39<08:51,  2.84s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.39 ms /   310 tokens\n",
      " 63%|██████▎   | 311/497 [14:42<08:51,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2859.98 ms /   313 tokens\n",
      " 63%|██████▎   | 312/497 [14:45<08:50,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2835.72 ms /   315 tokens\n",
      " 63%|██████▎   | 313/497 [14:48<08:47,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2899.45 ms /   318 tokens\n",
      " 63%|██████▎   | 314/497 [14:51<08:47,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2946.97 ms /   309 tokens\n",
      " 63%|██████▎   | 315/497 [14:54<08:49,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.38 ms /   305 tokens\n",
      " 64%|██████▎   | 316/497 [14:57<08:49,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.82 ms /   313 tokens\n",
      " 64%|██████▍   | 317/497 [15:00<08:43,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2861.74 ms /   314 tokens\n",
      " 64%|██████▍   | 318/497 [15:03<08:40,  2.91s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2858.08 ms /   306 tokens\n",
      " 64%|██████▍   | 319/497 [15:06<08:35,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2922.19 ms /   311 tokens\n",
      " 64%|██████▍   | 320/497 [15:09<08:35,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2923.05 ms /   311 tokens\n",
      " 65%|██████▍   | 321/497 [15:11<08:35,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2885.34 ms /   304 tokens\n",
      " 65%|██████▍   | 322/497 [15:14<08:31,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2866.04 ms /   313 tokens\n",
      " 65%|██████▍   | 323/497 [15:17<08:27,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2832.63 ms /   304 tokens\n",
      " 65%|██████▌   | 324/497 [15:20<08:22,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2858.53 ms /   312 tokens\n",
      " 65%|██████▌   | 325/497 [15:23<08:18,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2892.06 ms /   320 tokens\n",
      " 66%|██████▌   | 326/497 [15:26<08:16,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2845.86 ms /   309 tokens\n",
      " 66%|██████▌   | 327/497 [15:29<08:11,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2875.81 ms /   312 tokens\n",
      " 66%|██████▌   | 328/497 [15:32<08:10,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.10 ms /   307 tokens\n",
      " 66%|██████▌   | 329/497 [15:35<08:07,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2856.77 ms /   312 tokens\n",
      " 66%|██████▋   | 330/497 [15:38<08:03,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2899.03 ms /   312 tokens\n",
      " 67%|██████▋   | 331/497 [15:40<08:02,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.79 ms /   308 tokens\n",
      " 67%|██████▋   | 332/497 [15:43<07:58,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.50 ms /   309 tokens\n",
      " 67%|██████▋   | 333/497 [15:46<07:57,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2896.24 ms /   308 tokens\n",
      " 67%|██████▋   | 334/497 [15:49<07:55,  2.92s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2967.59 ms /   318 tokens\n",
      " 67%|██████▋   | 335/497 [15:52<07:56,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2929.94 ms /   310 tokens\n",
      " 68%|██████▊   | 336/497 [15:55<07:53,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.86 ms /   307 tokens\n",
      " 68%|██████▊   | 337/497 [15:58<07:47,  2.92s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.73 ms /   307 tokens\n",
      " 68%|██████▊   | 338/497 [16:01<07:46,  2.94s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2947.09 ms /   306 tokens\n",
      " 68%|██████▊   | 339/497 [16:04<07:45,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2927.08 ms /   310 tokens\n",
      " 68%|██████▊   | 340/497 [16:07<07:43,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2919.11 ms /   321 tokens\n",
      " 69%|██████▊   | 341/497 [16:10<07:40,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2970.68 ms /   310 tokens\n",
      " 69%|██████▉   | 342/497 [16:13<07:39,  2.97s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3135.72 ms /   313 tokens\n",
      " 69%|██████▉   | 343/497 [16:16<07:45,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3375.17 ms /   311 tokens\n",
      " 69%|██████▉   | 344/497 [16:19<07:59,  3.14s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3220.38 ms /   310 tokens\n",
      " 69%|██████▉   | 345/497 [16:23<08:02,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3334.73 ms /   308 tokens\n",
      " 70%|██████▉   | 346/497 [16:26<08:07,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3386.09 ms /   307 tokens\n",
      " 70%|██████▉   | 347/497 [16:29<08:13,  3.29s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3030.75 ms /   304 tokens\n",
      " 70%|███████   | 348/497 [16:33<08:00,  3.22s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3075.36 ms /   311 tokens\n",
      " 70%|███████   | 349/497 [16:36<07:51,  3.18s/it]Llama.generate: 85 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.58 ms /   302 tokens\n",
      " 70%|███████   | 350/497 [16:39<07:35,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3082.97 ms /   315 tokens\n",
      " 71%|███████   | 351/497 [16:42<07:33,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3046.88 ms /   311 tokens\n",
      " 71%|███████   | 352/497 [16:45<07:28,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3099.59 ms /   307 tokens\n",
      " 71%|███████   | 353/497 [16:48<07:27,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3058.25 ms /   312 tokens\n",
      " 71%|███████   | 354/497 [16:51<07:23,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3221.38 ms /   305 tokens\n",
      " 71%|███████▏  | 355/497 [16:54<07:27,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3144.21 ms /   308 tokens\n",
      " 72%|███████▏  | 356/497 [16:57<07:24,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.77 ms /   306 tokens\n",
      " 72%|███████▏  | 357/497 [17:01<07:19,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3195.46 ms /   317 tokens\n",
      " 72%|███████▏  | 358/497 [17:04<07:19,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3038.04 ms /   308 tokens\n",
      " 72%|███████▏  | 359/497 [17:07<07:13,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3127.51 ms /   319 tokens\n",
      " 72%|███████▏  | 360/497 [17:10<07:10,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3036.40 ms /   311 tokens\n",
      " 73%|███████▎  | 361/497 [17:13<07:04,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3269.58 ms /   324 tokens\n",
      " 73%|███████▎  | 362/497 [17:16<07:08,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2988.93 ms /   309 tokens\n",
      " 73%|███████▎  | 363/497 [17:19<06:58,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3195.37 ms /   313 tokens\n",
      " 73%|███████▎  | 364/497 [17:23<06:59,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.60 ms /   310 tokens\n",
      " 73%|███████▎  | 365/497 [17:26<06:53,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2711.96 ms /   267 tokens\n",
      " 74%|███████▎  | 366/497 [17:28<06:35,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.68 ms /   311 tokens\n",
      " 74%|███████▍  | 367/497 [17:31<06:33,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3037.08 ms /   307 tokens\n",
      " 74%|███████▍  | 368/497 [17:35<06:32,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3007.25 ms /   307 tokens\n",
      " 74%|███████▍  | 369/497 [17:38<06:29,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2936.31 ms /   305 tokens\n",
      " 74%|███████▍  | 370/497 [17:41<06:23,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.99 ms /   312 tokens\n",
      " 75%|███████▍  | 371/497 [17:44<06:22,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.47 ms /   312 tokens\n",
      " 75%|███████▍  | 372/497 [17:47<06:21,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2981.44 ms /   307 tokens\n",
      " 75%|███████▌  | 373/497 [17:50<06:16,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3212.66 ms /   316 tokens\n",
      " 75%|███████▌  | 374/497 [17:53<06:21,  3.10s/it]Llama.generate: 84 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2900.85 ms /   304 tokens\n",
      " 75%|███████▌  | 375/497 [17:56<06:12,  3.05s/it]Llama.generate: 85 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2953.15 ms /   308 tokens\n",
      " 76%|███████▌  | 376/497 [17:59<06:06,  3.03s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.20 ms /   306 tokens\n",
      " 76%|███████▌  | 377/497 [18:02<06:00,  3.01s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3021.09 ms /   309 tokens\n",
      " 76%|███████▌  | 378/497 [18:05<05:59,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3163.55 ms /   309 tokens\n",
      " 76%|███████▋  | 379/497 [18:08<06:02,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3169.75 ms /   309 tokens\n",
      " 76%|███████▋  | 380/497 [18:11<06:04,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3231.89 ms /   316 tokens\n",
      " 77%|███████▋  | 381/497 [18:15<06:05,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3158.77 ms /   309 tokens\n",
      " 77%|███████▋  | 382/497 [18:18<06:04,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3159.90 ms /   315 tokens\n",
      " 77%|███████▋  | 383/497 [18:21<06:02,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3278.17 ms /   312 tokens\n",
      " 77%|███████▋  | 384/497 [18:24<06:03,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3215.81 ms /   307 tokens\n",
      " 77%|███████▋  | 385/497 [18:27<06:01,  3.23s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3124.50 ms /   307 tokens\n",
      " 78%|███████▊  | 386/497 [18:31<05:56,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3204.02 ms /   313 tokens\n",
      " 78%|███████▊  | 387/497 [18:34<05:53,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.23 ms /   285 tokens\n",
      " 78%|███████▊  | 388/497 [18:37<05:37,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3235.95 ms /   304 tokens\n",
      " 78%|███████▊  | 389/497 [18:40<05:39,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3158.22 ms /   312 tokens\n",
      " 78%|███████▊  | 390/497 [18:43<05:37,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3210.56 ms /   313 tokens\n",
      " 79%|███████▊  | 391/497 [18:46<05:37,  3.18s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3139.03 ms /   311 tokens\n",
      " 79%|███████▉  | 392/497 [18:50<05:33,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3045.70 ms /   309 tokens\n",
      " 79%|███████▉  | 393/497 [18:53<05:27,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2934.08 ms /   308 tokens\n",
      " 79%|███████▉  | 394/497 [18:56<05:18,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.19 ms /   309 tokens\n",
      " 79%|███████▉  | 395/497 [18:59<05:12,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.50 ms /   310 tokens\n",
      " 80%|███████▉  | 396/497 [19:02<05:09,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3484.51 ms /   328 tokens\n",
      " 80%|███████▉  | 397/497 [19:05<05:19,  3.20s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3088.98 ms /   307 tokens\n",
      " 80%|████████  | 398/497 [19:08<05:14,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3213.65 ms /   311 tokens\n",
      " 80%|████████  | 399/497 [19:11<05:12,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3084.39 ms /   307 tokens\n",
      " 80%|████████  | 400/497 [19:15<05:07,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3104.36 ms /   314 tokens\n",
      " 81%|████████  | 401/497 [19:18<05:03,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2898.03 ms /   305 tokens\n",
      " 81%|████████  | 402/497 [19:21<04:53,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3053.55 ms /   310 tokens\n",
      " 81%|████████  | 403/497 [19:24<04:50,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2968.95 ms /   306 tokens\n",
      " 81%|████████▏ | 404/497 [19:27<04:44,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2933.05 ms /   308 tokens\n",
      " 81%|████████▏ | 405/497 [19:30<04:38,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3242.20 ms /   310 tokens\n",
      " 82%|████████▏ | 406/497 [19:33<04:42,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3328.12 ms /   314 tokens\n",
      " 82%|████████▏ | 407/497 [19:36<04:46,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.96 ms /   309 tokens\n",
      " 82%|████████▏ | 408/497 [19:39<04:38,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3062.55 ms /   308 tokens\n",
      " 82%|████████▏ | 409/497 [19:42<04:34,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3023.29 ms /   308 tokens\n",
      " 82%|████████▏ | 410/497 [19:46<04:29,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3430.52 ms /   318 tokens\n",
      " 83%|████████▎ | 411/497 [19:49<04:35,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.71 ms /   313 tokens\n",
      " 83%|████████▎ | 412/497 [19:52<04:30,  3.18s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2434.16 ms /   249 tokens\n",
      " 83%|████████▎ | 413/497 [19:55<04:09,  2.97s/it]Llama.generate: 84 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3141.10 ms /   319 tokens\n",
      " 83%|████████▎ | 414/497 [19:58<04:11,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2931.95 ms /   307 tokens\n",
      " 84%|████████▎ | 415/497 [20:01<04:06,  3.00s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2948.34 ms /   308 tokens\n",
      " 84%|████████▎ | 416/497 [20:04<04:02,  3.00s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2942.65 ms /   307 tokens\n",
      " 84%|████████▍ | 417/497 [20:07<03:59,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3081.38 ms /   316 tokens\n",
      " 84%|████████▍ | 418/497 [20:10<03:59,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3037.70 ms /   311 tokens\n",
      " 84%|████████▍ | 419/497 [20:13<03:57,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2898.83 ms /   299 tokens\n",
      " 85%|████████▍ | 420/497 [20:16<03:51,  3.01s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.07 ms /   314 tokens\n",
      " 85%|████████▍ | 421/497 [20:19<03:53,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3042.10 ms /   309 tokens\n",
      " 85%|████████▍ | 422/497 [20:22<03:50,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.75 ms /   308 tokens\n",
      " 85%|████████▌ | 423/497 [20:25<03:45,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3184.26 ms /   321 tokens\n",
      " 85%|████████▌ | 424/497 [20:28<03:46,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.78 ms /   312 tokens\n",
      " 86%|████████▌ | 425/497 [20:31<03:42,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3029.76 ms /   312 tokens\n",
      " 86%|████████▌ | 426/497 [20:34<03:38,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2945.10 ms /   309 tokens\n",
      " 86%|████████▌ | 427/497 [20:37<03:33,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3085.69 ms /   307 tokens\n",
      " 86%|████████▌ | 428/497 [20:40<03:31,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3190.80 ms /   311 tokens\n",
      " 86%|████████▋ | 429/497 [20:44<03:31,  3.12s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3047.18 ms /   306 tokens\n",
      " 87%|████████▋ | 430/497 [20:47<03:27,  3.10s/it]Llama.generate: 84 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3105.91 ms /   312 tokens\n",
      " 87%|████████▋ | 431/497 [20:50<03:25,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3224.58 ms /   309 tokens\n",
      " 87%|████████▋ | 432/497 [20:53<03:24,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3342.39 ms /   315 tokens\n",
      " 87%|████████▋ | 433/497 [20:57<03:26,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3168.01 ms /   308 tokens\n",
      " 87%|████████▋ | 434/497 [21:00<03:22,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3253.19 ms /   319 tokens\n",
      " 88%|████████▊ | 435/497 [21:03<03:20,  3.24s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3041.52 ms /   310 tokens\n",
      " 88%|████████▊ | 436/497 [21:06<03:14,  3.19s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3058.46 ms /   308 tokens\n",
      " 88%|████████▊ | 437/497 [21:09<03:09,  3.16s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3396.36 ms /   313 tokens\n",
      " 88%|████████▊ | 438/497 [21:13<03:10,  3.24s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3244.04 ms /   309 tokens\n",
      " 88%|████████▊ | 439/497 [21:16<03:08,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3099.05 ms /   310 tokens\n",
      " 89%|████████▊ | 440/497 [21:19<03:03,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3311.05 ms /   313 tokens\n",
      " 89%|████████▊ | 441/497 [21:22<03:02,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3233.63 ms /   307 tokens\n",
      " 89%|████████▉ | 442/497 [21:26<02:58,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2935.85 ms /   311 tokens\n",
      " 89%|████████▉ | 443/497 [21:29<02:50,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3196.99 ms /   310 tokens\n",
      " 89%|████████▉ | 444/497 [21:32<02:48,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.38 ms /   309 tokens\n",
      " 90%|████████▉ | 445/497 [21:35<02:44,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3021.64 ms /   305 tokens\n",
      " 90%|████████▉ | 446/497 [21:38<02:39,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3145.00 ms /   307 tokens\n",
      " 90%|████████▉ | 447/497 [21:41<02:37,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3094.59 ms /   311 tokens\n",
      " 90%|█████████ | 448/497 [21:44<02:33,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3112.29 ms /   310 tokens\n",
      " 90%|█████████ | 449/497 [21:47<02:30,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.21 ms /   312 tokens\n",
      " 91%|█████████ | 450/497 [21:51<02:28,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2976.07 ms /   311 tokens\n",
      " 91%|█████████ | 451/497 [21:54<02:23,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3157.03 ms /   309 tokens\n",
      " 91%|█████████ | 452/497 [21:57<02:21,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     720.91 ms /    71 tokens\n",
      " 91%|█████████ | 453/497 [21:58<01:46,  2.42s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3096.40 ms /   312 tokens\n",
      " 91%|█████████▏| 454/497 [22:01<01:53,  2.63s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2985.52 ms /   310 tokens\n",
      " 92%|█████████▏| 455/497 [22:04<01:55,  2.75s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3037.86 ms /   308 tokens\n",
      " 92%|█████████▏| 456/497 [22:07<01:56,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2982.45 ms /   311 tokens\n",
      " 92%|█████████▏| 457/497 [22:10<01:55,  2.89s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3172.97 ms /   315 tokens\n",
      " 92%|█████████▏| 458/497 [22:13<01:56,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3061.90 ms /   309 tokens\n",
      " 92%|█████████▏| 459/497 [22:16<01:54,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3158.09 ms /   313 tokens\n",
      " 93%|█████████▎| 460/497 [22:19<01:53,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.97 ms /   311 tokens\n",
      " 93%|█████████▎| 461/497 [22:22<01:51,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.72 ms /   305 tokens\n",
      " 93%|█████████▎| 462/497 [22:26<01:49,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3123.65 ms /   313 tokens\n",
      " 93%|█████████▎| 463/497 [22:29<01:46,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2407.03 ms /   238 tokens\n",
      " 93%|█████████▎| 464/497 [22:31<01:36,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.36 ms /   312 tokens\n",
      " 94%|█████████▎| 465/497 [22:34<01:35,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.10 ms /   314 tokens\n",
      " 94%|█████████▍| 466/497 [22:38<01:34,  3.05s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.29 ms /   316 tokens\n",
      " 94%|█████████▍| 467/497 [22:41<01:32,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3185.13 ms /   309 tokens\n",
      " 94%|█████████▍| 468/497 [22:44<01:30,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.30 ms /   312 tokens\n",
      " 94%|█████████▍| 469/497 [22:47<01:29,  3.19s/it]Llama.generate: 85 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2940.56 ms /   303 tokens\n",
      " 95%|█████████▍| 470/497 [22:50<01:24,  3.12s/it]Llama.generate: 85 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3001.51 ms /   309 tokens\n",
      " 95%|█████████▍| 471/497 [22:53<01:20,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2964.19 ms /   307 tokens\n",
      " 95%|█████████▍| 472/497 [22:56<01:16,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2997.09 ms /   310 tokens\n",
      " 95%|█████████▌| 473/497 [22:59<01:13,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2982.44 ms /   310 tokens\n",
      " 95%|█████████▌| 474/497 [23:02<01:09,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2966.88 ms /   307 tokens\n",
      " 96%|█████████▌| 475/497 [23:05<01:06,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3108.33 ms /   312 tokens\n",
      " 96%|█████████▌| 476/497 [23:08<01:04,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3138.60 ms /   316 tokens\n",
      " 96%|█████████▌| 477/497 [23:12<01:01,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2970.51 ms /   307 tokens\n",
      " 96%|█████████▌| 478/497 [23:15<00:58,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2999.83 ms /   311 tokens\n",
      " 96%|█████████▋| 479/497 [23:18<00:55,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3033.91 ms /   308 tokens\n",
      " 97%|█████████▋| 480/497 [23:21<00:52,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3214.73 ms /   312 tokens\n",
      " 97%|█████████▋| 481/497 [23:24<00:49,  3.12s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2960.81 ms /   304 tokens\n",
      " 97%|█████████▋| 482/497 [23:27<00:46,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2983.83 ms /   309 tokens\n",
      " 97%|█████████▋| 483/497 [23:30<00:42,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3060.72 ms /   312 tokens\n",
      " 97%|█████████▋| 484/497 [23:33<00:39,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.24 ms /   310 tokens\n",
      " 98%|█████████▊| 485/497 [23:36<00:36,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3126.31 ms /   318 tokens\n",
      " 98%|█████████▊| 486/497 [23:39<00:33,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2947.01 ms /   299 tokens\n",
      " 98%|█████████▊| 487/497 [23:42<00:30,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3076.63 ms /   310 tokens\n",
      " 98%|█████████▊| 488/497 [23:45<00:27,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3035.95 ms /   310 tokens\n",
      " 98%|█████████▊| 489/497 [23:48<00:24,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2957.90 ms /   306 tokens\n",
      " 99%|█████████▊| 490/497 [23:51<00:21,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3066.32 ms /   313 tokens\n",
      " 99%|█████████▉| 491/497 [23:55<00:18,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3119.65 ms /   315 tokens\n",
      " 99%|█████████▉| 492/497 [23:58<00:15,  3.08s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.44 ms /   308 tokens\n",
      " 99%|█████████▉| 493/497 [24:01<00:12,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3259.38 ms /   319 tokens\n",
      " 99%|█████████▉| 494/497 [24:04<00:09,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.71 ms /   313 tokens\n",
      "100%|█████████▉| 495/497 [24:07<00:06,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3106.70 ms /   310 tokens\n",
      "100%|█████████▉| 496/497 [24:10<00:03,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3303.11 ms /   309 tokens\n",
      "100%|██████████| 497/497 [24:14<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2391132224861441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Fusion\n",
    "\n",
    "The score is really bad here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized). And contrarily to the previous case, we have even more calls to the LLM hence the error propagates"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:40:17.918907Z",
     "start_time": "2025-01-13T13:14:09.751012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGFusion(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma-Fusion\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma-Fusion\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Storing documents embeddings (batch size is 1000): 0it [00:00, ?it/s]\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/497 [00:00<?, ?it/s]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3311.78 ms /   310 tokens\n",
      "  0%|          | 1/497 [00:03<27:43,  3.35s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3528.48 ms /   319 tokens\n",
      "  0%|          | 2/497 [00:06<28:39,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3303.18 ms /   307 tokens\n",
      "  1%|          | 3/497 [00:10<28:05,  3.41s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3306.65 ms /   309 tokens\n",
      "  1%|          | 4/497 [00:13<27:47,  3.38s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3301.46 ms /   311 tokens\n",
      "  1%|          | 5/497 [00:16<27:33,  3.36s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3293.52 ms /   310 tokens\n",
      "  1%|          | 6/497 [00:20<27:24,  3.35s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3590.17 ms /   312 tokens\n",
      "  1%|▏         | 7/497 [00:23<28:02,  3.43s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3313.61 ms /   310 tokens\n",
      "  2%|▏         | 8/497 [00:27<27:46,  3.41s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.07 ms /   311 tokens\n",
      "  2%|▏         | 9/497 [00:30<27:08,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.10 ms /   309 tokens\n",
      "  2%|▏         | 10/497 [00:33<26:19,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3045.07 ms /   311 tokens\n",
      "  2%|▏         | 11/497 [00:36<25:52,  3.19s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3077.38 ms /   307 tokens\n",
      "  2%|▏         | 12/497 [00:39<25:35,  3.17s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3005.87 ms /   311 tokens\n",
      "  3%|▎         | 13/497 [00:42<25:11,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2990.33 ms /   305 tokens\n",
      "  3%|▎         | 14/497 [00:45<24:55,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2909.18 ms /   305 tokens\n",
      "  3%|▎         | 15/497 [00:48<24:28,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2625.43 ms /   275 tokens\n",
      "  3%|▎         | 16/497 [00:51<23:30,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2949.78 ms /   309 tokens\n",
      "  3%|▎         | 17/497 [00:54<23:34,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2492.77 ms /   255 tokens\n",
      "  4%|▎         | 18/497 [00:56<22:30,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3092.68 ms /   314 tokens\n",
      "  4%|▍         | 19/497 [00:59<23:11,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2057.28 ms /   211 tokens\n",
      "  4%|▍         | 20/497 [01:01<21:09,  2.66s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.43 ms /   310 tokens\n",
      "  4%|▍         | 21/497 [01:04<21:55,  2.76s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3014.13 ms /   306 tokens\n",
      "  4%|▍         | 22/497 [01:07<22:31,  2.84s/it]Llama.generate: 83 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3391.71 ms /   323 tokens\n",
      "  5%|▍         | 23/497 [01:11<23:54,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3184.31 ms /   316 tokens\n",
      "  5%|▍         | 24/497 [01:14<24:17,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3341.54 ms /   310 tokens\n",
      "  5%|▌         | 25/497 [01:18<24:55,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3145.80 ms /   312 tokens\n",
      "  5%|▌         | 26/497 [01:21<24:52,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3076.74 ms /   308 tokens\n",
      "  5%|▌         | 27/497 [01:24<24:41,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3134.60 ms /   313 tokens\n",
      "  6%|▌         | 28/497 [01:27<24:40,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3223.81 ms /   316 tokens\n",
      "  6%|▌         | 29/497 [01:30<24:50,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3027.51 ms /   312 tokens\n",
      "  6%|▌         | 30/497 [01:33<24:29,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.69 ms /   310 tokens\n",
      "  6%|▌         | 31/497 [01:36<24:04,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3087.71 ms /   315 tokens\n",
      "  6%|▋         | 32/497 [01:39<24:03,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3122.82 ms /   316 tokens\n",
      "  7%|▋         | 33/497 [01:43<24:10,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.80 ms /   310 tokens\n",
      "  7%|▋         | 34/497 [01:46<23:50,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3244.47 ms /   304 tokens\n",
      "  7%|▋         | 35/497 [01:49<24:11,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3119.58 ms /   310 tokens\n",
      "  7%|▋         | 36/497 [01:52<24:09,  3.15s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.28 ms /   308 tokens\n",
      "  7%|▋         | 37/497 [01:55<24:06,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.43 ms /   309 tokens\n",
      "  8%|▊         | 38/497 [01:58<23:53,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3118.80 ms /   310 tokens\n",
      "  8%|▊         | 39/497 [02:01<23:55,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3259.12 ms /   315 tokens\n",
      "  8%|▊         | 40/497 [02:05<24:12,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3101.91 ms /   307 tokens\n",
      "  8%|▊         | 41/497 [02:08<24:03,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.22 ms /   309 tokens\n",
      "  8%|▊         | 42/497 [02:11<23:42,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3369.53 ms /   322 tokens\n",
      "  9%|▊         | 43/497 [02:14<24:17,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3318.76 ms /   309 tokens\n",
      "  9%|▉         | 44/497 [02:18<24:35,  3.26s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.79 ms /   309 tokens\n",
      "  9%|▉         | 45/497 [02:21<24:26,  3.25s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3150.07 ms /   313 tokens\n",
      "  9%|▉         | 46/497 [02:24<24:13,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3146.75 ms /   307 tokens\n",
      "  9%|▉         | 47/497 [02:27<24:05,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3228.31 ms /   312 tokens\n",
      " 10%|▉         | 48/497 [02:30<24:08,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3144.07 ms /   315 tokens\n",
      " 10%|▉         | 49/497 [02:34<23:57,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3038.60 ms /   305 tokens\n",
      " 10%|█         | 50/497 [02:37<23:35,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3067.95 ms /   305 tokens\n",
      " 10%|█         | 51/497 [02:40<23:24,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3124.03 ms /   314 tokens\n",
      " 10%|█         | 52/497 [02:43<23:21,  3.15s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3194.63 ms /   310 tokens\n",
      " 11%|█         | 53/497 [02:46<23:28,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2512.08 ms /   234 tokens\n",
      " 11%|█         | 54/497 [02:49<22:02,  2.98s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3332.25 ms /   316 tokens\n",
      " 11%|█         | 55/497 [02:52<22:49,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.58 ms /   306 tokens\n",
      " 11%|█▏        | 56/497 [02:55<22:56,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3250.14 ms /   313 tokens\n",
      " 11%|█▏        | 57/497 [02:58<23:12,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.01 ms /   314 tokens\n",
      " 12%|█▏        | 58/497 [03:02<23:30,  3.21s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3140.82 ms /   313 tokens\n",
      " 12%|█▏        | 59/497 [03:05<23:22,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3160.15 ms /   305 tokens\n",
      " 12%|█▏        | 60/497 [03:08<23:17,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.17 ms /   315 tokens\n",
      " 12%|█▏        | 61/497 [03:11<23:13,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2979.94 ms /   307 tokens\n",
      " 12%|█▏        | 62/497 [03:14<22:44,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3050.80 ms /   312 tokens\n",
      " 13%|█▎        | 63/497 [03:17<22:34,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.42 ms /   308 tokens\n",
      " 13%|█▎        | 64/497 [03:20<22:15,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3194.03 ms /   314 tokens\n",
      " 13%|█▎        | 65/497 [03:24<22:30,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.43 ms /   304 tokens\n",
      " 13%|█▎        | 66/497 [03:27<22:16,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3098.29 ms /   310 tokens\n",
      " 13%|█▎        | 67/497 [03:30<22:15,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3117.54 ms /   311 tokens\n",
      " 14%|█▎        | 68/497 [03:33<22:18,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2998.04 ms /   305 tokens\n",
      " 14%|█▍        | 69/497 [03:36<22:02,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3007.50 ms /   308 tokens\n",
      " 14%|█▍        | 70/497 [03:39<21:51,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3066.69 ms /   311 tokens\n",
      " 14%|█▍        | 71/497 [03:42<21:53,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3227.51 ms /   313 tokens\n",
      " 14%|█▍        | 72/497 [03:45<22:11,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3098.15 ms /   310 tokens\n",
      " 15%|█▍        | 73/497 [03:49<22:08,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3364.18 ms /   310 tokens\n",
      " 15%|█▍        | 74/497 [03:52<22:37,  3.21s/it]Llama.generate: 84 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3211.95 ms /   305 tokens\n",
      " 15%|█▌        | 75/497 [03:55<22:38,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3252.88 ms /   319 tokens\n",
      " 15%|█▌        | 76/497 [03:58<22:43,  3.24s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3213.98 ms /   315 tokens\n",
      " 15%|█▌        | 77/497 [04:02<22:42,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2165.66 ms /   212 tokens\n",
      " 16%|█▌        | 78/497 [04:04<20:27,  2.93s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3032.25 ms /   307 tokens\n",
      " 16%|█▌        | 79/497 [04:07<20:41,  2.97s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3144.10 ms /   309 tokens\n",
      " 16%|█▌        | 80/497 [04:10<21:04,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2932.26 ms /   305 tokens\n",
      " 16%|█▋        | 81/497 [04:13<20:52,  3.01s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3399.15 ms /   316 tokens\n",
      " 16%|█▋        | 82/497 [04:17<21:42,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.27 ms /   278 tokens\n",
      " 17%|█▋        | 83/497 [04:20<21:24,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3135.01 ms /   308 tokens\n",
      " 17%|█▋        | 84/497 [04:23<21:29,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.22 ms /   312 tokens\n",
      " 17%|█▋        | 85/497 [04:26<21:32,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3252.76 ms /   312 tokens\n",
      " 17%|█▋        | 86/497 [04:29<21:48,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3246.06 ms /   319 tokens\n",
      " 18%|█▊        | 87/497 [04:32<21:55,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3110.40 ms /   309 tokens\n",
      " 18%|█▊        | 88/497 [04:36<21:44,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.50 ms /   307 tokens\n",
      " 18%|█▊        | 89/497 [04:39<21:44,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3155.30 ms /   307 tokens\n",
      " 18%|█▊        | 90/497 [04:42<21:40,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3016.96 ms /   306 tokens\n",
      " 18%|█▊        | 91/497 [04:45<21:18,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.93 ms /   311 tokens\n",
      " 19%|█▊        | 92/497 [04:48<20:58,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3185.13 ms /   315 tokens\n",
      " 19%|█▊        | 93/497 [04:51<21:08,  3.14s/it]Llama.generate: 85 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.33 ms /   306 tokens\n",
      " 19%|█▉        | 94/497 [04:54<20:58,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3334.39 ms /   313 tokens\n",
      " 19%|█▉        | 95/497 [04:58<21:23,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3020.34 ms /   308 tokens\n",
      " 19%|█▉        | 96/497 [05:01<21:03,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3086.75 ms /   315 tokens\n",
      " 20%|█▉        | 97/497 [05:04<20:55,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3189.10 ms /   317 tokens\n",
      " 20%|█▉        | 98/497 [05:07<21:03,  3.17s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3009.10 ms /   309 tokens\n",
      " 20%|█▉        | 99/497 [05:10<20:44,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3172.38 ms /   311 tokens\n",
      " 20%|██        | 100/497 [05:13<20:50,  3.15s/it]Llama.generate: 88 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3008.48 ms /   305 tokens\n",
      " 20%|██        | 101/497 [05:16<20:33,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3156.76 ms /   312 tokens\n",
      " 21%|██        | 102/497 [05:20<20:38,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2980.73 ms /   308 tokens\n",
      " 21%|██        | 103/497 [05:23<20:20,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3146.11 ms /   312 tokens\n",
      " 21%|██        | 104/497 [05:26<20:27,  3.12s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.24 ms /   310 tokens\n",
      " 21%|██        | 105/497 [05:29<20:34,  3.15s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3040.21 ms /   308 tokens\n",
      " 21%|██▏       | 106/497 [05:32<20:21,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.68 ms /   307 tokens\n",
      " 22%|██▏       | 107/497 [05:35<20:02,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3126.38 ms /   316 tokens\n",
      " 22%|██▏       | 108/497 [05:38<20:09,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3007.62 ms /   310 tokens\n",
      " 22%|██▏       | 109/497 [05:41<19:57,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2981.88 ms /   297 tokens\n",
      " 22%|██▏       | 110/497 [05:44<19:46,  3.07s/it]Llama.generate: 85 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1219.77 ms /   127 tokens\n",
      " 22%|██▏       | 111/497 [05:45<16:11,  2.52s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.97 ms /   315 tokens\n",
      " 23%|██▎       | 112/497 [05:49<17:18,  2.70s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3101.54 ms /   316 tokens\n",
      " 23%|██▎       | 113/497 [05:52<18:06,  2.83s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2995.33 ms /   308 tokens\n",
      " 23%|██▎       | 114/497 [05:55<18:25,  2.89s/it]Llama.generate: 84 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3334.25 ms /   325 tokens\n",
      " 23%|██▎       | 115/497 [05:58<19:17,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.13 ms /   309 tokens\n",
      " 23%|██▎       | 116/497 [06:01<19:20,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3113.53 ms /   312 tokens\n",
      " 24%|██▎       | 117/497 [06:04<19:28,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.91 ms /   309 tokens\n",
      " 24%|██▎       | 118/497 [06:07<19:24,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.80 ms /   311 tokens\n",
      " 24%|██▍       | 119/497 [06:10<19:18,  3.07s/it]Llama.generate: 90 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3038.04 ms /   304 tokens\n",
      " 24%|██▍       | 120/497 [06:14<19:16,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.50 ms /   309 tokens\n",
      " 24%|██▍       | 121/497 [06:17<19:10,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2972.75 ms /   307 tokens\n",
      " 25%|██▍       | 122/497 [06:20<18:59,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3251.06 ms /   314 tokens\n",
      " 25%|██▍       | 123/497 [06:23<19:22,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2991.55 ms /   311 tokens\n",
      " 25%|██▍       | 124/497 [06:26<19:12,  3.09s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.62 ms /   311 tokens\n",
      " 25%|██▌       | 125/497 [06:29<19:03,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.72 ms /   307 tokens\n",
      " 25%|██▌       | 126/497 [06:32<18:53,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.55 ms /   309 tokens\n",
      " 26%|██▌       | 127/497 [06:35<18:50,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3217.21 ms /   310 tokens\n",
      " 26%|██▌       | 128/497 [06:38<19:08,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3201.96 ms /   304 tokens\n",
      " 26%|██▌       | 129/497 [06:41<19:17,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.45 ms /   308 tokens\n",
      " 26%|██▌       | 130/497 [06:45<19:18,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3331.77 ms /   319 tokens\n",
      " 26%|██▋       | 131/497 [06:48<19:39,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3327.77 ms /   312 tokens\n",
      " 27%|██▋       | 132/497 [06:51<19:49,  3.26s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3321.56 ms /   308 tokens\n",
      " 27%|██▋       | 133/497 [06:55<19:57,  3.29s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3531.40 ms /   311 tokens\n",
      " 27%|██▋       | 134/497 [06:58<20:23,  3.37s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3392.12 ms /   311 tokens\n",
      " 27%|██▋       | 135/497 [07:02<20:26,  3.39s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3275.25 ms /   308 tokens\n",
      " 27%|██▋       | 136/497 [07:05<20:12,  3.36s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3032.44 ms /   307 tokens\n",
      " 28%|██▊       | 137/497 [07:08<19:36,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.19 ms /   312 tokens\n",
      " 28%|██▊       | 138/497 [07:11<19:18,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3471.06 ms /   323 tokens\n",
      " 28%|██▊       | 139/497 [07:15<19:45,  3.31s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2994.01 ms /   309 tokens\n",
      " 28%|██▊       | 140/497 [07:18<19:10,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3004.85 ms /   311 tokens\n",
      " 28%|██▊       | 141/497 [07:21<18:47,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3175.85 ms /   308 tokens\n",
      " 29%|██▊       | 142/497 [07:24<18:48,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3448.42 ms /   326 tokens\n",
      " 29%|██▉       | 143/497 [07:27<19:16,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3007.79 ms /   303 tokens\n",
      " 29%|██▉       | 144/497 [07:30<18:49,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3198.61 ms /   308 tokens\n",
      " 29%|██▉       | 145/497 [07:34<18:50,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3199.33 ms /   317 tokens\n",
      " 29%|██▉       | 146/497 [07:37<18:48,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3015.30 ms /   308 tokens\n",
      " 30%|██▉       | 147/497 [07:40<18:28,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.88 ms /   309 tokens\n",
      " 30%|██▉       | 148/497 [07:43<18:11,  3.13s/it]Llama.generate: 84 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3138.88 ms /   318 tokens\n",
      " 30%|██▉       | 149/497 [07:46<18:11,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3009.72 ms /   311 tokens\n",
      " 30%|███       | 150/497 [07:49<17:58,  3.11s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3150.98 ms /   315 tokens\n",
      " 30%|███       | 151/497 [07:52<18:03,  3.13s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.61 ms /   311 tokens\n",
      " 31%|███       | 152/497 [07:56<17:59,  3.13s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.77 ms /   281 tokens\n",
      " 31%|███       | 153/497 [07:59<17:46,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    40 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3563.92 ms /   339 tokens\n",
      " 31%|███       | 154/497 [08:02<18:33,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3023.52 ms /   311 tokens\n",
      " 31%|███       | 155/497 [08:05<18:12,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3169.57 ms /   317 tokens\n",
      " 31%|███▏      | 156/497 [08:08<18:09,  3.20s/it]Llama.generate: 84 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2891.03 ms /   302 tokens\n",
      " 32%|███▏      | 157/497 [08:11<17:38,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3121.91 ms /   309 tokens\n",
      " 32%|███▏      | 158/497 [08:14<17:39,  3.13s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.74 ms /   309 tokens\n",
      " 32%|███▏      | 159/497 [08:18<17:31,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3000.53 ms /   310 tokens\n",
      " 32%|███▏      | 160/497 [08:21<17:20,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3136.31 ms /   315 tokens\n",
      " 32%|███▏      | 161/497 [08:24<17:24,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.63 ms /   312 tokens\n",
      " 33%|███▎      | 162/497 [08:27<17:18,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3382.22 ms /   318 tokens\n",
      " 33%|███▎      | 163/497 [08:30<17:47,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3222.84 ms /   308 tokens\n",
      " 33%|███▎      | 164/497 [08:34<17:50,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3031.14 ms /   308 tokens\n",
      " 33%|███▎      | 165/497 [08:37<17:32,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3147.43 ms /   311 tokens\n",
      " 33%|███▎      | 166/497 [08:40<17:29,  3.17s/it]Llama.generate: 85 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2852.13 ms /   302 tokens\n",
      " 34%|███▎      | 167/497 [08:43<16:57,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3131.18 ms /   312 tokens\n",
      " 34%|███▍      | 168/497 [08:46<17:03,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3326.37 ms /   318 tokens\n",
      " 34%|███▍      | 169/497 [08:49<17:23,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.34 ms /   311 tokens\n",
      " 34%|███▍      | 170/497 [08:52<17:13,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3099.23 ms /   308 tokens\n",
      " 34%|███▍      | 171/497 [08:55<17:06,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3073.14 ms /   309 tokens\n",
      " 35%|███▍      | 172/497 [08:59<16:59,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3055.84 ms /   306 tokens\n",
      " 35%|███▍      | 173/497 [09:02<16:53,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    48 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3918.05 ms /   347 tokens\n",
      " 35%|███▌      | 174/497 [09:06<18:10,  3.37s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3217.37 ms /   311 tokens\n",
      " 35%|███▌      | 175/497 [09:09<17:54,  3.34s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3242.09 ms /   312 tokens\n",
      " 35%|███▌      | 176/497 [09:12<17:44,  3.32s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3015.36 ms /   305 tokens\n",
      " 36%|███▌      | 177/497 [09:15<17:15,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.39 ms /   304 tokens\n",
      " 36%|███▌      | 178/497 [09:18<17:04,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3262.63 ms /   304 tokens\n",
      " 36%|███▌      | 179/497 [09:22<17:08,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3179.56 ms /   308 tokens\n",
      " 36%|███▌      | 180/497 [09:25<17:02,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2979.28 ms /   309 tokens\n",
      " 36%|███▋      | 181/497 [09:28<16:38,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3105.48 ms /   310 tokens\n",
      " 37%|███▋      | 182/497 [09:31<16:33,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3094.97 ms /   311 tokens\n",
      " 37%|███▋      | 183/497 [09:34<16:27,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3381.01 ms /   315 tokens\n",
      " 37%|███▋      | 184/497 [09:37<16:50,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3121.23 ms /   305 tokens\n",
      " 37%|███▋      | 185/497 [09:41<16:40,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3107.01 ms /   309 tokens\n",
      " 37%|███▋      | 186/497 [09:44<16:30,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3046.73 ms /   306 tokens\n",
      " 38%|███▊      | 187/497 [09:47<16:17,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3180.11 ms /   310 tokens\n",
      " 38%|███▊      | 188/497 [09:50<16:18,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3205.29 ms /   309 tokens\n",
      " 38%|███▊      | 189/497 [09:53<16:21,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3352.03 ms /   313 tokens\n",
      " 38%|███▊      | 190/497 [09:57<16:36,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3059.67 ms /   308 tokens\n",
      " 38%|███▊      | 191/497 [10:00<16:19,  3.20s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3369.84 ms /   315 tokens\n",
      " 39%|███▊      | 192/497 [10:03<16:35,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3523.97 ms /   316 tokens\n",
      " 39%|███▉      | 193/497 [10:07<16:58,  3.35s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2472.66 ms /   240 tokens\n",
      " 39%|███▉      | 194/497 [10:09<15:39,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3353.74 ms /   317 tokens\n",
      " 39%|███▉      | 195/497 [10:13<16:01,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3061.99 ms /   310 tokens\n",
      " 39%|███▉      | 196/497 [10:16<15:49,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3310.03 ms /   316 tokens\n",
      " 40%|███▉      | 197/497 [10:19<16:03,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3191.02 ms /   312 tokens\n",
      " 40%|███▉      | 198/497 [10:22<16:02,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.50 ms /   310 tokens\n",
      " 40%|████      | 199/497 [10:25<15:46,  3.17s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.15 ms /   311 tokens\n",
      " 40%|████      | 200/497 [10:28<15:32,  3.14s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.38 ms /   314 tokens\n",
      " 40%|████      | 201/497 [10:32<15:31,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2995.86 ms /   310 tokens\n",
      " 41%|████      | 202/497 [10:35<15:16,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3153.01 ms /   312 tokens\n",
      " 41%|████      | 203/497 [10:38<15:20,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3124.22 ms /   305 tokens\n",
      " 41%|████      | 204/497 [10:41<15:18,  3.14s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3065.20 ms /   309 tokens\n",
      " 41%|████      | 205/497 [10:44<15:13,  3.13s/it]Llama.generate: 85 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.47 ms /   305 tokens\n",
      " 41%|████▏     | 206/497 [10:47<15:04,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.74 ms /   313 tokens\n",
      " 42%|████▏     | 207/497 [10:50<15:10,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3223.30 ms /   316 tokens\n",
      " 42%|████▏     | 208/497 [10:54<15:16,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3041.52 ms /   305 tokens\n",
      " 42%|████▏     | 209/497 [10:57<15:04,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.82 ms /   314 tokens\n",
      " 42%|████▏     | 210/497 [11:00<15:03,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3002.56 ms /   308 tokens\n",
      " 42%|████▏     | 211/497 [11:03<14:49,  3.11s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3069.22 ms /   307 tokens\n",
      " 43%|████▎     | 212/497 [11:06<14:45,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.71 ms /   310 tokens\n",
      " 43%|████▎     | 213/497 [11:09<14:43,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3338.44 ms /   307 tokens\n",
      " 43%|████▎     | 214/497 [11:12<15:02,  3.19s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3340.64 ms /   314 tokens\n",
      " 43%|████▎     | 215/497 [11:16<15:13,  3.24s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.92 ms /   311 tokens\n",
      " 43%|████▎     | 216/497 [11:19<15:06,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3234.00 ms /   315 tokens\n",
      " 44%|████▎     | 217/497 [11:22<15:06,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3094.40 ms /   313 tokens\n",
      " 44%|████▍     | 218/497 [11:25<14:53,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3470.42 ms /   326 tokens\n",
      " 44%|████▍     | 219/497 [11:29<15:15,  3.29s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.43 ms /   305 tokens\n",
      " 44%|████▍     | 220/497 [11:32<14:51,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.29 ms /   308 tokens\n",
      " 44%|████▍     | 221/497 [11:35<14:35,  3.17s/it]Llama.generate: 84 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3367.49 ms /   317 tokens\n",
      " 45%|████▍     | 222/497 [11:38<14:52,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3319.00 ms /   312 tokens\n",
      " 45%|████▍     | 223/497 [11:42<14:57,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3128.56 ms /   310 tokens\n",
      " 45%|████▌     | 224/497 [11:45<14:44,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3207.19 ms /   310 tokens\n",
      " 45%|████▌     | 225/497 [11:48<14:40,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.76 ms /   316 tokens\n",
      " 45%|████▌     | 226/497 [11:51<14:37,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3143.36 ms /   308 tokens\n",
      " 46%|████▌     | 227/497 [11:55<14:27,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.73 ms /   308 tokens\n",
      " 46%|████▌     | 228/497 [11:58<14:12,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3035.98 ms /   310 tokens\n",
      " 46%|████▌     | 229/497 [12:01<14:00,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3421.72 ms /   311 tokens\n",
      " 46%|████▋     | 230/497 [12:04<14:23,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3411.18 ms /   316 tokens\n",
      " 46%|████▋     | 231/497 [12:08<14:36,  3.29s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2947.28 ms /   306 tokens\n",
      " 47%|████▋     | 232/497 [12:11<14:07,  3.20s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.47 ms /   314 tokens\n",
      " 47%|████▋     | 233/497 [12:14<14:04,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3175.70 ms /   307 tokens\n",
      " 47%|████▋     | 234/497 [12:17<14:01,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3262.16 ms /   312 tokens\n",
      " 47%|████▋     | 235/497 [12:20<14:06,  3.23s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3378.12 ms /   316 tokens\n",
      " 47%|████▋     | 236/497 [12:24<14:15,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2351.44 ms /   240 tokens\n",
      " 48%|████▊     | 237/497 [12:26<13:03,  3.01s/it]Llama.generate: 84 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.44 ms /   315 tokens\n",
      " 48%|████▊     | 238/497 [12:29<13:14,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3046.97 ms /   307 tokens\n",
      " 48%|████▊     | 239/497 [12:32<13:11,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3214.83 ms /   319 tokens\n",
      " 48%|████▊     | 240/497 [12:36<13:23,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3328.39 ms /   307 tokens\n",
      " 48%|████▊     | 241/497 [12:39<13:37,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3287.03 ms /   312 tokens\n",
      " 49%|████▊     | 242/497 [12:42<13:44,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3051.60 ms /   309 tokens\n",
      " 49%|████▉     | 243/497 [12:45<13:29,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3285.11 ms /   316 tokens\n",
      " 49%|████▉     | 244/497 [12:49<13:36,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2941.12 ms /   305 tokens\n",
      " 49%|████▉     | 245/497 [12:52<13:12,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3058.17 ms /   309 tokens\n",
      " 49%|████▉     | 246/497 [12:55<13:05,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3143.13 ms /   306 tokens\n",
      " 50%|████▉     | 247/497 [12:58<13:05,  3.14s/it]Llama.generate: 85 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3159.33 ms /   314 tokens\n",
      " 50%|████▉     | 248/497 [13:01<13:06,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3295.41 ms /   307 tokens\n",
      " 50%|█████     | 249/497 [13:04<13:15,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3251.15 ms /   318 tokens\n",
      " 50%|█████     | 250/497 [13:08<13:18,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2904.80 ms /   304 tokens\n",
      " 51%|█████     | 251/497 [13:11<12:53,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3368.25 ms /   326 tokens\n",
      " 51%|█████     | 252/497 [13:14<13:10,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3262.83 ms /   316 tokens\n",
      " 51%|█████     | 253/497 [13:17<13:11,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3304.00 ms /   308 tokens\n",
      " 51%|█████     | 254/497 [13:21<13:14,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3161.30 ms /   310 tokens\n",
      " 51%|█████▏    | 255/497 [13:24<13:05,  3.25s/it]Llama.generate: 84 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3189.28 ms /   305 tokens\n",
      " 52%|█████▏    | 256/497 [13:27<13:01,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3155.38 ms /   307 tokens\n",
      " 52%|█████▏    | 257/497 [13:30<12:53,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3192.25 ms /   311 tokens\n",
      " 52%|█████▏    | 258/497 [13:33<12:51,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2986.63 ms /   310 tokens\n",
      " 52%|█████▏    | 259/497 [13:36<12:32,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.91 ms /   308 tokens\n",
      " 52%|█████▏    | 260/497 [13:40<12:27,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.54 ms /   309 tokens\n",
      " 53%|█████▎    | 261/497 [13:43<12:25,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.30 ms /   310 tokens\n",
      " 53%|█████▎    | 262/497 [13:46<12:19,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.10 ms /   309 tokens\n",
      " 53%|█████▎    | 263/497 [13:49<12:11,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3027.23 ms /   305 tokens\n",
      " 53%|█████▎    | 264/497 [13:52<12:03,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.01 ms /   311 tokens\n",
      " 53%|█████▎    | 265/497 [13:55<11:55,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3177.27 ms /   308 tokens\n",
      " 54%|█████▎    | 266/497 [13:58<12:01,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3125.39 ms /   314 tokens\n",
      " 54%|█████▎    | 267/497 [14:01<12:00,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3253.79 ms /   316 tokens\n",
      " 54%|█████▍    | 268/497 [14:05<12:07,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3037.01 ms /   308 tokens\n",
      " 54%|█████▍    | 269/497 [14:08<11:56,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3163.58 ms /   312 tokens\n",
      " 54%|█████▍    | 270/497 [14:11<11:57,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3014.49 ms /   310 tokens\n",
      " 55%|█████▍    | 271/497 [14:14<11:46,  3.13s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1910.87 ms /   194 tokens\n",
      " 55%|█████▍    | 272/497 [14:16<10:23,  2.77s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3005.60 ms /   307 tokens\n",
      " 55%|█████▍    | 273/497 [14:19<10:37,  2.85s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.46 ms /   311 tokens\n",
      " 55%|█████▌    | 274/497 [14:22<10:49,  2.91s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3181.42 ms /   313 tokens\n",
      " 55%|█████▌    | 275/497 [14:25<11:06,  3.00s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3275.03 ms /   306 tokens\n",
      " 56%|█████▌    | 276/497 [14:29<11:22,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3177.84 ms /   309 tokens\n",
      " 56%|█████▌    | 277/497 [14:32<11:28,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2985.68 ms /   311 tokens\n",
      " 56%|█████▌    | 278/497 [14:35<11:17,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.38 ms /   310 tokens\n",
      " 56%|█████▌    | 279/497 [14:38<11:13,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2968.76 ms /   310 tokens\n",
      " 56%|█████▋    | 280/497 [14:41<11:03,  3.06s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2971.04 ms /   308 tokens\n",
      " 57%|█████▋    | 281/497 [14:44<10:57,  3.04s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3049.80 ms /   310 tokens\n",
      " 57%|█████▋    | 282/497 [14:47<10:56,  3.05s/it]Llama.generate: 84 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2983.73 ms /   304 tokens\n",
      " 57%|█████▋    | 283/497 [14:50<10:51,  3.04s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.71 ms /   309 tokens\n",
      " 57%|█████▋    | 284/497 [14:53<10:48,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3014.43 ms /   310 tokens\n",
      " 57%|█████▋    | 285/497 [14:56<10:45,  3.04s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3417.16 ms /   316 tokens\n",
      " 58%|█████▊    | 286/497 [14:59<11:07,  3.16s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3383.96 ms /   309 tokens\n",
      " 58%|█████▊    | 287/497 [15:03<11:19,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3354.61 ms /   316 tokens\n",
      " 58%|█████▊    | 288/497 [15:06<11:26,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2972.80 ms /   305 tokens\n",
      " 58%|█████▊    | 289/497 [15:09<11:04,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.81 ms /   310 tokens\n",
      " 58%|█████▊    | 290/497 [15:12<10:54,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3265.83 ms /   309 tokens\n",
      " 59%|█████▊    | 291/497 [15:16<10:59,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1914.94 ms /   180 tokens\n",
      " 59%|█████▉    | 292/497 [15:18<09:38,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3365.53 ms /   313 tokens\n",
      " 59%|█████▉    | 293/497 [15:21<10:10,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3248.98 ms /   311 tokens\n",
      " 59%|█████▉    | 294/497 [15:24<10:25,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3256.31 ms /   307 tokens\n",
      " 59%|█████▉    | 295/497 [15:28<10:34,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3250.53 ms /   308 tokens\n",
      " 60%|█████▉    | 296/497 [15:31<10:39,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.45 ms /   279 tokens\n",
      " 60%|█████▉    | 297/497 [15:34<10:18,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3233.34 ms /   313 tokens\n",
      " 60%|█████▉    | 298/497 [15:37<10:25,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.17 ms /   313 tokens\n",
      " 60%|██████    | 299/497 [15:40<10:32,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3338.91 ms /   321 tokens\n",
      " 60%|██████    | 300/497 [15:44<10:38,  3.24s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3054.99 ms /   308 tokens\n",
      " 61%|██████    | 301/497 [15:47<10:26,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3168.50 ms /   312 tokens\n",
      " 61%|██████    | 302/497 [15:50<10:23,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3260.79 ms /   321 tokens\n",
      " 61%|██████    | 303/497 [15:53<10:25,  3.22s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.29 ms /   307 tokens\n",
      " 61%|██████    | 304/497 [15:56<10:16,  3.19s/it]Llama.generate: 84 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.67 ms /   312 tokens\n",
      " 61%|██████▏   | 305/497 [15:59<10:06,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.06 ms /   311 tokens\n",
      " 62%|██████▏   | 306/497 [16:03<10:02,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2966.74 ms /   308 tokens\n",
      " 62%|██████▏   | 307/497 [16:06<09:49,  3.10s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2937.79 ms /   306 tokens\n",
      " 62%|██████▏   | 308/497 [16:09<09:39,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2969.82 ms /   308 tokens\n",
      " 62%|██████▏   | 309/497 [16:12<09:32,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3038.54 ms /   312 tokens\n",
      " 62%|██████▏   | 310/497 [16:15<09:31,  3.05s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.32 ms /   310 tokens\n",
      " 63%|██████▎   | 311/497 [16:18<09:26,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.49 ms /   313 tokens\n",
      " 63%|██████▎   | 312/497 [16:21<09:33,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3312.12 ms /   315 tokens\n",
      " 63%|██████▎   | 313/497 [16:24<09:44,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3293.31 ms /   318 tokens\n",
      " 63%|██████▎   | 314/497 [16:28<09:48,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3175.93 ms /   309 tokens\n",
      " 63%|██████▎   | 315/497 [16:31<09:45,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3029.80 ms /   305 tokens\n",
      " 64%|██████▎   | 316/497 [16:34<09:33,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.23 ms /   313 tokens\n",
      " 64%|██████▍   | 317/497 [16:37<09:38,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3230.70 ms /   314 tokens\n",
      " 64%|██████▍   | 318/497 [16:40<09:38,  3.23s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3067.43 ms /   306 tokens\n",
      " 64%|██████▍   | 319/497 [16:43<09:27,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3531.24 ms /   311 tokens\n",
      " 64%|██████▍   | 320/497 [16:47<09:44,  3.30s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3178.14 ms /   311 tokens\n",
      " 65%|██████▍   | 321/497 [16:50<09:36,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3495.38 ms /   304 tokens\n",
      " 65%|██████▍   | 322/497 [16:54<09:45,  3.35s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3728.78 ms /   313 tokens\n",
      " 65%|██████▍   | 323/497 [16:58<10:04,  3.47s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3079.57 ms /   304 tokens\n",
      " 65%|██████▌   | 324/497 [17:01<09:41,  3.36s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3040.86 ms /   312 tokens\n",
      " 65%|██████▌   | 325/497 [17:04<09:23,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3366.90 ms /   320 tokens\n",
      " 66%|██████▌   | 326/497 [17:07<09:26,  3.31s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3329.22 ms /   309 tokens\n",
      " 66%|██████▌   | 327/497 [17:10<09:25,  3.33s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3313.46 ms /   312 tokens\n",
      " 66%|██████▌   | 328/497 [17:14<09:23,  3.33s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3056.30 ms /   307 tokens\n",
      " 66%|██████▌   | 329/497 [17:17<09:07,  3.26s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3098.66 ms /   312 tokens\n",
      " 66%|██████▋   | 330/497 [17:20<08:58,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3291.30 ms /   312 tokens\n",
      " 67%|██████▋   | 331/497 [17:23<08:59,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3106.00 ms /   308 tokens\n",
      " 67%|██████▋   | 332/497 [17:26<08:51,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2969.18 ms /   309 tokens\n",
      " 67%|██████▋   | 333/497 [17:29<08:37,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3009.83 ms /   308 tokens\n",
      " 67%|██████▋   | 334/497 [17:33<08:28,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3126.51 ms /   318 tokens\n",
      " 67%|██████▋   | 335/497 [17:36<08:27,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2980.66 ms /   310 tokens\n",
      " 68%|██████▊   | 336/497 [17:39<08:18,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2985.80 ms /   307 tokens\n",
      " 68%|██████▊   | 337/497 [17:42<08:11,  3.07s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2948.26 ms /   307 tokens\n",
      " 68%|██████▊   | 338/497 [17:45<08:03,  3.04s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2941.47 ms /   306 tokens\n",
      " 68%|██████▊   | 339/497 [17:48<07:57,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2995.56 ms /   310 tokens\n",
      " 68%|██████▊   | 340/497 [17:51<07:54,  3.02s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3499.14 ms /   321 tokens\n",
      " 69%|██████▊   | 341/497 [17:54<08:15,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3330.24 ms /   310 tokens\n",
      " 69%|██████▉   | 342/497 [17:58<08:20,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3337.93 ms /   313 tokens\n",
      " 69%|██████▉   | 343/497 [18:01<08:23,  3.27s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3028.50 ms /   311 tokens\n",
      " 69%|██████▉   | 344/497 [18:04<08:10,  3.21s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3094.52 ms /   310 tokens\n",
      " 69%|██████▉   | 345/497 [18:07<08:03,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3021.88 ms /   308 tokens\n",
      " 70%|██████▉   | 346/497 [18:10<07:54,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3055.71 ms /   307 tokens\n",
      " 70%|██████▉   | 347/497 [18:13<07:48,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3067.79 ms /   304 tokens\n",
      " 70%|███████   | 348/497 [18:16<07:44,  3.12s/it]Llama.generate: 85 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3413.49 ms /   311 tokens\n",
      " 70%|███████   | 349/497 [18:20<07:55,  3.22s/it]Llama.generate: 85 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2378.01 ms /   227 tokens\n",
      " 70%|███████   | 350/497 [18:22<07:17,  2.98s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3489.13 ms /   315 tokens\n",
      " 71%|███████   | 351/497 [18:26<07:39,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3163.16 ms /   311 tokens\n",
      " 71%|███████   | 352/497 [18:29<07:37,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3046.62 ms /   307 tokens\n",
      " 71%|███████   | 353/497 [18:32<07:31,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3094.20 ms /   312 tokens\n",
      " 71%|███████   | 354/497 [18:35<07:27,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2969.11 ms /   305 tokens\n",
      " 71%|███████▏  | 355/497 [18:38<07:18,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.02 ms /   308 tokens\n",
      " 72%|███████▏  | 356/497 [18:41<07:14,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1413.09 ms /   141 tokens\n",
      " 72%|███████▏  | 357/497 [18:43<06:02,  2.59s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.05 ms /   317 tokens\n",
      " 72%|███████▏  | 358/497 [18:46<06:24,  2.77s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3048.99 ms /   308 tokens\n",
      " 72%|███████▏  | 359/497 [18:49<06:34,  2.86s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3153.31 ms /   319 tokens\n",
      " 72%|███████▏  | 360/497 [18:52<06:45,  2.96s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.25 ms /   311 tokens\n",
      " 73%|███████▎  | 361/497 [18:55<06:46,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3311.91 ms /   324 tokens\n",
      " 73%|███████▎  | 362/497 [18:58<06:57,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3151.39 ms /   309 tokens\n",
      " 73%|███████▎  | 363/497 [19:02<06:58,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3221.02 ms /   313 tokens\n",
      " 73%|███████▎  | 364/497 [19:05<06:59,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3224.60 ms /   310 tokens\n",
      " 73%|███████▎  | 365/497 [19:08<07:01,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3200.56 ms /   314 tokens\n",
      " 74%|███████▎  | 366/497 [19:11<06:59,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3039.04 ms /   311 tokens\n",
      " 74%|███████▍  | 367/497 [19:14<06:50,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2952.41 ms /   307 tokens\n",
      " 74%|███████▍  | 368/497 [19:17<06:40,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2972.08 ms /   307 tokens\n",
      " 74%|███████▍  | 369/497 [19:20<06:33,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3188.64 ms /   305 tokens\n",
      " 74%|███████▍  | 370/497 [19:24<06:36,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3211.32 ms /   312 tokens\n",
      " 75%|███████▍  | 371/497 [19:27<06:37,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3101.11 ms /   312 tokens\n",
      " 75%|███████▍  | 372/497 [19:30<06:33,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.53 ms /   307 tokens\n",
      " 75%|███████▌  | 373/497 [19:33<06:29,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3261.85 ms /   316 tokens\n",
      " 75%|███████▌  | 374/497 [19:36<06:31,  3.18s/it]Llama.generate: 84 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3056.44 ms /   304 tokens\n",
      " 75%|███████▌  | 375/497 [19:40<06:24,  3.15s/it]Llama.generate: 85 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3106.34 ms /   308 tokens\n",
      " 76%|███████▌  | 376/497 [19:43<06:21,  3.15s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2979.69 ms /   306 tokens\n",
      " 76%|███████▌  | 377/497 [19:46<06:12,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2987.97 ms /   309 tokens\n",
      " 76%|███████▌  | 378/497 [19:49<06:06,  3.08s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3080.11 ms /   309 tokens\n",
      " 76%|███████▋  | 379/497 [19:52<06:04,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2419.27 ms /   251 tokens\n",
      " 76%|███████▋  | 380/497 [19:54<05:39,  2.90s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3102.42 ms /   316 tokens\n",
      " 77%|███████▋  | 381/497 [19:57<05:44,  2.97s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2979.04 ms /   309 tokens\n",
      " 77%|███████▋  | 382/497 [20:00<05:42,  2.98s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3099.78 ms /   315 tokens\n",
      " 77%|███████▋  | 383/497 [20:04<05:44,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3042.07 ms /   312 tokens\n",
      " 77%|███████▋  | 384/497 [20:07<05:43,  3.04s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2957.83 ms /   307 tokens\n",
      " 77%|███████▋  | 385/497 [20:10<05:38,  3.03s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3015.48 ms /   307 tokens\n",
      " 78%|███████▊  | 386/497 [20:13<05:36,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3097.73 ms /   313 tokens\n",
      " 78%|███████▊  | 387/497 [20:16<05:36,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3058.69 ms /   309 tokens\n",
      " 78%|███████▊  | 388/497 [20:19<05:34,  3.06s/it]Llama.generate: 83 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3036.13 ms /   304 tokens\n",
      " 78%|███████▊  | 389/497 [20:22<05:31,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3109.00 ms /   312 tokens\n",
      " 78%|███████▊  | 390/497 [20:25<05:30,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3224.03 ms /   313 tokens\n",
      " 79%|███████▊  | 391/497 [20:28<05:32,  3.14s/it]Llama.generate: 84 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3158.57 ms /   311 tokens\n",
      " 79%|███████▉  | 392/497 [20:31<05:30,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2992.76 ms /   309 tokens\n",
      " 79%|███████▉  | 393/497 [20:35<05:23,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3061.34 ms /   308 tokens\n",
      " 79%|███████▉  | 394/497 [20:38<05:19,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     914.47 ms /    92 tokens\n",
      " 79%|███████▉  | 395/497 [20:39<04:10,  2.46s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3015.12 ms /   310 tokens\n",
      " 80%|███████▉  | 396/497 [20:42<04:26,  2.64s/it]Llama.generate: 83 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3342.56 ms /   328 tokens\n",
      " 80%|███████▉  | 397/497 [20:45<04:45,  2.86s/it]Llama.generate: 84 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3117.22 ms /   307 tokens\n",
      " 80%|████████  | 398/497 [20:48<04:51,  2.95s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3256.09 ms /   311 tokens\n",
      " 80%|████████  | 399/497 [20:51<04:58,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3021.97 ms /   307 tokens\n",
      " 80%|████████  | 400/497 [20:54<04:55,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3267.75 ms /   314 tokens\n",
      " 81%|████████  | 401/497 [20:58<04:59,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.20 ms /   305 tokens\n",
      " 81%|████████  | 402/497 [21:01<04:54,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3159.28 ms /   310 tokens\n",
      " 81%|████████  | 403/497 [21:04<04:54,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3085.05 ms /   306 tokens\n",
      " 81%|████████▏ | 404/497 [21:07<04:50,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3208.90 ms /   308 tokens\n",
      " 81%|████████▏ | 405/497 [21:10<04:50,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3032.99 ms /   310 tokens\n",
      " 82%|████████▏ | 406/497 [21:13<04:45,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3336.19 ms /   314 tokens\n",
      " 82%|████████▏ | 407/497 [21:17<04:48,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.62 ms /   309 tokens\n",
      " 82%|████████▏ | 408/497 [21:20<04:42,  3.17s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2974.26 ms /   308 tokens\n",
      " 82%|████████▏ | 409/497 [21:23<04:34,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3038.51 ms /   308 tokens\n",
      " 82%|████████▏ | 410/497 [21:26<04:30,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3222.59 ms /   318 tokens\n",
      " 83%|████████▎ | 411/497 [21:29<04:30,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3096.17 ms /   313 tokens\n",
      " 83%|████████▎ | 412/497 [21:32<04:26,  3.14s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3333.76 ms /   316 tokens\n",
      " 83%|████████▎ | 413/497 [21:36<04:29,  3.21s/it]Llama.generate: 84 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.25 ms /   319 tokens\n",
      " 83%|████████▎ | 414/497 [21:39<04:26,  3.21s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3155.15 ms /   307 tokens\n",
      " 84%|████████▎ | 415/497 [21:42<04:22,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2999.90 ms /   308 tokens\n",
      " 84%|████████▎ | 416/497 [21:45<04:15,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3215.79 ms /   307 tokens\n",
      " 84%|████████▍ | 417/497 [21:48<04:14,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3142.24 ms /   292 tokens\n",
      " 84%|████████▍ | 418/497 [21:52<04:11,  3.18s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3300.42 ms /   311 tokens\n",
      " 84%|████████▍ | 419/497 [21:55<04:11,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3377.79 ms /   309 tokens\n",
      " 85%|████████▍ | 420/497 [21:58<04:12,  3.28s/it]Llama.generate: 84 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3493.24 ms /   314 tokens\n",
      " 85%|████████▍ | 421/497 [22:02<04:15,  3.36s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3348.23 ms /   309 tokens\n",
      " 85%|████████▍ | 422/497 [22:05<04:12,  3.36s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3029.83 ms /   308 tokens\n",
      " 85%|████████▌ | 423/497 [22:08<04:02,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.87 ms /   321 tokens\n",
      " 85%|████████▌ | 424/497 [22:12<04:00,  3.29s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3033.83 ms /   304 tokens\n",
      " 86%|████████▌ | 425/497 [22:15<03:51,  3.22s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3376.19 ms /   312 tokens\n",
      " 86%|████████▌ | 426/497 [22:18<03:52,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3262.65 ms /   309 tokens\n",
      " 86%|████████▌ | 427/497 [22:21<03:49,  3.28s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3381.28 ms /   307 tokens\n",
      " 86%|████████▌ | 428/497 [22:25<03:49,  3.32s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3151.86 ms /   311 tokens\n",
      " 86%|████████▋ | 429/497 [22:28<03:42,  3.28s/it]Llama.generate: 84 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3085.29 ms /   306 tokens\n",
      " 87%|████████▋ | 430/497 [22:31<03:36,  3.23s/it]Llama.generate: 84 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3063.91 ms /   312 tokens\n",
      " 87%|████████▋ | 431/497 [22:34<03:30,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2977.72 ms /   309 tokens\n",
      " 87%|████████▋ | 432/497 [22:37<03:23,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3128.57 ms /   315 tokens\n",
      " 87%|████████▋ | 433/497 [22:40<03:21,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.61 ms /   308 tokens\n",
      " 87%|████████▋ | 434/497 [22:43<03:15,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3126.90 ms /   319 tokens\n",
      " 88%|████████▊ | 435/497 [22:46<03:13,  3.12s/it]Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.62 ms /   310 tokens\n",
      " 88%|████████▊ | 436/497 [22:49<03:08,  3.09s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3255.35 ms /   308 tokens\n",
      " 88%|████████▊ | 437/497 [22:53<03:09,  3.15s/it]Llama.generate: 84 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1615.93 ms /   157 tokens\n",
      " 88%|████████▊ | 438/497 [22:54<02:39,  2.70s/it]Llama.generate: 84 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2991.82 ms /   309 tokens\n",
      " 88%|████████▊ | 439/497 [22:57<02:42,  2.80s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.31 ms /   310 tokens\n",
      " 89%|████████▊ | 440/497 [23:00<02:43,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3195.19 ms /   313 tokens\n",
      " 89%|████████▊ | 441/497 [23:04<02:46,  2.98s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3188.75 ms /   307 tokens\n",
      " 89%|████████▉ | 442/497 [23:07<02:47,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2249.31 ms /   229 tokens\n",
      " 89%|████████▉ | 443/497 [23:09<02:32,  2.82s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.05 ms /   310 tokens\n",
      " 89%|████████▉ | 444/497 [23:12<02:32,  2.88s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3067.22 ms /   309 tokens\n",
      " 90%|████████▉ | 445/497 [23:15<02:33,  2.94s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.95 ms /   305 tokens\n",
      " 90%|████████▉ | 446/497 [23:18<02:32,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3335.52 ms /   307 tokens\n",
      " 90%|████████▉ | 447/497 [23:22<02:35,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3028.85 ms /   311 tokens\n",
      " 90%|█████████ | 448/497 [23:25<02:31,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.71 ms /   310 tokens\n",
      " 90%|█████████ | 449/497 [23:28<02:29,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3042.51 ms /   312 tokens\n",
      " 91%|█████████ | 450/497 [23:31<02:25,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3191.29 ms /   311 tokens\n",
      " 91%|█████████ | 451/497 [23:34<02:24,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3082.54 ms /   309 tokens\n",
      " 91%|█████████ | 452/497 [23:37<02:20,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.51 ms /   306 tokens\n",
      " 91%|█████████ | 453/497 [23:40<02:16,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1135.22 ms /   110 tokens\n",
      " 91%|█████████▏| 454/497 [23:42<01:48,  2.52s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2995.51 ms /   310 tokens\n",
      " 92%|█████████▏| 455/497 [23:45<01:52,  2.67s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3046.83 ms /   308 tokens\n",
      " 92%|█████████▏| 456/497 [23:48<01:54,  2.80s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3003.44 ms /   311 tokens\n",
      " 92%|█████████▏| 457/497 [23:51<01:54,  2.87s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3170.51 ms /   315 tokens\n",
      " 92%|█████████▏| 458/497 [23:54<01:55,  2.97s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3008.43 ms /   309 tokens\n",
      " 92%|█████████▏| 459/497 [23:57<01:53,  2.99s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.49 ms /   313 tokens\n",
      " 93%|█████████▎| 460/497 [24:00<01:52,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3063.27 ms /   311 tokens\n",
      " 93%|█████████▎| 461/497 [24:03<01:49,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2958.70 ms /   305 tokens\n",
      " 93%|█████████▎| 462/497 [24:06<01:46,  3.03s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3051.30 ms /   313 tokens\n",
      " 93%|█████████▎| 463/497 [24:09<01:43,  3.05s/it]Llama.generate: 83 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3101.64 ms /   317 tokens\n",
      " 93%|█████████▎| 464/497 [24:12<01:41,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3027.71 ms /   312 tokens\n",
      " 94%|█████████▎| 465/497 [24:16<01:38,  3.07s/it]Llama.generate: 83 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3366.12 ms /   314 tokens\n",
      " 94%|█████████▍| 466/497 [24:19<01:38,  3.17s/it]Llama.generate: 84 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3331.58 ms /   316 tokens\n",
      " 94%|█████████▍| 467/497 [24:22<01:36,  3.23s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3258.44 ms /   309 tokens\n",
      " 94%|█████████▍| 468/497 [24:26<01:34,  3.25s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3259.25 ms /   312 tokens\n",
      " 94%|█████████▍| 469/497 [24:29<01:31,  3.26s/it]Llama.generate: 85 prefix-match hit, remaining 4 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     4 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.89 ms /   303 tokens\n",
      " 95%|█████████▍| 470/497 [24:32<01:25,  3.15s/it]Llama.generate: 85 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3060.02 ms /   309 tokens\n",
      " 95%|█████████▍| 471/497 [24:35<01:21,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3081.17 ms /   307 tokens\n",
      " 95%|█████████▍| 472/497 [24:38<01:18,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3131.42 ms /   310 tokens\n",
      " 95%|█████████▌| 473/497 [24:41<01:15,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3053.90 ms /   310 tokens\n",
      " 95%|█████████▌| 474/497 [24:44<01:11,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3028.84 ms /   307 tokens\n",
      " 96%|█████████▌| 475/497 [24:47<01:08,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3133.80 ms /   312 tokens\n",
      " 96%|█████████▌| 476/497 [24:50<01:05,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3203.38 ms /   316 tokens\n",
      " 96%|█████████▌| 477/497 [24:54<01:03,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.61 ms /   307 tokens\n",
      " 96%|█████████▌| 478/497 [24:57<00:59,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3050.26 ms /   311 tokens\n",
      " 96%|█████████▋| 479/497 [25:00<00:56,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3075.72 ms /   308 tokens\n",
      " 97%|█████████▋| 480/497 [25:03<00:52,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3161.85 ms /   312 tokens\n",
      " 97%|█████████▋| 481/497 [25:06<00:50,  3.14s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2980.10 ms /   304 tokens\n",
      " 97%|█████████▋| 482/497 [25:09<00:46,  3.10s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3036.27 ms /   309 tokens\n",
      " 97%|█████████▋| 483/497 [25:12<00:43,  3.09s/it]Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3139.20 ms /   312 tokens\n",
      " 97%|█████████▋| 484/497 [25:15<00:40,  3.11s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3153.31 ms /   310 tokens\n",
      " 98%|█████████▊| 485/497 [25:19<00:37,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3158.37 ms /   318 tokens\n",
      " 98%|█████████▊| 486/497 [25:22<00:34,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3003.33 ms /   307 tokens\n",
      " 98%|█████████▊| 487/497 [25:25<00:31,  3.12s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3211.62 ms /   310 tokens\n",
      " 98%|█████████▊| 488/497 [25:28<00:28,  3.15s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3154.01 ms /   310 tokens\n",
      " 98%|█████████▊| 489/497 [25:31<00:25,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.61 ms /   306 tokens\n",
      " 99%|█████████▊| 490/497 [25:34<00:21,  3.13s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3153.48 ms /   313 tokens\n",
      " 99%|█████████▉| 491/497 [25:37<00:18,  3.14s/it]Llama.generate: 83 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3170.12 ms /   315 tokens\n",
      " 99%|█████████▉| 492/497 [25:41<00:15,  3.16s/it]Llama.generate: 84 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3134.00 ms /   308 tokens\n",
      " 99%|█████████▉| 493/497 [25:44<00:12,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3234.66 ms /   319 tokens\n",
      " 99%|█████████▉| 494/497 [25:47<00:09,  3.19s/it]Llama.generate: 83 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3181.98 ms /   313 tokens\n",
      "100%|█████████▉| 495/497 [25:50<00:06,  3.20s/it]Llama.generate: 83 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3044.10 ms /   310 tokens\n",
      "100%|█████████▉| 496/497 [25:53<00:03,  3.16s/it]Llama.generate: 83 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     907.34 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3039.77 ms /   309 tokens\n",
      "100%|██████████| 497/497 [25:56<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010731052984574111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With Mistral Instruct instead of Gemma"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:43:19.738687Z",
     "start_time": "2025-01-13T13:43:11.446537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_path = \"../test_datasets/rag-mini-bioasq/corpus\"\n",
    "vector_store_path = \"../test_chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "reload_vector_store = True # Add non existing documents\n",
    "reset_vector_store = False # Remove previous documents\n",
    "\n",
    "# splitter = None\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    ")\n",
    "\n",
    "# delete previous model from memory\n",
    "del rag\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "llm = Mistral(model_download_path=model_download_path) # reset your notebook if it crashes (it probably means you don't have enough memory)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file: using device CUDA0 (NVIDIA GeForce RTX 3060) - 9411 MiB free\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from ../models\\Mistral-7B-Instruct-v0.3-GGUF\\models--MaziyarPanahi--Mistral-7B-Instruct-v0.3-GGUF\\snapshots\\ce89f595755a4bf2e2e05d155cc43cb847c78978\\Mistral-7B-Instruct-v0.3.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--mistralai--Mistral-7B-Instruc...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = ./imatrix.dat\n",
      "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 74\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:     48 '[control_46]' is not marked as EOG\n",
      "llm_load_vocab: control token:    624 '[control_622]' is not marked as EOG\n",
      "llm_load_vocab: control token:    216 '[control_214]' is not marked as EOG\n",
      "llm_load_vocab: control token:     40 '[control_38]' is not marked as EOG\n",
      "llm_load_vocab: control token:    322 '[control_320]' is not marked as EOG\n",
      "llm_load_vocab: control token:      4 '[/INST]' is not marked as EOG\n",
      "llm_load_vocab: control token:    366 '[control_364]' is not marked as EOG\n",
      "llm_load_vocab: control token:     32 '[control_30]' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: control token:    425 '[control_423]' is not marked as EOG\n",
      "llm_load_vocab: control token:     44 '[control_42]' is not marked as EOG\n",
      "llm_load_vocab: control token:      7 '[/AVAILABLE_TOOLS]' is not marked as EOG\n",
      "llm_load_vocab: control token:    206 '[control_204]' is not marked as EOG\n",
      "llm_load_vocab: control token:     22 '[control_20]' is not marked as EOG\n",
      "llm_load_vocab: control token:    241 '[control_239]' is not marked as EOG\n",
      "llm_load_vocab: control token:    112 '[control_110]' is not marked as EOG\n",
      "llm_load_vocab: control token:    398 '[control_396]' is not marked as EOG\n",
      "llm_load_vocab: control token:      5 '[TOOL_CALLS]' is not marked as EOG\n",
      "llm_load_vocab: control token:    655 '[control_653]' is not marked as EOG\n",
      "llm_load_vocab: control token:    725 '[control_723]' is not marked as EOG\n",
      "llm_load_vocab: control token:    340 '[control_338]' is not marked as EOG\n",
      "llm_load_vocab: control token:    523 '[control_521]' is not marked as EOG\n",
      "llm_load_vocab: control token:    242 '[control_240]' is not marked as EOG\n",
      "llm_load_vocab: control token:      3 '[INST]' is not marked as EOG\n",
      "llm_load_vocab: control token:    364 '[control_362]' is not marked as EOG\n",
      "llm_load_vocab: control token:     38 '[control_36]' is not marked as EOG\n",
      "llm_load_vocab: control token:    333 '[control_331]' is not marked as EOG\n",
      "llm_load_vocab: control token:    530 '[control_528]' is not marked as EOG\n",
      "llm_load_vocab: control token:    251 '[control_249]' is not marked as EOG\n",
      "llm_load_vocab: control token:      6 '[AVAILABLE_TOOLS]' is not marked as EOG\n",
      "llm_load_vocab: control token:     63 '[control_61]' is not marked as EOG\n",
      "llm_load_vocab: control token:    346 '[control_344]' is not marked as EOG\n",
      "llm_load_vocab: control token:     12 '[control_10]' is not marked as EOG\n",
      "llm_load_vocab: control token:    409 '[control_407]' is not marked as EOG\n",
      "llm_load_vocab: control token:      8 '[TOOL_RESULTS]' is not marked as EOG\n",
      "llm_load_vocab: control token:    694 '[control_692]' is not marked as EOG\n",
      "llm_load_vocab: control token:    153 '[control_151]' is not marked as EOG\n",
      "llm_load_vocab: control token:    374 '[control_372]' is not marked as EOG\n",
      "llm_load_vocab: control token:      9 '[/TOOL_RESULTS]' is not marked as EOG\n",
      "llm_load_vocab: control token:     59 '[control_57]' is not marked as EOG\n",
      "llm_load_vocab: control token:    191 '[control_189]' is not marked as EOG\n",
      "llm_load_vocab: control token:    284 '[control_282]' is not marked as EOG\n",
      "llm_load_vocab: control token:     10 '[control_8]' is not marked as EOG\n",
      "llm_load_vocab: control token:     58 '[control_56]' is not marked as EOG\n",
      "llm_load_vocab: control token:    190 '[control_188]' is not marked as EOG\n",
      "llm_load_vocab: control token:    285 '[control_283]' is not marked as EOG\n",
      "llm_load_vocab: control token:     11 '[control_9]' is not marked as EOG\n",
      "llm_load_vocab: control token:     62 '[control_60]' is not marked as EOG\n",
      "llm_load_vocab: control token:    347 '[control_345]' is not marked as EOG\n",
      "llm_load_vocab: control token:     13 '[control_11]' is not marked as EOG\n",
      "llm_load_vocab: control token:    348 '[control_346]' is not marked as EOG\n",
      "llm_load_vocab: control token:     14 '[control_12]' is not marked as EOG\n",
      "llm_load_vocab: control token:    349 '[control_347]' is not marked as EOG\n",
      "llm_load_vocab: control token:     15 '[control_13]' is not marked as EOG\n",
      "llm_load_vocab: control token:    698 '[control_696]' is not marked as EOG\n",
      "llm_load_vocab: control token:    405 '[control_403]' is not marked as EOG\n",
      "llm_load_vocab: control token:    157 '[control_155]' is not marked as EOG\n",
      "llm_load_vocab: control token:     60 '[control_58]' is not marked as EOG\n",
      "llm_load_vocab: control token:    342 '[control_340]' is not marked as EOG\n",
      "llm_load_vocab: control token:     16 '[control_14]' is not marked as EOG\n",
      "llm_load_vocab: control token:    699 '[control_697]' is not marked as EOG\n",
      "llm_load_vocab: control token:    404 '[control_402]' is not marked as EOG\n",
      "llm_load_vocab: control token:    156 '[control_154]' is not marked as EOG\n",
      "llm_load_vocab: control token:     61 '[control_59]' is not marked as EOG\n",
      "llm_load_vocab: control token:    343 '[control_341]' is not marked as EOG\n",
      "llm_load_vocab: control token:     17 '[control_15]' is not marked as EOG\n",
      "llm_load_vocab: control token:    344 '[control_342]' is not marked as EOG\n",
      "llm_load_vocab: control token:     18 '[control_16]' is not marked as EOG\n",
      "llm_load_vocab: control token:    345 '[control_343]' is not marked as EOG\n",
      "llm_load_vocab: control token:     19 '[control_17]' is not marked as EOG\n",
      "llm_load_vocab: control token:    161 '[control_159]' is not marked as EOG\n",
      "llm_load_vocab: control token:     56 '[control_54]' is not marked as EOG\n",
      "llm_load_vocab: control token:     20 '[control_18]' is not marked as EOG\n",
      "llm_load_vocab: control token:     57 '[control_55]' is not marked as EOG\n",
      "llm_load_vocab: control token:    160 '[control_158]' is not marked as EOG\n",
      "llm_load_vocab: control token:     21 '[control_19]' is not marked as EOG\n",
      "llm_load_vocab: control token:    207 '[control_205]' is not marked as EOG\n",
      "llm_load_vocab: control token:     23 '[control_21]' is not marked as EOG\n",
      "llm_load_vocab: control token:     24 '[control_22]' is not marked as EOG\n",
      "llm_load_vocab: control token:    208 '[control_206]' is not marked as EOG\n",
      "llm_load_vocab: control token:    209 '[control_207]' is not marked as EOG\n",
      "llm_load_vocab: control token:     25 '[control_23]' is not marked as EOG\n",
      "llm_load_vocab: control token:    380 '[control_378]' is not marked as EOG\n",
      "llm_load_vocab: control token:     26 '[control_24]' is not marked as EOG\n",
      "llm_load_vocab: control token:    202 '[control_200]' is not marked as EOG\n",
      "llm_load_vocab: control token:    381 '[control_379]' is not marked as EOG\n",
      "llm_load_vocab: control token:    203 '[control_201]' is not marked as EOG\n",
      "llm_load_vocab: control token:     27 '[control_25]' is not marked as EOG\n",
      "llm_load_vocab: control token:     28 '[control_26]' is not marked as EOG\n",
      "llm_load_vocab: control token:    204 '[control_202]' is not marked as EOG\n",
      "llm_load_vocab: control token:    205 '[control_203]' is not marked as EOG\n",
      "llm_load_vocab: control token:     29 '[control_27]' is not marked as EOG\n",
      "llm_load_vocab: control token:    376 '[control_374]' is not marked as EOG\n",
      "llm_load_vocab: control token:     30 '[control_28]' is not marked as EOG\n",
      "llm_load_vocab: control token:    377 '[control_375]' is not marked as EOG\n",
      "llm_load_vocab: control token:     31 '[control_29]' is not marked as EOG\n",
      "llm_load_vocab: control token:    367 '[control_365]' is not marked as EOG\n",
      "llm_load_vocab: control token:     33 '[control_31]' is not marked as EOG\n",
      "llm_load_vocab: control token:    368 '[control_366]' is not marked as EOG\n",
      "llm_load_vocab: control token:     34 '[control_32]' is not marked as EOG\n",
      "llm_load_vocab: control token:    369 '[control_367]' is not marked as EOG\n",
      "llm_load_vocab: control token:     35 '[control_33]' is not marked as EOG\n",
      "llm_load_vocab: control token:    362 '[control_360]' is not marked as EOG\n",
      "llm_load_vocab: control token:    220 '[control_218]' is not marked as EOG\n",
      "llm_load_vocab: control token:     36 '[control_34]' is not marked as EOG\n",
      "llm_load_vocab: control token:    363 '[control_361]' is not marked as EOG\n",
      "llm_load_vocab: control token:    221 '[control_219]' is not marked as EOG\n",
      "llm_load_vocab: control token:     37 '[control_35]' is not marked as EOG\n",
      "llm_load_vocab: control token:    365 '[control_363]' is not marked as EOG\n",
      "llm_load_vocab: control token:     39 '[control_37]' is not marked as EOG\n",
      "llm_load_vocab: control token:     41 '[control_39]' is not marked as EOG\n",
      "llm_load_vocab: control token:    217 '[control_215]' is not marked as EOG\n",
      "llm_load_vocab: control token:     42 '[control_40]' is not marked as EOG\n",
      "llm_load_vocab: control token:     43 '[control_41]' is not marked as EOG\n",
      "llm_load_vocab: control token:     45 '[control_43]' is not marked as EOG\n",
      "llm_load_vocab: control token:    481 '[control_479]' is not marked as EOG\n",
      "llm_load_vocab: control token:     46 '[control_44]' is not marked as EOG\n",
      "llm_load_vocab: control token:    480 '[control_478]' is not marked as EOG\n",
      "llm_load_vocab: control token:     47 '[control_45]' is not marked as EOG\n",
      "llm_load_vocab: control token:     49 '[control_47]' is not marked as EOG\n",
      "llm_load_vocab: control token:    477 '[control_475]' is not marked as EOG\n",
      "llm_load_vocab: control token:     50 '[control_48]' is not marked as EOG\n",
      "llm_load_vocab: control token:    476 '[control_474]' is not marked as EOG\n",
      "llm_load_vocab: control token:     51 '[control_49]' is not marked as EOG\n",
      "llm_load_vocab: control token:     52 '[control_50]' is not marked as EOG\n",
      "llm_load_vocab: control token:     53 '[control_51]' is not marked as EOG\n",
      "llm_load_vocab: control token:    411 '[control_409]' is not marked as EOG\n",
      "llm_load_vocab: control token:     54 '[control_52]' is not marked as EOG\n",
      "llm_load_vocab: control token:    410 '[control_408]' is not marked as EOG\n",
      "llm_load_vocab: control token:     55 '[control_53]' is not marked as EOG\n",
      "llm_load_vocab: control token:     64 '[control_62]' is not marked as EOG\n",
      "llm_load_vocab: control token:     65 '[control_63]' is not marked as EOG\n",
      "llm_load_vocab: control token:     66 '[control_64]' is not marked as EOG\n",
      "llm_load_vocab: control token:     67 '[control_65]' is not marked as EOG\n",
      "llm_load_vocab: control token:     68 '[control_66]' is not marked as EOG\n",
      "llm_load_vocab: control token:     69 '[control_67]' is not marked as EOG\n",
      "llm_load_vocab: control token:     70 '[control_68]' is not marked as EOG\n",
      "llm_load_vocab: control token:     71 '[control_69]' is not marked as EOG\n",
      "llm_load_vocab: control token:     72 '[control_70]' is not marked as EOG\n",
      "llm_load_vocab: control token:     73 '[control_71]' is not marked as EOG\n",
      "llm_load_vocab: control token:     74 '[control_72]' is not marked as EOG\n",
      "llm_load_vocab: control token:     75 '[control_73]' is not marked as EOG\n",
      "llm_load_vocab: control token:     76 '[control_74]' is not marked as EOG\n",
      "llm_load_vocab: control token:     77 '[control_75]' is not marked as EOG\n",
      "llm_load_vocab: control token:     78 '[control_76]' is not marked as EOG\n",
      "llm_load_vocab: control token:     79 '[control_77]' is not marked as EOG\n",
      "llm_load_vocab: control token:     80 '[control_78]' is not marked as EOG\n",
      "llm_load_vocab: control token:     81 '[control_79]' is not marked as EOG\n",
      "llm_load_vocab: control token:     82 '[control_80]' is not marked as EOG\n",
      "llm_load_vocab: control token:     83 '[control_81]' is not marked as EOG\n",
      "llm_load_vocab: control token:     84 '[control_82]' is not marked as EOG\n",
      "llm_load_vocab: control token:     85 '[control_83]' is not marked as EOG\n",
      "llm_load_vocab: control token:     86 '[control_84]' is not marked as EOG\n",
      "llm_load_vocab: control token:     87 '[control_85]' is not marked as EOG\n",
      "llm_load_vocab: control token:     88 '[control_86]' is not marked as EOG\n",
      "llm_load_vocab: control token:     89 '[control_87]' is not marked as EOG\n",
      "llm_load_vocab: control token:     90 '[control_88]' is not marked as EOG\n",
      "llm_load_vocab: control token:     91 '[control_89]' is not marked as EOG\n",
      "llm_load_vocab: control token:     92 '[control_90]' is not marked as EOG\n",
      "llm_load_vocab: control token:     93 '[control_91]' is not marked as EOG\n",
      "llm_load_vocab: control token:     94 '[control_92]' is not marked as EOG\n",
      "llm_load_vocab: control token:     95 '[control_93]' is not marked as EOG\n",
      "llm_load_vocab: control token:     96 '[control_94]' is not marked as EOG\n",
      "llm_load_vocab: control token:     97 '[control_95]' is not marked as EOG\n",
      "llm_load_vocab: control token:     98 '[control_96]' is not marked as EOG\n",
      "llm_load_vocab: control token:     99 '[control_97]' is not marked as EOG\n",
      "llm_load_vocab: control token:    100 '[control_98]' is not marked as EOG\n",
      "llm_load_vocab: control token:    101 '[control_99]' is not marked as EOG\n",
      "llm_load_vocab: control token:    454 '[control_452]' is not marked as EOG\n",
      "llm_load_vocab: control token:    102 '[control_100]' is not marked as EOG\n",
      "llm_load_vocab: control token:    455 '[control_453]' is not marked as EOG\n",
      "llm_load_vocab: control token:    103 '[control_101]' is not marked as EOG\n",
      "llm_load_vocab: control token:    452 '[control_450]' is not marked as EOG\n",
      "llm_load_vocab: control token:    104 '[control_102]' is not marked as EOG\n",
      "llm_load_vocab: control token:    390 '[control_388]' is not marked as EOG\n",
      "llm_load_vocab: control token:    453 '[control_451]' is not marked as EOG\n",
      "llm_load_vocab: control token:    105 '[control_103]' is not marked as EOG\n",
      "llm_load_vocab: control token:    391 '[control_389]' is not marked as EOG\n",
      "llm_load_vocab: control token:    458 '[control_456]' is not marked as EOG\n",
      "llm_load_vocab: control token:    106 '[control_104]' is not marked as EOG\n",
      "llm_load_vocab: control token:    459 '[control_457]' is not marked as EOG\n",
      "llm_load_vocab: control token:    107 '[control_105]' is not marked as EOG\n",
      "llm_load_vocab: control token:    456 '[control_454]' is not marked as EOG\n",
      "llm_load_vocab: control token:    108 '[control_106]' is not marked as EOG\n",
      "llm_load_vocab: control token:    457 '[control_455]' is not marked as EOG\n",
      "llm_load_vocab: control token:    109 '[control_107]' is not marked as EOG\n",
      "llm_load_vocab: control token:    110 '[control_108]' is not marked as EOG\n",
      "llm_load_vocab: control token:    384 '[control_382]' is not marked as EOG\n",
      "llm_load_vocab: control token:    111 '[control_109]' is not marked as EOG\n",
      "llm_load_vocab: control token:    385 '[control_383]' is not marked as EOG\n",
      "llm_load_vocab: control token:    240 '[control_238]' is not marked as EOG\n",
      "llm_load_vocab: control token:    113 '[control_111]' is not marked as EOG\n",
      "llm_load_vocab: control token:    399 '[control_397]' is not marked as EOG\n",
      "llm_load_vocab: control token:    114 '[control_112]' is not marked as EOG\n",
      "llm_load_vocab: control token:    396 '[control_394]' is not marked as EOG\n",
      "llm_load_vocab: control token:    115 '[control_113]' is not marked as EOG\n",
      "llm_load_vocab: control token:    397 '[control_395]' is not marked as EOG\n",
      "llm_load_vocab: control token:    116 '[control_114]' is not marked as EOG\n",
      "llm_load_vocab: control token:    394 '[control_392]' is not marked as EOG\n",
      "llm_load_vocab: control token:    395 '[control_393]' is not marked as EOG\n",
      "llm_load_vocab: control token:    117 '[control_115]' is not marked as EOG\n",
      "llm_load_vocab: control token:    450 '[control_448]' is not marked as EOG\n",
      "llm_load_vocab: control token:    657 '[control_655]' is not marked as EOG\n",
      "llm_load_vocab: control token:    118 '[control_116]' is not marked as EOG\n",
      "llm_load_vocab: control token:    392 '[control_390]' is not marked as EOG\n",
      "llm_load_vocab: control token:    656 '[control_654]' is not marked as EOG\n",
      "llm_load_vocab: control token:    451 '[control_449]' is not marked as EOG\n",
      "llm_load_vocab: control token:    119 '[control_117]' is not marked as EOG\n",
      "llm_load_vocab: control token:    393 '[control_391]' is not marked as EOG\n",
      "llm_load_vocab: control token:    448 '[control_446]' is not marked as EOG\n",
      "llm_load_vocab: control token:    120 '[control_118]' is not marked as EOG\n",
      "llm_load_vocab: control token:    233 '[control_231]' is not marked as EOG\n",
      "llm_load_vocab: control token:    449 '[control_447]' is not marked as EOG\n",
      "llm_load_vocab: control token:    121 '[control_119]' is not marked as EOG\n",
      "llm_load_vocab: control token:    232 '[control_230]' is not marked as EOG\n",
      "llm_load_vocab: control token:    122 '[control_120]' is not marked as EOG\n",
      "llm_load_vocab: control token:    123 '[control_121]' is not marked as EOG\n",
      "llm_load_vocab: control token:    124 '[control_122]' is not marked as EOG\n",
      "llm_load_vocab: control token:    125 '[control_123]' is not marked as EOG\n",
      "llm_load_vocab: control token:    126 '[control_124]' is not marked as EOG\n",
      "llm_load_vocab: control token:    127 '[control_125]' is not marked as EOG\n",
      "llm_load_vocab: control token:    510 '[control_508]' is not marked as EOG\n",
      "llm_load_vocab: control token:    231 '[control_229]' is not marked as EOG\n",
      "llm_load_vocab: control token:    128 '[control_126]' is not marked as EOG\n",
      "llm_load_vocab: control token:    129 '[control_127]' is not marked as EOG\n",
      "llm_load_vocab: control token:    230 '[control_228]' is not marked as EOG\n",
      "llm_load_vocab: control token:    511 '[control_509]' is not marked as EOG\n",
      "llm_load_vocab: control token:    130 '[control_128]' is not marked as EOG\n",
      "llm_load_vocab: control token:    229 '[control_227]' is not marked as EOG\n",
      "llm_load_vocab: control token:    508 '[control_506]' is not marked as EOG\n",
      "llm_load_vocab: control token:    131 '[control_129]' is not marked as EOG\n",
      "llm_load_vocab: control token:    228 '[control_226]' is not marked as EOG\n",
      "llm_load_vocab: control token:    509 '[control_507]' is not marked as EOG\n",
      "llm_load_vocab: control token:    132 '[control_130]' is not marked as EOG\n",
      "llm_load_vocab: control token:    261 '[control_259]' is not marked as EOG\n",
      "llm_load_vocab: control token:    133 '[control_131]' is not marked as EOG\n",
      "llm_load_vocab: control token:    260 '[control_258]' is not marked as EOG\n",
      "llm_load_vocab: control token:    520 '[control_518]' is not marked as EOG\n",
      "llm_load_vocab: control token:    134 '[control_132]' is not marked as EOG\n",
      "llm_load_vocab: control token:    135 '[control_133]' is not marked as EOG\n",
      "llm_load_vocab: control token:    521 '[control_519]' is not marked as EOG\n",
      "llm_load_vocab: control token:    136 '[control_134]' is not marked as EOG\n",
      "llm_load_vocab: control token:    137 '[control_135]' is not marked as EOG\n",
      "llm_load_vocab: control token:    138 '[control_136]' is not marked as EOG\n",
      "llm_load_vocab: control token:    139 '[control_137]' is not marked as EOG\n",
      "llm_load_vocab: control token:    514 '[control_512]' is not marked as EOG\n",
      "llm_load_vocab: control token:    140 '[control_138]' is not marked as EOG\n",
      "llm_load_vocab: control token:    253 '[control_251]' is not marked as EOG\n",
      "llm_load_vocab: control token:    515 '[control_513]' is not marked as EOG\n",
      "llm_load_vocab: control token:    141 '[control_139]' is not marked as EOG\n",
      "llm_load_vocab: control token:    252 '[control_250]' is not marked as EOG\n",
      "llm_load_vocab: control token:    414 '[control_412]' is not marked as EOG\n",
      "llm_load_vocab: control token:    142 '[control_140]' is not marked as EOG\n",
      "llm_load_vocab: control token:    415 '[control_413]' is not marked as EOG\n",
      "llm_load_vocab: control token:    143 '[control_141]' is not marked as EOG\n",
      "llm_load_vocab: control token:    412 '[control_410]' is not marked as EOG\n",
      "llm_load_vocab: control token:    144 '[control_142]' is not marked as EOG\n",
      "llm_load_vocab: control token:    413 '[control_411]' is not marked as EOG\n",
      "llm_load_vocab: control token:    145 '[control_143]' is not marked as EOG\n",
      "llm_load_vocab: control token:    418 '[control_416]' is not marked as EOG\n",
      "llm_load_vocab: control token:    146 '[control_144]' is not marked as EOG\n",
      "llm_load_vocab: control token:    419 '[control_417]' is not marked as EOG\n",
      "llm_load_vocab: control token:    147 '[control_145]' is not marked as EOG\n",
      "llm_load_vocab: control token:    416 '[control_414]' is not marked as EOG\n",
      "llm_load_vocab: control token:    148 '[control_146]' is not marked as EOG\n",
      "llm_load_vocab: control token:    417 '[control_415]' is not marked as EOG\n",
      "llm_load_vocab: control token:    149 '[control_147]' is not marked as EOG\n",
      "llm_load_vocab: control token:    150 '[control_148]' is not marked as EOG\n",
      "llm_load_vocab: control token:    151 '[control_149]' is not marked as EOG\n",
      "llm_load_vocab: control token:    695 '[control_693]' is not marked as EOG\n",
      "llm_load_vocab: control token:    408 '[control_406]' is not marked as EOG\n",
      "llm_load_vocab: control token:    152 '[control_150]' is not marked as EOG\n",
      "llm_load_vocab: control token:    406 '[control_404]' is not marked as EOG\n",
      "llm_load_vocab: control token:    693 '[control_691]' is not marked as EOG\n",
      "llm_load_vocab: control token:    154 '[control_152]' is not marked as EOG\n",
      "llm_load_vocab: control token:    692 '[control_690]' is not marked as EOG\n",
      "llm_load_vocab: control token:    407 '[control_405]' is not marked as EOG\n",
      "llm_load_vocab: control token:    155 '[control_153]' is not marked as EOG\n",
      "llm_load_vocab: control token:    697 '[control_695]' is not marked as EOG\n",
      "llm_load_vocab: control token:    402 '[control_400]' is not marked as EOG\n",
      "llm_load_vocab: control token:    158 '[control_156]' is not marked as EOG\n",
      "llm_load_vocab: control token:    696 '[control_694]' is not marked as EOG\n",
      "llm_load_vocab: control token:    403 '[control_401]' is not marked as EOG\n",
      "llm_load_vocab: control token:    159 '[control_157]' is not marked as EOG\n",
      "llm_load_vocab: control token:    162 '[control_160]' is not marked as EOG\n",
      "llm_load_vocab: control token:    163 '[control_161]' is not marked as EOG\n",
      "llm_load_vocab: control token:    164 '[control_162]' is not marked as EOG\n",
      "llm_load_vocab: control token:    165 '[control_163]' is not marked as EOG\n",
      "llm_load_vocab: control token:    166 '[control_164]' is not marked as EOG\n",
      "llm_load_vocab: control token:    167 '[control_165]' is not marked as EOG\n",
      "llm_load_vocab: control token:    761 '[control_759]' is not marked as EOG\n",
      "llm_load_vocab: control token:    168 '[control_166]' is not marked as EOG\n",
      "llm_load_vocab: control token:    760 '[control_758]' is not marked as EOG\n",
      "llm_load_vocab: control token:    169 '[control_167]' is not marked as EOG\n",
      "llm_load_vocab: control token:    759 '[control_757]' is not marked as EOG\n",
      "llm_load_vocab: control token:    170 '[control_168]' is not marked as EOG\n",
      "llm_load_vocab: control token:    758 '[control_756]' is not marked as EOG\n",
      "llm_load_vocab: control token:    171 '[control_169]' is not marked as EOG\n",
      "llm_load_vocab: control token:    464 '[control_462]' is not marked as EOG\n",
      "llm_load_vocab: control token:    172 '[control_170]' is not marked as EOG\n",
      "llm_load_vocab: control token:    465 '[control_463]' is not marked as EOG\n",
      "llm_load_vocab: control token:    173 '[control_171]' is not marked as EOG\n",
      "llm_load_vocab: control token:    462 '[control_460]' is not marked as EOG\n",
      "llm_load_vocab: control token:    174 '[control_172]' is not marked as EOG\n",
      "llm_load_vocab: control token:    463 '[control_461]' is not marked as EOG\n",
      "llm_load_vocab: control token:    175 '[control_173]' is not marked as EOG\n",
      "llm_load_vocab: control token:    468 '[control_466]' is not marked as EOG\n",
      "llm_load_vocab: control token:    176 '[control_174]' is not marked as EOG\n",
      "llm_load_vocab: control token:    469 '[control_467]' is not marked as EOG\n",
      "llm_load_vocab: control token:    177 '[control_175]' is not marked as EOG\n",
      "llm_load_vocab: control token:    466 '[control_464]' is not marked as EOG\n",
      "llm_load_vocab: control token:    178 '[control_176]' is not marked as EOG\n",
      "llm_load_vocab: control token:    467 '[control_465]' is not marked as EOG\n",
      "llm_load_vocab: control token:    179 '[control_177]' is not marked as EOG\n",
      "llm_load_vocab: control token:    180 '[control_178]' is not marked as EOG\n",
      "llm_load_vocab: control token:    181 '[control_179]' is not marked as EOG\n",
      "llm_load_vocab: control token:    182 '[control_180]' is not marked as EOG\n",
      "llm_load_vocab: control token:    183 '[control_181]' is not marked as EOG\n",
      "llm_load_vocab: control token:    184 '[control_182]' is not marked as EOG\n",
      "llm_load_vocab: control token:    291 '[control_289]' is not marked as EOG\n",
      "llm_load_vocab: control token:    185 '[control_183]' is not marked as EOG\n",
      "llm_load_vocab: control token:    290 '[control_288]' is not marked as EOG\n",
      "llm_load_vocab: control token:    186 '[control_184]' is not marked as EOG\n",
      "llm_load_vocab: control token:    187 '[control_185]' is not marked as EOG\n",
      "llm_load_vocab: control token:    188 '[control_186]' is not marked as EOG\n",
      "llm_load_vocab: control token:    189 '[control_187]' is not marked as EOG\n",
      "llm_load_vocab: control token:    192 '[control_190]' is not marked as EOG\n",
      "llm_load_vocab: control token:    318 '[control_316]' is not marked as EOG\n",
      "llm_load_vocab: control token:    193 '[control_191]' is not marked as EOG\n",
      "llm_load_vocab: control token:    319 '[control_317]' is not marked as EOG\n",
      "llm_load_vocab: control token:    194 '[control_192]' is not marked as EOG\n",
      "llm_load_vocab: control token:    316 '[control_314]' is not marked as EOG\n",
      "llm_load_vocab: control token:    195 '[control_193]' is not marked as EOG\n",
      "llm_load_vocab: control token:    317 '[control_315]' is not marked as EOG\n",
      "llm_load_vocab: control token:    196 '[control_194]' is not marked as EOG\n",
      "llm_load_vocab: control token:    314 '[control_312]' is not marked as EOG\n",
      "llm_load_vocab: control token:    315 '[control_313]' is not marked as EOG\n",
      "llm_load_vocab: control token:    197 '[control_195]' is not marked as EOG\n",
      "llm_load_vocab: control token:    198 '[control_196]' is not marked as EOG\n",
      "llm_load_vocab: control token:    312 '[control_310]' is not marked as EOG\n",
      "llm_load_vocab: control token:    551 '[control_549]' is not marked as EOG\n",
      "llm_load_vocab: control token:    270 '[control_268]' is not marked as EOG\n",
      "llm_load_vocab: control token:    199 '[control_197]' is not marked as EOG\n",
      "llm_load_vocab: control token:    313 '[control_311]' is not marked as EOG\n",
      "llm_load_vocab: control token:    550 '[control_548]' is not marked as EOG\n",
      "llm_load_vocab: control token:    271 '[control_269]' is not marked as EOG\n",
      "llm_load_vocab: control token:    268 '[control_266]' is not marked as EOG\n",
      "llm_load_vocab: control token:    549 '[control_547]' is not marked as EOG\n",
      "llm_load_vocab: control token:    200 '[control_198]' is not marked as EOG\n",
      "llm_load_vocab: control token:    548 '[control_546]' is not marked as EOG\n",
      "llm_load_vocab: control token:    269 '[control_267]' is not marked as EOG\n",
      "llm_load_vocab: control token:    201 '[control_199]' is not marked as EOG\n",
      "llm_load_vocab: control token:    372 '[control_370]' is not marked as EOG\n",
      "llm_load_vocab: control token:    210 '[control_208]' is not marked as EOG\n",
      "llm_load_vocab: control token:    373 '[control_371]' is not marked as EOG\n",
      "llm_load_vocab: control token:    211 '[control_209]' is not marked as EOG\n",
      "llm_load_vocab: control token:    370 '[control_368]' is not marked as EOG\n",
      "llm_load_vocab: control token:    212 '[control_210]' is not marked as EOG\n",
      "llm_load_vocab: control token:    371 '[control_369]' is not marked as EOG\n",
      "llm_load_vocab: control token:    213 '[control_211]' is not marked as EOG\n",
      "llm_load_vocab: control token:    214 '[control_212]' is not marked as EOG\n",
      "llm_load_vocab: control token:    215 '[control_213]' is not marked as EOG\n",
      "llm_load_vocab: control token:    218 '[control_216]' is not marked as EOG\n",
      "llm_load_vocab: control token:    219 '[control_217]' is not marked as EOG\n",
      "llm_load_vocab: control token:    222 '[control_220]' is not marked as EOG\n",
      "llm_load_vocab: control token:    503 '[control_501]' is not marked as EOG\n",
      "llm_load_vocab: control token:    502 '[control_500]' is not marked as EOG\n",
      "llm_load_vocab: control token:    223 '[control_221]' is not marked as EOG\n",
      "llm_load_vocab: control token:    224 '[control_222]' is not marked as EOG\n",
      "llm_load_vocab: control token:    505 '[control_503]' is not marked as EOG\n",
      "llm_load_vocab: control token:    504 '[control_502]' is not marked as EOG\n",
      "llm_load_vocab: control token:    225 '[control_223]' is not marked as EOG\n",
      "llm_load_vocab: control token:    226 '[control_224]' is not marked as EOG\n",
      "llm_load_vocab: control token:    507 '[control_505]' is not marked as EOG\n",
      "llm_load_vocab: control token:    227 '[control_225]' is not marked as EOG\n",
      "llm_load_vocab: control token:    506 '[control_504]' is not marked as EOG\n",
      "llm_load_vocab: control token:    660 '[control_658]' is not marked as EOG\n",
      "llm_load_vocab: control token:    447 '[control_445]' is not marked as EOG\n",
      "llm_load_vocab: control token:    234 '[control_232]' is not marked as EOG\n",
      "llm_load_vocab: control token:    661 '[control_659]' is not marked as EOG\n",
      "llm_load_vocab: control token:    446 '[control_444]' is not marked as EOG\n",
      "llm_load_vocab: control token:    235 '[control_233]' is not marked as EOG\n",
      "llm_load_vocab: control token:    445 '[control_443]' is not marked as EOG\n",
      "llm_load_vocab: control token:    236 '[control_234]' is not marked as EOG\n",
      "llm_load_vocab: control token:    444 '[control_442]' is not marked as EOG\n",
      "llm_load_vocab: control token:    237 '[control_235]' is not marked as EOG\n",
      "llm_load_vocab: control token:    443 '[control_441]' is not marked as EOG\n",
      "llm_load_vocab: control token:    238 '[control_236]' is not marked as EOG\n",
      "llm_load_vocab: control token:    401 '[control_399]' is not marked as EOG\n",
      "llm_load_vocab: control token:    442 '[control_440]' is not marked as EOG\n",
      "llm_load_vocab: control token:    239 '[control_237]' is not marked as EOG\n",
      "llm_load_vocab: control token:    400 '[control_398]' is not marked as EOG\n",
      "llm_load_vocab: control token:    341 '[control_339]' is not marked as EOG\n",
      "llm_load_vocab: control token:    724 '[control_722]' is not marked as EOG\n",
      "llm_load_vocab: control token:    522 '[control_520]' is not marked as EOG\n",
      "llm_load_vocab: control token:    243 '[control_241]' is not marked as EOG\n",
      "llm_load_vocab: control token:    525 '[control_523]' is not marked as EOG\n",
      "llm_load_vocab: control token:    244 '[control_242]' is not marked as EOG\n",
      "llm_load_vocab: control token:    245 '[control_243]' is not marked as EOG\n",
      "llm_load_vocab: control token:    524 '[control_522]' is not marked as EOG\n",
      "llm_load_vocab: control token:    246 '[control_244]' is not marked as EOG\n",
      "llm_load_vocab: control token:    527 '[control_525]' is not marked as EOG\n",
      "llm_load_vocab: control token:    526 '[control_524]' is not marked as EOG\n",
      "llm_load_vocab: control token:    247 '[control_245]' is not marked as EOG\n",
      "llm_load_vocab: control token:    529 '[control_527]' is not marked as EOG\n",
      "llm_load_vocab: control token:    248 '[control_246]' is not marked as EOG\n",
      "llm_load_vocab: control token:    249 '[control_247]' is not marked as EOG\n",
      "llm_load_vocab: control token:    528 '[control_526]' is not marked as EOG\n",
      "llm_load_vocab: control token:    332 '[control_330]' is not marked as EOG\n",
      "llm_load_vocab: control token:    531 '[control_529]' is not marked as EOG\n",
      "llm_load_vocab: control token:    250 '[control_248]' is not marked as EOG\n",
      "llm_load_vocab: control token:    254 '[control_252]' is not marked as EOG\n",
      "llm_load_vocab: control token:    513 '[control_511]' is not marked as EOG\n",
      "llm_load_vocab: control token:    255 '[control_253]' is not marked as EOG\n",
      "llm_load_vocab: control token:    512 '[control_510]' is not marked as EOG\n",
      "llm_load_vocab: control token:    519 '[control_517]' is not marked as EOG\n",
      "llm_load_vocab: control token:    256 '[control_254]' is not marked as EOG\n",
      "llm_load_vocab: control token:    518 '[control_516]' is not marked as EOG\n",
      "llm_load_vocab: control token:    257 '[control_255]' is not marked as EOG\n",
      "llm_load_vocab: control token:    517 '[control_515]' is not marked as EOG\n",
      "llm_load_vocab: control token:    258 '[control_256]' is not marked as EOG\n",
      "llm_load_vocab: control token:    516 '[control_514]' is not marked as EOG\n",
      "llm_load_vocab: control token:    259 '[control_257]' is not marked as EOG\n",
      "llm_load_vocab: control token:    320 '[control_318]' is not marked as EOG\n",
      "llm_load_vocab: control token:    543 '[control_541]' is not marked as EOG\n",
      "llm_load_vocab: control token:    262 '[control_260]' is not marked as EOG\n",
      "llm_load_vocab: control token:    321 '[control_319]' is not marked as EOG\n",
      "llm_load_vocab: control token:    542 '[control_540]' is not marked as EOG\n",
      "llm_load_vocab: control token:    263 '[control_261]' is not marked as EOG\n",
      "llm_load_vocab: control token:    264 '[control_262]' is not marked as EOG\n",
      "llm_load_vocab: control token:    545 '[control_543]' is not marked as EOG\n",
      "llm_load_vocab: control token:    544 '[control_542]' is not marked as EOG\n",
      "llm_load_vocab: control token:    265 '[control_263]' is not marked as EOG\n",
      "llm_load_vocab: control token:    547 '[control_545]' is not marked as EOG\n",
      "llm_load_vocab: control token:    266 '[control_264]' is not marked as EOG\n",
      "llm_load_vocab: control token:    267 '[control_265]' is not marked as EOG\n",
      "llm_load_vocab: control token:    546 '[control_544]' is not marked as EOG\n",
      "llm_load_vocab: control token:    310 '[control_308]' is not marked as EOG\n",
      "llm_load_vocab: control token:    733 '[control_731]' is not marked as EOG\n",
      "llm_load_vocab: control token:    539 '[control_537]' is not marked as EOG\n",
      "llm_load_vocab: control token:    272 '[control_270]' is not marked as EOG\n",
      "llm_load_vocab: control token:    311 '[control_309]' is not marked as EOG\n",
      "llm_load_vocab: control token:    732 '[control_730]' is not marked as EOG\n",
      "llm_load_vocab: control token:    273 '[control_271]' is not marked as EOG\n",
      "llm_load_vocab: control token:    538 '[control_536]' is not marked as EOG\n",
      "llm_load_vocab: control token:    537 '[control_535]' is not marked as EOG\n",
      "llm_load_vocab: control token:    274 '[control_272]' is not marked as EOG\n",
      "llm_load_vocab: control token:    536 '[control_534]' is not marked as EOG\n",
      "llm_load_vocab: control token:    275 '[control_273]' is not marked as EOG\n",
      "llm_load_vocab: control token:    535 '[control_533]' is not marked as EOG\n",
      "llm_load_vocab: control token:    276 '[control_274]' is not marked as EOG\n",
      "llm_load_vocab: control token:    534 '[control_532]' is not marked as EOG\n",
      "llm_load_vocab: control token:    277 '[control_275]' is not marked as EOG\n",
      "llm_load_vocab: control token:    533 '[control_531]' is not marked as EOG\n",
      "llm_load_vocab: control token:    278 '[control_276]' is not marked as EOG\n",
      "llm_load_vocab: control token:    532 '[control_530]' is not marked as EOG\n",
      "llm_load_vocab: control token:    279 '[control_277]' is not marked as EOG\n",
      "llm_load_vocab: control token:    741 '[control_739]' is not marked as EOG\n",
      "llm_load_vocab: control token:    302 '[control_300]' is not marked as EOG\n",
      "llm_load_vocab: control token:    280 '[control_278]' is not marked as EOG\n",
      "llm_load_vocab: control token:    303 '[control_301]' is not marked as EOG\n",
      "llm_load_vocab: control token:    740 '[control_738]' is not marked as EOG\n",
      "llm_load_vocab: control token:    281 '[control_279]' is not marked as EOG\n",
      "llm_load_vocab: control token:    282 '[control_280]' is not marked as EOG\n",
      "llm_load_vocab: control token:    283 '[control_281]' is not marked as EOG\n",
      "llm_load_vocab: control token:    286 '[control_284]' is not marked as EOG\n",
      "llm_load_vocab: control token:    287 '[control_285]' is not marked as EOG\n",
      "llm_load_vocab: control token:    288 '[control_286]' is not marked as EOG\n",
      "llm_load_vocab: control token:    289 '[control_287]' is not marked as EOG\n",
      "llm_load_vocab: control token:    292 '[control_290]' is not marked as EOG\n",
      "llm_load_vocab: control token:    293 '[control_291]' is not marked as EOG\n",
      "llm_load_vocab: control token:    294 '[control_292]' is not marked as EOG\n",
      "llm_load_vocab: control token:    295 '[control_293]' is not marked as EOG\n",
      "llm_load_vocab: control token:    296 '[control_294]' is not marked as EOG\n",
      "llm_load_vocab: control token:    297 '[control_295]' is not marked as EOG\n",
      "llm_load_vocab: control token:    298 '[control_296]' is not marked as EOG\n",
      "llm_load_vocab: control token:    299 '[control_297]' is not marked as EOG\n",
      "llm_load_vocab: control token:    300 '[control_298]' is not marked as EOG\n",
      "llm_load_vocab: control token:    301 '[control_299]' is not marked as EOG\n",
      "llm_load_vocab: control token:    304 '[control_302]' is not marked as EOG\n",
      "llm_load_vocab: control token:    305 '[control_303]' is not marked as EOG\n",
      "llm_load_vocab: control token:    306 '[control_304]' is not marked as EOG\n",
      "llm_load_vocab: control token:    307 '[control_305]' is not marked as EOG\n",
      "llm_load_vocab: control token:    308 '[control_306]' is not marked as EOG\n",
      "llm_load_vocab: control token:    309 '[control_307]' is not marked as EOG\n",
      "llm_load_vocab: control token:    323 '[control_321]' is not marked as EOG\n",
      "llm_load_vocab: control token:    324 '[control_322]' is not marked as EOG\n",
      "llm_load_vocab: control token:    325 '[control_323]' is not marked as EOG\n",
      "llm_load_vocab: control token:    326 '[control_324]' is not marked as EOG\n",
      "llm_load_vocab: control token:    327 '[control_325]' is not marked as EOG\n",
      "llm_load_vocab: control token:    328 '[control_326]' is not marked as EOG\n",
      "llm_load_vocab: control token:    329 '[control_327]' is not marked as EOG\n",
      "llm_load_vocab: control token:    330 '[control_328]' is not marked as EOG\n",
      "llm_load_vocab: control token:    331 '[control_329]' is not marked as EOG\n",
      "llm_load_vocab: control token:    334 '[control_332]' is not marked as EOG\n",
      "llm_load_vocab: control token:    731 '[control_729]' is not marked as EOG\n",
      "llm_load_vocab: control token:    730 '[control_728]' is not marked as EOG\n",
      "llm_load_vocab: control token:    335 '[control_333]' is not marked as EOG\n",
      "llm_load_vocab: control token:    336 '[control_334]' is not marked as EOG\n",
      "llm_load_vocab: control token:    337 '[control_335]' is not marked as EOG\n",
      "llm_load_vocab: control token:    338 '[control_336]' is not marked as EOG\n",
      "llm_load_vocab: control token:    339 '[control_337]' is not marked as EOG\n",
      "llm_load_vocab: control token:    350 '[control_348]' is not marked as EOG\n",
      "llm_load_vocab: control token:    351 '[control_349]' is not marked as EOG\n",
      "llm_load_vocab: control token:    352 '[control_350]' is not marked as EOG\n",
      "llm_load_vocab: control token:    353 '[control_351]' is not marked as EOG\n",
      "llm_load_vocab: control token:    354 '[control_352]' is not marked as EOG\n",
      "llm_load_vocab: control token:    355 '[control_353]' is not marked as EOG\n",
      "llm_load_vocab: control token:    356 '[control_354]' is not marked as EOG\n",
      "llm_load_vocab: control token:    357 '[control_355]' is not marked as EOG\n",
      "llm_load_vocab: control token:    358 '[control_356]' is not marked as EOG\n",
      "llm_load_vocab: control token:    359 '[control_357]' is not marked as EOG\n",
      "llm_load_vocab: control token:    360 '[control_358]' is not marked as EOG\n",
      "llm_load_vocab: control token:    361 '[control_359]' is not marked as EOG\n",
      "llm_load_vocab: control token:    375 '[control_373]' is not marked as EOG\n",
      "llm_load_vocab: control token:    378 '[control_376]' is not marked as EOG\n",
      "llm_load_vocab: control token:    379 '[control_377]' is not marked as EOG\n",
      "llm_load_vocab: control token:    460 '[control_458]' is not marked as EOG\n",
      "llm_load_vocab: control token:    382 '[control_380]' is not marked as EOG\n",
      "llm_load_vocab: control token:    461 '[control_459]' is not marked as EOG\n",
      "llm_load_vocab: control token:    383 '[control_381]' is not marked as EOG\n",
      "llm_load_vocab: control token:    386 '[control_384]' is not marked as EOG\n",
      "llm_load_vocab: control token:    387 '[control_385]' is not marked as EOG\n",
      "llm_load_vocab: control token:    388 '[control_386]' is not marked as EOG\n",
      "llm_load_vocab: control token:    389 '[control_387]' is not marked as EOG\n",
      "llm_load_vocab: control token:    420 '[control_418]' is not marked as EOG\n",
      "llm_load_vocab: control token:    421 '[control_419]' is not marked as EOG\n",
      "llm_load_vocab: control token:    422 '[control_420]' is not marked as EOG\n",
      "llm_load_vocab: control token:    423 '[control_421]' is not marked as EOG\n",
      "llm_load_vocab: control token:    424 '[control_422]' is not marked as EOG\n",
      "llm_load_vocab: control token:    426 '[control_424]' is not marked as EOG\n",
      "llm_load_vocab: control token:    427 '[control_425]' is not marked as EOG\n",
      "llm_load_vocab: control token:    428 '[control_426]' is not marked as EOG\n",
      "llm_load_vocab: control token:    429 '[control_427]' is not marked as EOG\n",
      "llm_load_vocab: control token:    430 '[control_428]' is not marked as EOG\n",
      "llm_load_vocab: control token:    431 '[control_429]' is not marked as EOG\n",
      "llm_load_vocab: control token:    432 '[control_430]' is not marked as EOG\n",
      "llm_load_vocab: control token:    433 '[control_431]' is not marked as EOG\n",
      "llm_load_vocab: control token:    434 '[control_432]' is not marked as EOG\n",
      "llm_load_vocab: control token:    435 '[control_433]' is not marked as EOG\n",
      "llm_load_vocab: control token:    436 '[control_434]' is not marked as EOG\n",
      "llm_load_vocab: control token:    437 '[control_435]' is not marked as EOG\n",
      "llm_load_vocab: control token:    438 '[control_436]' is not marked as EOG\n",
      "llm_load_vocab: control token:    439 '[control_437]' is not marked as EOG\n",
      "llm_load_vocab: control token:    440 '[control_438]' is not marked as EOG\n",
      "llm_load_vocab: control token:    441 '[control_439]' is not marked as EOG\n",
      "llm_load_vocab: control token:    470 '[control_468]' is not marked as EOG\n",
      "llm_load_vocab: control token:    471 '[control_469]' is not marked as EOG\n",
      "llm_load_vocab: control token:    472 '[control_470]' is not marked as EOG\n",
      "llm_load_vocab: control token:    473 '[control_471]' is not marked as EOG\n",
      "llm_load_vocab: control token:    474 '[control_472]' is not marked as EOG\n",
      "llm_load_vocab: control token:    475 '[control_473]' is not marked as EOG\n",
      "llm_load_vocab: control token:    478 '[control_476]' is not marked as EOG\n",
      "llm_load_vocab: control token:    479 '[control_477]' is not marked as EOG\n",
      "llm_load_vocab: control token:    617 '[control_615]' is not marked as EOG\n",
      "llm_load_vocab: control token:    482 '[control_480]' is not marked as EOG\n",
      "llm_load_vocab: control token:    616 '[control_614]' is not marked as EOG\n",
      "llm_load_vocab: control token:    483 '[control_481]' is not marked as EOG\n",
      "llm_load_vocab: control token:    619 '[control_617]' is not marked as EOG\n",
      "llm_load_vocab: control token:    484 '[control_482]' is not marked as EOG\n",
      "llm_load_vocab: control token:    618 '[control_616]' is not marked as EOG\n",
      "llm_load_vocab: control token:    485 '[control_483]' is not marked as EOG\n",
      "llm_load_vocab: control token:    613 '[control_611]' is not marked as EOG\n",
      "llm_load_vocab: control token:    486 '[control_484]' is not marked as EOG\n",
      "llm_load_vocab: control token:    612 '[control_610]' is not marked as EOG\n",
      "llm_load_vocab: control token:    487 '[control_485]' is not marked as EOG\n",
      "llm_load_vocab: control token:    615 '[control_613]' is not marked as EOG\n",
      "llm_load_vocab: control token:    488 '[control_486]' is not marked as EOG\n",
      "llm_load_vocab: control token:    489 '[control_487]' is not marked as EOG\n",
      "llm_load_vocab: control token:    614 '[control_612]' is not marked as EOG\n",
      "llm_load_vocab: control token:    490 '[control_488]' is not marked as EOG\n",
      "llm_load_vocab: control token:    491 '[control_489]' is not marked as EOG\n",
      "llm_load_vocab: control token:    665 '[control_663]' is not marked as EOG\n",
      "llm_load_vocab: control token:    492 '[control_490]' is not marked as EOG\n",
      "llm_load_vocab: control token:    664 '[control_662]' is not marked as EOG\n",
      "llm_load_vocab: control token:    493 '[control_491]' is not marked as EOG\n",
      "llm_load_vocab: control token:    663 '[control_661]' is not marked as EOG\n",
      "llm_load_vocab: control token:    494 '[control_492]' is not marked as EOG\n",
      "llm_load_vocab: control token:    662 '[control_660]' is not marked as EOG\n",
      "llm_load_vocab: control token:    495 '[control_493]' is not marked as EOG\n",
      "llm_load_vocab: control token:    669 '[control_667]' is not marked as EOG\n",
      "llm_load_vocab: control token:    496 '[control_494]' is not marked as EOG\n",
      "llm_load_vocab: control token:    668 '[control_666]' is not marked as EOG\n",
      "llm_load_vocab: control token:    497 '[control_495]' is not marked as EOG\n",
      "llm_load_vocab: control token:    667 '[control_665]' is not marked as EOG\n",
      "llm_load_vocab: control token:    498 '[control_496]' is not marked as EOG\n",
      "llm_load_vocab: control token:    666 '[control_664]' is not marked as EOG\n",
      "llm_load_vocab: control token:    499 '[control_497]' is not marked as EOG\n",
      "llm_load_vocab: control token:    500 '[control_498]' is not marked as EOG\n",
      "llm_load_vocab: control token:    501 '[control_499]' is not marked as EOG\n",
      "llm_load_vocab: control token:    540 '[control_538]' is not marked as EOG\n",
      "llm_load_vocab: control token:    541 '[control_539]' is not marked as EOG\n",
      "llm_load_vocab: control token:    552 '[control_550]' is not marked as EOG\n",
      "llm_load_vocab: control token:    553 '[control_551]' is not marked as EOG\n",
      "llm_load_vocab: control token:    554 '[control_552]' is not marked as EOG\n",
      "llm_load_vocab: control token:    555 '[control_553]' is not marked as EOG\n",
      "llm_load_vocab: control token:    556 '[control_554]' is not marked as EOG\n",
      "llm_load_vocab: control token:    557 '[control_555]' is not marked as EOG\n",
      "llm_load_vocab: control token:    558 '[control_556]' is not marked as EOG\n",
      "llm_load_vocab: control token:    559 '[control_557]' is not marked as EOG\n",
      "llm_load_vocab: control token:    560 '[control_558]' is not marked as EOG\n",
      "llm_load_vocab: control token:    561 '[control_559]' is not marked as EOG\n",
      "llm_load_vocab: control token:    562 '[control_560]' is not marked as EOG\n",
      "llm_load_vocab: control token:    563 '[control_561]' is not marked as EOG\n",
      "llm_load_vocab: control token:    564 '[control_562]' is not marked as EOG\n",
      "llm_load_vocab: control token:    565 '[control_563]' is not marked as EOG\n",
      "llm_load_vocab: control token:    566 '[control_564]' is not marked as EOG\n",
      "llm_load_vocab: control token:    567 '[control_565]' is not marked as EOG\n",
      "llm_load_vocab: control token:    568 '[control_566]' is not marked as EOG\n",
      "llm_load_vocab: control token:    569 '[control_567]' is not marked as EOG\n",
      "llm_load_vocab: control token:    570 '[control_568]' is not marked as EOG\n",
      "llm_load_vocab: control token:    571 '[control_569]' is not marked as EOG\n",
      "llm_load_vocab: control token:    572 '[control_570]' is not marked as EOG\n",
      "llm_load_vocab: control token:    573 '[control_571]' is not marked as EOG\n",
      "llm_load_vocab: control token:    574 '[control_572]' is not marked as EOG\n",
      "llm_load_vocab: control token:    575 '[control_573]' is not marked as EOG\n",
      "llm_load_vocab: control token:    576 '[control_574]' is not marked as EOG\n",
      "llm_load_vocab: control token:    577 '[control_575]' is not marked as EOG\n",
      "llm_load_vocab: control token:    578 '[control_576]' is not marked as EOG\n",
      "llm_load_vocab: control token:    579 '[control_577]' is not marked as EOG\n",
      "llm_load_vocab: control token:    580 '[control_578]' is not marked as EOG\n",
      "llm_load_vocab: control token:    581 '[control_579]' is not marked as EOG\n",
      "llm_load_vocab: control token:    582 '[control_580]' is not marked as EOG\n",
      "llm_load_vocab: control token:    583 '[control_581]' is not marked as EOG\n",
      "llm_load_vocab: control token:    584 '[control_582]' is not marked as EOG\n",
      "llm_load_vocab: control token:    585 '[control_583]' is not marked as EOG\n",
      "llm_load_vocab: control token:    586 '[control_584]' is not marked as EOG\n",
      "llm_load_vocab: control token:    587 '[control_585]' is not marked as EOG\n",
      "llm_load_vocab: control token:    588 '[control_586]' is not marked as EOG\n",
      "llm_load_vocab: control token:    589 '[control_587]' is not marked as EOG\n",
      "llm_load_vocab: control token:    590 '[control_588]' is not marked as EOG\n",
      "llm_load_vocab: control token:    591 '[control_589]' is not marked as EOG\n",
      "llm_load_vocab: control token:    592 '[control_590]' is not marked as EOG\n",
      "llm_load_vocab: control token:    593 '[control_591]' is not marked as EOG\n",
      "llm_load_vocab: control token:    594 '[control_592]' is not marked as EOG\n",
      "llm_load_vocab: control token:    595 '[control_593]' is not marked as EOG\n",
      "llm_load_vocab: control token:    596 '[control_594]' is not marked as EOG\n",
      "llm_load_vocab: control token:    690 '[control_688]' is not marked as EOG\n",
      "llm_load_vocab: control token:    597 '[control_595]' is not marked as EOG\n",
      "llm_load_vocab: control token:    691 '[control_689]' is not marked as EOG\n",
      "llm_load_vocab: control token:    598 '[control_596]' is not marked as EOG\n",
      "llm_load_vocab: control token:    599 '[control_597]' is not marked as EOG\n",
      "llm_load_vocab: control token:    600 '[control_598]' is not marked as EOG\n",
      "llm_load_vocab: control token:    686 '[control_684]' is not marked as EOG\n",
      "llm_load_vocab: control token:    601 '[control_599]' is not marked as EOG\n",
      "llm_load_vocab: control token:    687 '[control_685]' is not marked as EOG\n",
      "llm_load_vocab: control token:    711 '[control_709]' is not marked as EOG\n",
      "llm_load_vocab: control token:    602 '[control_600]' is not marked as EOG\n",
      "llm_load_vocab: control token:    710 '[control_708]' is not marked as EOG\n",
      "llm_load_vocab: control token:    603 '[control_601]' is not marked as EOG\n",
      "llm_load_vocab: control token:    604 '[control_602]' is not marked as EOG\n",
      "llm_load_vocab: control token:    605 '[control_603]' is not marked as EOG\n",
      "llm_load_vocab: control token:    606 '[control_604]' is not marked as EOG\n",
      "llm_load_vocab: control token:    607 '[control_605]' is not marked as EOG\n",
      "llm_load_vocab: control token:    608 '[control_606]' is not marked as EOG\n",
      "llm_load_vocab: control token:    609 '[control_607]' is not marked as EOG\n",
      "llm_load_vocab: control token:    703 '[control_701]' is not marked as EOG\n",
      "llm_load_vocab: control token:    610 '[control_608]' is not marked as EOG\n",
      "llm_load_vocab: control token:    702 '[control_700]' is not marked as EOG\n",
      "llm_load_vocab: control token:    611 '[control_609]' is not marked as EOG\n",
      "llm_load_vocab: control token:    620 '[control_618]' is not marked as EOG\n",
      "llm_load_vocab: control token:    621 '[control_619]' is not marked as EOG\n",
      "llm_load_vocab: control token:    622 '[control_620]' is not marked as EOG\n",
      "llm_load_vocab: control token:    770 '[control_768]' is not marked as EOG\n",
      "llm_load_vocab: control token:    623 '[control_621]' is not marked as EOG\n",
      "llm_load_vocab: control token:    625 '[control_623]' is not marked as EOG\n",
      "llm_load_vocab: control token:    626 '[control_624]' is not marked as EOG\n",
      "llm_load_vocab: control token:    627 '[control_625]' is not marked as EOG\n",
      "llm_load_vocab: control token:    628 '[control_626]' is not marked as EOG\n",
      "llm_load_vocab: control token:    629 '[control_627]' is not marked as EOG\n",
      "llm_load_vocab: control token:    763 '[control_761]' is not marked as EOG\n",
      "llm_load_vocab: control token:    630 '[control_628]' is not marked as EOG\n",
      "llm_load_vocab: control token:    762 '[control_760]' is not marked as EOG\n",
      "llm_load_vocab: control token:    631 '[control_629]' is not marked as EOG\n",
      "llm_load_vocab: control token:    632 '[control_630]' is not marked as EOG\n",
      "llm_load_vocab: control token:    633 '[control_631]' is not marked as EOG\n",
      "llm_load_vocab: control token:    634 '[control_632]' is not marked as EOG\n",
      "llm_load_vocab: control token:    635 '[control_633]' is not marked as EOG\n",
      "llm_load_vocab: control token:    636 '[control_634]' is not marked as EOG\n",
      "llm_load_vocab: control token:    637 '[control_635]' is not marked as EOG\n",
      "llm_load_vocab: control token:    638 '[control_636]' is not marked as EOG\n",
      "llm_load_vocab: control token:    721 '[control_719]' is not marked as EOG\n",
      "llm_load_vocab: control token:    639 '[control_637]' is not marked as EOG\n",
      "llm_load_vocab: control token:    720 '[control_718]' is not marked as EOG\n",
      "llm_load_vocab: control token:    719 '[control_717]' is not marked as EOG\n",
      "llm_load_vocab: control token:    640 '[control_638]' is not marked as EOG\n",
      "llm_load_vocab: control token:    718 '[control_716]' is not marked as EOG\n",
      "llm_load_vocab: control token:    641 '[control_639]' is not marked as EOG\n",
      "llm_load_vocab: control token:    642 '[control_640]' is not marked as EOG\n",
      "llm_load_vocab: control token:    643 '[control_641]' is not marked as EOG\n",
      "llm_load_vocab: control token:    644 '[control_642]' is not marked as EOG\n",
      "llm_load_vocab: control token:    645 '[control_643]' is not marked as EOG\n",
      "llm_load_vocab: control token:    646 '[control_644]' is not marked as EOG\n",
      "llm_load_vocab: control token:    647 '[control_645]' is not marked as EOG\n",
      "llm_load_vocab: control token:    648 '[control_646]' is not marked as EOG\n",
      "llm_load_vocab: control token:    649 '[control_647]' is not marked as EOG\n",
      "llm_load_vocab: control token:    650 '[control_648]' is not marked as EOG\n",
      "llm_load_vocab: control token:    651 '[control_649]' is not marked as EOG\n",
      "llm_load_vocab: control token:    652 '[control_650]' is not marked as EOG\n",
      "llm_load_vocab: control token:    653 '[control_651]' is not marked as EOG\n",
      "llm_load_vocab: control token:    654 '[control_652]' is not marked as EOG\n",
      "llm_load_vocab: control token:    658 '[control_656]' is not marked as EOG\n",
      "llm_load_vocab: control token:    659 '[control_657]' is not marked as EOG\n",
      "llm_load_vocab: control token:    670 '[control_668]' is not marked as EOG\n",
      "llm_load_vocab: control token:    671 '[control_669]' is not marked as EOG\n",
      "llm_load_vocab: control token:    672 '[control_670]' is not marked as EOG\n",
      "llm_load_vocab: control token:    673 '[control_671]' is not marked as EOG\n",
      "llm_load_vocab: control token:    674 '[control_672]' is not marked as EOG\n",
      "llm_load_vocab: control token:    675 '[control_673]' is not marked as EOG\n",
      "llm_load_vocab: control token:    676 '[control_674]' is not marked as EOG\n",
      "llm_load_vocab: control token:    677 '[control_675]' is not marked as EOG\n",
      "llm_load_vocab: control token:    678 '[control_676]' is not marked as EOG\n",
      "llm_load_vocab: control token:    679 '[control_677]' is not marked as EOG\n",
      "llm_load_vocab: control token:    680 '[control_678]' is not marked as EOG\n",
      "llm_load_vocab: control token:    681 '[control_679]' is not marked as EOG\n",
      "llm_load_vocab: control token:    682 '[control_680]' is not marked as EOG\n",
      "llm_load_vocab: control token:    683 '[control_681]' is not marked as EOG\n",
      "llm_load_vocab: control token:    684 '[control_682]' is not marked as EOG\n",
      "llm_load_vocab: control token:    685 '[control_683]' is not marked as EOG\n",
      "llm_load_vocab: control token:    688 '[control_686]' is not marked as EOG\n",
      "llm_load_vocab: control token:    689 '[control_687]' is not marked as EOG\n",
      "llm_load_vocab: control token:    700 '[control_698]' is not marked as EOG\n",
      "llm_load_vocab: control token:    701 '[control_699]' is not marked as EOG\n",
      "llm_load_vocab: control token:    704 '[control_702]' is not marked as EOG\n",
      "llm_load_vocab: control token:    705 '[control_703]' is not marked as EOG\n",
      "llm_load_vocab: control token:    706 '[control_704]' is not marked as EOG\n",
      "llm_load_vocab: control token:    707 '[control_705]' is not marked as EOG\n",
      "llm_load_vocab: control token:    708 '[control_706]' is not marked as EOG\n",
      "llm_load_vocab: control token:    709 '[control_707]' is not marked as EOG\n",
      "llm_load_vocab: control token:    712 '[control_710]' is not marked as EOG\n",
      "llm_load_vocab: control token:    713 '[control_711]' is not marked as EOG\n",
      "llm_load_vocab: control token:    714 '[control_712]' is not marked as EOG\n",
      "llm_load_vocab: control token:    715 '[control_713]' is not marked as EOG\n",
      "llm_load_vocab: control token:    716 '[control_714]' is not marked as EOG\n",
      "llm_load_vocab: control token:    717 '[control_715]' is not marked as EOG\n",
      "llm_load_vocab: control token:    722 '[control_720]' is not marked as EOG\n",
      "llm_load_vocab: control token:    723 '[control_721]' is not marked as EOG\n",
      "llm_load_vocab: control token:    726 '[control_724]' is not marked as EOG\n",
      "llm_load_vocab: control token:    727 '[control_725]' is not marked as EOG\n",
      "llm_load_vocab: control token:    728 '[control_726]' is not marked as EOG\n",
      "llm_load_vocab: control token:    729 '[control_727]' is not marked as EOG\n",
      "llm_load_vocab: control token:    734 '[control_732]' is not marked as EOG\n",
      "llm_load_vocab: control token:    735 '[control_733]' is not marked as EOG\n",
      "llm_load_vocab: control token:    736 '[control_734]' is not marked as EOG\n",
      "llm_load_vocab: control token:    737 '[control_735]' is not marked as EOG\n",
      "llm_load_vocab: control token:    738 '[control_736]' is not marked as EOG\n",
      "llm_load_vocab: control token:    739 '[control_737]' is not marked as EOG\n",
      "llm_load_vocab: control token:    742 '[control_740]' is not marked as EOG\n",
      "llm_load_vocab: control token:    743 '[control_741]' is not marked as EOG\n",
      "llm_load_vocab: control token:    744 '[control_742]' is not marked as EOG\n",
      "llm_load_vocab: control token:    745 '[control_743]' is not marked as EOG\n",
      "llm_load_vocab: control token:    746 '[control_744]' is not marked as EOG\n",
      "llm_load_vocab: control token:    747 '[control_745]' is not marked as EOG\n",
      "llm_load_vocab: control token:    748 '[control_746]' is not marked as EOG\n",
      "llm_load_vocab: control token:    749 '[control_747]' is not marked as EOG\n",
      "llm_load_vocab: control token:    750 '[control_748]' is not marked as EOG\n",
      "llm_load_vocab: control token:    751 '[control_749]' is not marked as EOG\n",
      "llm_load_vocab: control token:    752 '[control_750]' is not marked as EOG\n",
      "llm_load_vocab: control token:    753 '[control_751]' is not marked as EOG\n",
      "llm_load_vocab: control token:    754 '[control_752]' is not marked as EOG\n",
      "llm_load_vocab: control token:    755 '[control_753]' is not marked as EOG\n",
      "llm_load_vocab: control token:    756 '[control_754]' is not marked as EOG\n",
      "llm_load_vocab: control token:    757 '[control_755]' is not marked as EOG\n",
      "llm_load_vocab: control token:    764 '[control_762]' is not marked as EOG\n",
      "llm_load_vocab: control token:    765 '[control_763]' is not marked as EOG\n",
      "llm_load_vocab: control token:    766 '[control_764]' is not marked as EOG\n",
      "llm_load_vocab: control token:    767 '[control_765]' is not marked as EOG\n",
      "llm_load_vocab: control token:    768 '[control_766]' is not marked as EOG\n",
      "llm_load_vocab: control token:    769 '[control_767]' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 771\n",
      "llm_load_vocab: token to piece cache size = 0.1731 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32768\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.25 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = models--mistralai--Mistral-7B-Instruct-v0.3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 781 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  7209.02 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   136.00 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 32768\n",
      "llama_new_context_with_model: n_ctx_per_seq = 32768\n",
      "llama_new_context_with_model: n_batch       = 32\n",
      "llama_new_context_with_model: n_ubatch      = 8\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 10000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_kv_cache_init: kv_size = 32768, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  4096.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    36.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     4.13 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'models--mistralai--Mistral-7B-Instruct-v0.3', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '32768', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'quantize.imatrix.chunks_count': '74', 'quantize.imatrix.file': './imatrix.dat', 'quantize.imatrix.dataset': 'group_40.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Decomposition\n",
    "\n",
    "The score is worse here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:08:53.270927Z",
     "start_time": "2025-01-13T13:43:38.445765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGDecomposition(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Mistral-Decomposition\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Mistral-Decomposition\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Storing documents embeddings (batch size is 1000): 0it [00:00, ?it/s]\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/497 [00:00<?, ?it/s]llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4742.86 ms /   201 tokens\n",
      "  0%|          | 1/497 [00:04<39:22,  4.76s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3921.65 ms /   119 tokens\n",
      "  0%|          | 2/497 [00:08<35:25,  4.29s/it]Llama.generate: 89 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2186.40 ms /    66 tokens\n",
      "  1%|          | 3/497 [00:10<27:31,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3647.87 ms /   104 tokens\n",
      "  1%|          | 4/497 [00:14<28:32,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3848.60 ms /   111 tokens\n",
      "  1%|          | 5/497 [00:18<29:41,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3617.71 ms /   111 tokens\n",
      "  1%|          | 6/497 [00:22<29:42,  3.63s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2557.47 ms /    82 tokens\n",
      "  1%|▏         | 7/497 [00:24<26:50,  3.29s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2197.84 ms /    70 tokens\n",
      "  2%|▏         | 8/497 [00:26<24:02,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2363.85 ms /    76 tokens\n",
      "  2%|▏         | 9/497 [00:29<22:33,  2.77s/it]Llama.generate: 89 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2019.19 ms /    64 tokens\n",
      "  2%|▏         | 10/497 [00:31<20:43,  2.55s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.60 ms /    89 tokens\n",
      "  2%|▏         | 11/497 [00:34<21:30,  2.65s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2943.02 ms /    87 tokens\n",
      "  2%|▏         | 12/497 [00:37<22:13,  2.75s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3402.72 ms /   100 tokens\n",
      "  3%|▎         | 13/497 [00:40<23:53,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2336.71 ms /    68 tokens\n",
      "  3%|▎         | 14/497 [00:43<22:21,  2.78s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2239.58 ms /    66 tokens\n",
      "  3%|▎         | 15/497 [00:45<21:05,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2131.64 ms /    65 tokens\n",
      "  3%|▎         | 16/497 [00:47<19:55,  2.48s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.22 ms /    86 tokens\n",
      "  3%|▎         | 17/497 [00:50<20:48,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2930.61 ms /    92 tokens\n",
      "  4%|▎         | 18/497 [00:53<21:38,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3963.14 ms /   119 tokens\n",
      "  4%|▍         | 19/497 [00:57<24:40,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2645.35 ms /    80 tokens\n",
      "  4%|▍         | 20/497 [00:59<23:37,  2.97s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3480.83 ms /   108 tokens\n",
      "  4%|▍         | 21/497 [01:03<24:51,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3179.93 ms /    90 tokens\n",
      "  4%|▍         | 22/497 [01:06<24:59,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3974.45 ms /   132 tokens\n",
      "  5%|▍         | 23/497 [01:10<26:55,  3.41s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3454.50 ms /   104 tokens\n",
      "  5%|▍         | 24/497 [01:14<27:04,  3.43s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2490.22 ms /    76 tokens\n",
      "  5%|▌         | 25/497 [01:16<24:49,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2808.76 ms /    90 tokens\n",
      "  5%|▌         | 26/497 [01:19<24:02,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.06 ms /    83 tokens\n",
      "  5%|▌         | 27/497 [01:22<23:23,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3608.10 ms /   110 tokens\n",
      "  6%|▌         | 28/497 [01:26<24:52,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3991.01 ms /   121 tokens\n",
      "  6%|▌         | 29/497 [01:30<26:47,  3.43s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2440.45 ms /    78 tokens\n",
      "  6%|▌         | 30/497 [01:32<24:26,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.38 ms /    91 tokens\n",
      "  6%|▌         | 31/497 [01:35<24:11,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3340.56 ms /   101 tokens\n",
      "  6%|▋         | 32/497 [01:38<24:44,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3395.11 ms /   107 tokens\n",
      "  7%|▋         | 33/497 [01:42<25:12,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2227.10 ms /    66 tokens\n",
      "  7%|▋         | 34/497 [01:44<22:51,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12287.48 ms /   306 tokens\n",
      "  7%|▋         | 35/497 [01:56<44:24,  5.77s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2002.13 ms /    64 tokens\n",
      "  7%|▋         | 36/497 [01:58<35:42,  4.65s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.20 ms /    91 tokens\n",
      "  7%|▋         | 37/497 [02:02<32:19,  4.22s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2690.45 ms /    79 tokens\n",
      "  8%|▊         | 38/497 [02:04<28:47,  3.76s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2769.06 ms /    83 tokens\n",
      "  8%|▊         | 39/497 [02:07<26:32,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3960.53 ms /   125 tokens\n",
      "  8%|▊         | 40/497 [02:11<27:40,  3.63s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5935.46 ms /   162 tokens\n",
      "  8%|▊         | 41/497 [02:17<32:55,  4.33s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2285.70 ms /    73 tokens\n",
      "  8%|▊         | 42/497 [02:19<28:15,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3436.56 ms /   118 tokens\n",
      "  9%|▊         | 43/497 [02:23<27:36,  3.65s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1912.89 ms /    61 tokens\n",
      "  9%|▉         | 44/497 [02:25<23:39,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2158.59 ms /    67 tokens\n",
      "  9%|▉         | 45/497 [02:27<21:29,  2.85s/it]Llama.generate: 89 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3007.62 ms /    93 tokens\n",
      "  9%|▉         | 46/497 [02:30<21:50,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2480.07 ms /    76 tokens\n",
      "  9%|▉         | 47/497 [02:33<20:54,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3138.81 ms /    99 tokens\n",
      " 10%|▉         | 48/497 [02:36<21:43,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3246.62 ms /   101 tokens\n",
      " 10%|▉         | 49/497 [02:39<22:30,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3401.90 ms /    97 tokens\n",
      " 10%|█         | 50/497 [02:42<23:21,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3062.67 ms /    89 tokens\n",
      " 10%|█         | 51/497 [02:46<23:13,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3346.13 ms /   100 tokens\n",
      " 10%|█         | 52/497 [02:49<23:43,  3.20s/it]Llama.generate: 87 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3635.48 ms /   107 tokens\n",
      " 11%|█         | 53/497 [02:53<24:42,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3607.97 ms /   113 tokens\n",
      " 11%|█         | 54/497 [02:56<25:19,  3.43s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.29 ms /    93 tokens\n",
      " 11%|█         | 55/497 [02:59<23:54,  3.24s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2466.16 ms /    75 tokens\n",
      " 11%|█▏        | 56/497 [03:02<22:12,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3911.07 ms /   118 tokens\n",
      " 11%|█▏        | 57/497 [03:05<24:09,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.19 ms /   109 tokens\n",
      " 12%|█▏        | 58/497 [03:09<24:28,  3.35s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2684.74 ms /    87 tokens\n",
      " 12%|█▏        | 59/497 [03:12<23:02,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5770.63 ms /   160 tokens\n",
      " 12%|█▏        | 60/497 [03:17<28:43,  3.94s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3528.50 ms /   109 tokens\n",
      " 12%|█▏        | 61/497 [03:21<27:50,  3.83s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3656.66 ms /   106 tokens\n",
      " 12%|█▏        | 62/497 [03:25<27:27,  3.79s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2755.26 ms /    84 tokens\n",
      " 13%|█▎        | 63/497 [03:27<25:11,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2836.39 ms /    87 tokens\n",
      " 13%|█▎        | 64/497 [03:30<23:48,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3387.52 ms /   100 tokens\n",
      " 13%|█▎        | 65/497 [03:34<24:01,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2039.15 ms /    59 tokens\n",
      " 13%|█▎        | 66/497 [03:36<21:14,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1919.19 ms /    63 tokens\n",
      " 13%|█▎        | 67/497 [03:38<19:00,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2420.90 ms /    77 tokens\n",
      " 14%|█▎        | 68/497 [03:40<18:31,  2.59s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.53 ms /    61 tokens\n",
      " 14%|█▍        | 69/497 [03:42<17:21,  2.43s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4010.06 ms /   116 tokens\n",
      " 14%|█▍        | 70/497 [03:46<20:47,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3011.10 ms /    95 tokens\n",
      " 14%|█▍        | 71/497 [03:49<20:58,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2636.61 ms /    84 tokens\n",
      " 14%|█▍        | 72/497 [03:52<20:18,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2757.21 ms /    86 tokens\n",
      " 15%|█▍        | 73/497 [03:55<20:06,  2.85s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2197.17 ms /    70 tokens\n",
      " 15%|█▍        | 74/497 [03:57<18:44,  2.66s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4095.66 ms /   113 tokens\n",
      " 15%|█▌        | 75/497 [04:01<21:48,  3.10s/it]Llama.generate: 89 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4509.36 ms /   136 tokens\n",
      " 15%|█▌        | 76/497 [04:06<24:47,  3.53s/it]Llama.generate: 89 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3628.80 ms /   112 tokens\n",
      " 15%|█▌        | 77/497 [04:09<24:59,  3.57s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2935.35 ms /    91 tokens\n",
      " 16%|█▌        | 78/497 [04:12<23:39,  3.39s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3433.84 ms /    99 tokens\n",
      " 16%|█▌        | 79/497 [04:16<23:45,  3.41s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2131.37 ms /    67 tokens\n",
      " 16%|█▌        | 80/497 [04:18<21:04,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1797.46 ms /    55 tokens\n",
      " 16%|█▋        | 81/497 [04:20<18:32,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2645.99 ms /    86 tokens\n",
      " 16%|█▋        | 82/497 [04:22<18:28,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.65 ms /    90 tokens\n",
      " 17%|█▋        | 83/497 [04:25<18:53,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3386.10 ms /   101 tokens\n",
      " 17%|█▋        | 84/497 [04:29<20:14,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3072.77 ms /    95 tokens\n",
      " 17%|█▋        | 85/497 [04:32<20:30,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4430.99 ms /   136 tokens\n",
      " 17%|█▋        | 86/497 [04:36<23:30,  3.43s/it]Llama.generate: 86 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4163.29 ms /   131 tokens\n",
      " 18%|█▊        | 87/497 [04:41<25:00,  3.66s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2530.03 ms /    81 tokens\n",
      " 18%|█▊        | 88/497 [04:43<22:41,  3.33s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2109.07 ms /    66 tokens\n",
      " 18%|█▊        | 89/497 [04:45<20:13,  2.97s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1808.18 ms /    55 tokens\n",
      " 18%|█▊        | 90/497 [04:47<17:51,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2445.43 ms /    74 tokens\n",
      " 18%|█▊        | 91/497 [04:50<17:30,  2.59s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3375.32 ms /   104 tokens\n",
      " 19%|█▊        | 92/497 [04:53<19:07,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.04 ms /    94 tokens\n",
      " 19%|█▊        | 93/497 [04:56<19:26,  2.89s/it]Llama.generate: 88 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1893.80 ms /    57 tokens\n",
      " 19%|█▉        | 94/497 [04:58<17:28,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3440.61 ms /   106 tokens\n",
      " 19%|█▉        | 95/497 [05:01<19:11,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2092.18 ms /    64 tokens\n",
      " 19%|█▉        | 96/497 [05:03<17:37,  2.64s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3549.38 ms /   110 tokens\n",
      " 20%|█▉        | 97/497 [05:07<19:28,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3892.70 ms /   123 tokens\n",
      " 20%|█▉        | 98/497 [05:11<21:24,  3.22s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2291.75 ms /    71 tokens\n",
      " 20%|█▉        | 99/497 [05:13<19:36,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2263.26 ms /    72 tokens\n",
      " 20%|██        | 100/497 [05:16<18:14,  2.76s/it]Llama.generate: 91 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2487.19 ms /    74 tokens\n",
      " 20%|██        | 101/497 [05:18<17:44,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3345.05 ms /   101 tokens\n",
      " 21%|██        | 102/497 [05:22<19:04,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2883.19 ms /    83 tokens\n",
      " 21%|██        | 103/497 [05:24<19:02,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2209.79 ms /    73 tokens\n",
      " 21%|██        | 104/497 [05:27<17:42,  2.70s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2036.13 ms /    64 tokens\n",
      " 21%|██        | 105/497 [05:29<16:23,  2.51s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2534.03 ms /    79 tokens\n",
      " 21%|██▏       | 106/497 [05:31<16:28,  2.53s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1868.98 ms /    56 tokens\n",
      " 22%|██▏       | 107/497 [05:33<15:14,  2.34s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4798.15 ms /   144 tokens\n",
      " 22%|██▏       | 108/497 [05:38<20:01,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3282.24 ms /   102 tokens\n",
      " 22%|██▏       | 109/497 [05:41<20:24,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3884.62 ms /   128 tokens\n",
      " 22%|██▏       | 110/497 [05:45<21:48,  3.38s/it]Llama.generate: 88 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2239.73 ms /    71 tokens\n",
      " 22%|██▏       | 111/497 [05:48<19:36,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.69 ms /   107 tokens\n",
      " 23%|██▎       | 112/497 [05:51<20:05,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2489.80 ms /    82 tokens\n",
      " 23%|██▎       | 113/497 [05:53<18:53,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2228.59 ms /    69 tokens\n",
      " 23%|██▎       | 114/497 [05:56<17:30,  2.74s/it]Llama.generate: 87 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4278.48 ms /   133 tokens\n",
      " 23%|██▎       | 115/497 [06:00<20:28,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2503.49 ms /    81 tokens\n",
      " 23%|██▎       | 116/497 [06:03<19:07,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2357.85 ms /    76 tokens\n",
      " 24%|██▎       | 117/497 [06:05<17:53,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.40 ms /    87 tokens\n",
      " 24%|██▎       | 118/497 [06:08<17:49,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4539.85 ms /   129 tokens\n",
      " 24%|██▍       | 119/497 [06:12<21:05,  3.35s/it]Llama.generate: 93 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2681.14 ms /    76 tokens\n",
      " 24%|██▍       | 120/497 [06:15<19:50,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2427.78 ms /    76 tokens\n",
      " 24%|██▍       | 121/497 [06:17<18:28,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2119.48 ms /    64 tokens\n",
      " 25%|██▍       | 122/497 [06:20<16:55,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2924.00 ms /    91 tokens\n",
      " 25%|██▍       | 123/497 [06:23<17:21,  2.78s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2527.16 ms /    79 tokens\n",
      " 25%|██▍       | 124/497 [06:25<16:53,  2.72s/it]Llama.generate: 88 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2922.95 ms /    87 tokens\n",
      " 25%|██▌       | 125/497 [06:28<17:16,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2785.11 ms /    84 tokens\n",
      " 25%|██▌       | 126/497 [06:31<17:17,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.31 ms /   102 tokens\n",
      " 26%|██▌       | 127/497 [06:34<18:28,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2181.75 ms /    70 tokens\n",
      " 26%|██▌       | 128/497 [06:37<16:58,  2.76s/it]Llama.generate: 89 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2332.35 ms /    70 tokens\n",
      " 26%|██▌       | 129/497 [06:39<16:11,  2.64s/it]Llama.generate: 89 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3164.56 ms /    92 tokens\n",
      " 26%|██▌       | 130/497 [06:42<17:09,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.23 ms /   100 tokens\n",
      " 26%|██▋       | 131/497 [06:45<17:43,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2376.20 ms /    78 tokens\n",
      " 27%|██▋       | 132/497 [06:48<16:44,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1918.40 ms /    61 tokens\n",
      " 27%|██▋       | 133/497 [06:50<15:15,  2.51s/it]Llama.generate: 87 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3120.80 ms /    98 tokens\n",
      " 27%|██▋       | 134/497 [06:53<16:22,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2331.40 ms /    74 tokens\n",
      " 27%|██▋       | 135/497 [06:55<15:43,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2916.96 ms /    91 tokens\n",
      " 27%|██▋       | 136/497 [06:58<16:17,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3728.14 ms /   109 tokens\n",
      " 28%|██▊       | 137/497 [07:02<18:07,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2635.23 ms /    84 tokens\n",
      " 28%|██▊       | 138/497 [07:04<17:24,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5788.34 ms /   173 tokens\n",
      " 28%|██▊       | 139/497 [07:10<22:34,  3.78s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2555.97 ms /    78 tokens\n",
      " 28%|██▊       | 140/497 [07:13<20:23,  3.43s/it]Llama.generate: 89 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2291.27 ms /    75 tokens\n",
      " 28%|██▊       | 141/497 [07:15<18:22,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2007.47 ms /    62 tokens\n",
      " 29%|██▊       | 142/497 [07:17<16:26,  2.78s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5226.66 ms /   162 tokens\n",
      " 29%|██▉       | 143/497 [07:23<20:46,  3.52s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3149.56 ms /    92 tokens\n",
      " 29%|██▉       | 144/497 [07:26<20:07,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2487.46 ms /    73 tokens\n",
      " 29%|██▉       | 145/497 [07:28<18:28,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3273.07 ms /   101 tokens\n",
      " 29%|██▉       | 146/497 [07:32<18:41,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2443.70 ms /    71 tokens\n",
      " 30%|██▉       | 147/497 [07:34<17:22,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2486.19 ms /    73 tokens\n",
      " 30%|██▉       | 148/497 [07:37<16:30,  2.84s/it]Llama.generate: 87 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3896.81 ms /   122 tokens\n",
      " 30%|██▉       | 149/497 [07:40<18:22,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2163.36 ms /    71 tokens\n",
      " 30%|███       | 150/497 [07:43<16:36,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3583.09 ms /   116 tokens\n",
      " 30%|███       | 151/497 [07:46<17:52,  3.10s/it]Llama.generate: 88 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2616.67 ms /    81 tokens\n",
      " 31%|███       | 152/497 [07:49<17:02,  2.96s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3334.83 ms /   103 tokens\n",
      " 31%|███       | 153/497 [07:52<17:42,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5486.42 ms /   178 tokens\n",
      " 31%|███       | 154/497 [07:58<21:49,  3.82s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3088.96 ms /    96 tokens\n",
      " 31%|███       | 155/497 [08:01<20:33,  3.61s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3202.56 ms /   104 tokens\n",
      " 31%|███▏      | 156/497 [08:04<19:50,  3.49s/it]Llama.generate: 87 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2402.85 ms /    68 tokens\n",
      " 32%|███▏      | 157/497 [08:07<17:59,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2127.33 ms /    67 tokens\n",
      " 32%|███▏      | 158/497 [08:09<16:12,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1948.04 ms /    62 tokens\n",
      " 32%|███▏      | 159/497 [08:11<14:40,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2065.51 ms /    64 tokens\n",
      " 32%|███▏      | 160/497 [08:13<13:46,  2.45s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3217.79 ms /   101 tokens\n",
      " 32%|███▏      | 161/497 [08:16<15:03,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2819.51 ms /    90 tokens\n",
      " 33%|███▎      | 162/497 [08:19<15:16,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3324.82 ms /   109 tokens\n",
      " 33%|███▎      | 163/497 [08:22<16:16,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2170.43 ms /    68 tokens\n",
      " 33%|███▎      | 164/497 [08:24<14:59,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1924.70 ms /    62 tokens\n",
      " 33%|███▎      | 165/497 [08:26<13:43,  2.48s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2220.42 ms /    70 tokens\n",
      " 33%|███▎      | 166/497 [08:29<13:18,  2.41s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1888.05 ms /    54 tokens\n",
      " 34%|███▎      | 167/497 [08:31<12:27,  2.27s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.95 ms /    81 tokens\n",
      " 34%|███▍      | 168/497 [08:33<13:08,  2.40s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2834.34 ms /    93 tokens\n",
      " 34%|███▍      | 169/497 [08:36<13:51,  2.54s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3086.86 ms /    95 tokens\n",
      " 34%|███▍      | 170/497 [08:39<14:46,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1855.31 ms /    58 tokens\n",
      " 34%|███▍      | 171/497 [08:41<13:23,  2.47s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3328.84 ms /    95 tokens\n",
      " 35%|███▍      | 172/497 [08:45<14:48,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3035.60 ms /    87 tokens\n",
      " 35%|███▍      | 173/497 [08:48<15:17,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 51 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7710.88 ms /   226 tokens\n",
      " 35%|███▌      | 174/497 [08:55<23:10,  4.30s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2272.71 ms /    69 tokens\n",
      " 35%|███▌      | 175/497 [08:58<19:53,  3.71s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2854.91 ms /    89 tokens\n",
      " 35%|███▌      | 176/497 [09:01<18:31,  3.46s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1958.00 ms /    60 tokens\n",
      " 36%|███▌      | 177/497 [09:03<16:05,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5669.34 ms /   156 tokens\n",
      " 36%|███▌      | 178/497 [09:08<20:19,  3.82s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2034.50 ms /    61 tokens\n",
      " 36%|███▌      | 179/497 [09:10<17:27,  3.29s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.27 ms /    90 tokens\n",
      " 36%|███▌      | 180/497 [09:13<16:52,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1831.81 ms /    59 tokens\n",
      " 36%|███▋      | 181/497 [09:15<14:42,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2057.35 ms /    64 tokens\n",
      " 37%|███▋      | 182/497 [09:17<13:33,  2.58s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.14 ms /    84 tokens\n",
      " 37%|███▋      | 183/497 [09:20<13:41,  2.62s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3125.71 ms /   101 tokens\n",
      " 37%|███▋      | 184/497 [09:23<14:29,  2.78s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1713.62 ms /    52 tokens\n",
      " 37%|███▋      | 185/497 [09:25<12:50,  2.47s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3055.92 ms /    95 tokens\n",
      " 37%|███▋      | 186/497 [09:28<13:46,  2.66s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2619.63 ms /    77 tokens\n",
      " 38%|███▊      | 187/497 [09:31<13:42,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3022.49 ms /    94 tokens\n",
      " 38%|███▊      | 188/497 [09:34<14:17,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1605.17 ms /    52 tokens\n",
      " 38%|███▊      | 189/497 [09:35<12:27,  2.43s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5222.67 ms /   149 tokens\n",
      " 38%|███▊      | 190/497 [09:40<16:47,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2406.95 ms /    75 tokens\n",
      " 38%|███▊      | 191/497 [09:43<15:27,  3.03s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.84 ms /    91 tokens\n",
      " 39%|███▊      | 192/497 [09:46<15:10,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4338.92 ms /   131 tokens\n",
      " 39%|███▉      | 193/497 [09:50<17:13,  3.40s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2316.93 ms /    73 tokens\n",
      " 39%|███▉      | 194/497 [09:53<15:34,  3.08s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2998.10 ms /    95 tokens\n",
      " 39%|███▉      | 195/497 [09:56<15:26,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2274.71 ms /    73 tokens\n",
      " 39%|███▉      | 196/497 [09:58<14:14,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3773.33 ms /   119 tokens\n",
      " 40%|███▉      | 197/497 [10:02<15:39,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3220.37 ms /   100 tokens\n",
      " 40%|███▉      | 198/497 [10:05<15:47,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3422.85 ms /   102 tokens\n",
      " 40%|████      | 199/497 [10:08<16:08,  3.25s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2678.52 ms /    84 tokens\n",
      " 40%|████      | 200/497 [10:11<15:18,  3.09s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3185.77 ms /    97 tokens\n",
      " 40%|████      | 201/497 [10:14<15:26,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2144.95 ms /    68 tokens\n",
      " 41%|████      | 202/497 [10:16<13:58,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2508.42 ms /    79 tokens\n",
      " 41%|████      | 203/497 [10:19<13:29,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2319.33 ms /    70 tokens\n",
      " 41%|████      | 204/497 [10:21<12:50,  2.63s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2343.02 ms /    75 tokens\n",
      " 41%|████      | 205/497 [10:24<12:24,  2.55s/it]Llama.generate: 88 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2716.28 ms /    80 tokens\n",
      " 41%|████▏     | 206/497 [10:26<12:39,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4281.91 ms /   135 tokens\n",
      " 42%|████▏     | 207/497 [10:31<15:04,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3579.17 ms /   114 tokens\n",
      " 42%|████▏     | 208/497 [10:34<15:44,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1998.89 ms /    62 tokens\n",
      " 42%|████▏     | 209/497 [10:36<13:54,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4146.41 ms /   127 tokens\n",
      " 42%|████▏     | 210/497 [10:41<15:42,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1726.77 ms /    54 tokens\n",
      " 42%|████▏     | 211/497 [10:42<13:28,  2.83s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5120.57 ms /   143 tokens\n",
      " 43%|████▎     | 212/497 [10:48<16:43,  3.52s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2603.74 ms /    81 tokens\n",
      " 43%|████▎     | 213/497 [10:50<15:25,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3199.41 ms /    95 tokens\n",
      " 43%|████▎     | 214/497 [10:53<15:18,  3.24s/it]Llama.generate: 87 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3315.10 ms /   106 tokens\n",
      " 43%|████▎     | 215/497 [10:57<15:24,  3.28s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2099.26 ms /    66 tokens\n",
      " 43%|████▎     | 216/497 [10:59<13:43,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3145.72 ms /   101 tokens\n",
      " 44%|████▎     | 217/497 [11:02<14:01,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2859.24 ms /    90 tokens\n",
      " 44%|████▍     | 218/497 [11:05<13:49,  2.97s/it]Llama.generate: 86 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5368.01 ms /   168 tokens\n",
      " 44%|████▍     | 219/497 [11:10<17:08,  3.70s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1635.30 ms /    51 tokens\n",
      " 44%|████▍     | 220/497 [11:12<14:16,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2561.52 ms /    80 tokens\n",
      " 44%|████▍     | 221/497 [11:15<13:32,  2.94s/it]Llama.generate: 87 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4299.48 ms /   135 tokens\n",
      " 45%|████▍     | 222/497 [11:19<15:23,  3.36s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3487.29 ms /   106 tokens\n",
      " 45%|████▍     | 223/497 [11:22<15:34,  3.41s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3117.88 ms /    91 tokens\n",
      " 45%|████▌     | 224/497 [11:26<15:09,  3.33s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3752.48 ms /   112 tokens\n",
      " 45%|████▌     | 225/497 [11:29<15:43,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3124.56 ms /   102 tokens\n",
      " 45%|████▌     | 226/497 [11:33<15:15,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2563.78 ms /    80 tokens\n",
      " 46%|████▌     | 227/497 [11:35<14:08,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3557.83 ms /   103 tokens\n",
      " 46%|████▌     | 228/497 [11:39<14:40,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2175.57 ms /    70 tokens\n",
      " 46%|████▌     | 229/497 [11:41<13:11,  2.95s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2833.48 ms /    88 tokens\n",
      " 46%|████▋     | 230/497 [11:44<13:01,  2.93s/it]Llama.generate: 89 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3393.32 ms /   110 tokens\n",
      " 46%|████▋     | 231/497 [11:47<13:38,  3.08s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2136.48 ms /    64 tokens\n",
      " 47%|████▋     | 232/497 [11:49<12:22,  2.80s/it]Llama.generate: 87 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3637.40 ms /   115 tokens\n",
      " 47%|████▋     | 233/497 [11:53<13:28,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2643.52 ms /    77 tokens\n",
      " 47%|████▋     | 234/497 [11:56<12:54,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3384.80 ms /   103 tokens\n",
      " 47%|████▋     | 235/497 [11:59<13:28,  3.09s/it]Llama.generate: 87 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4990.62 ms /   151 tokens\n",
      " 47%|████▋     | 236/497 [12:04<15:56,  3.67s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2515.59 ms /    74 tokens\n",
      " 48%|████▊     | 237/497 [12:07<14:25,  3.33s/it]Llama.generate: 87 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4617.74 ms /   140 tokens\n",
      " 48%|████▊     | 238/497 [12:11<16:04,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3143.93 ms /    93 tokens\n",
      " 48%|████▊     | 239/497 [12:15<15:19,  3.56s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3247.66 ms /   104 tokens\n",
      " 48%|████▊     | 240/497 [12:18<14:53,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2606.88 ms /    78 tokens\n",
      " 48%|████▊     | 241/497 [12:20<13:46,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2733.70 ms /    82 tokens\n",
      " 49%|████▊     | 242/497 [12:23<13:08,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2984.33 ms /    87 tokens\n",
      " 49%|████▉     | 243/497 [12:26<12:59,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2747.66 ms /    87 tokens\n",
      " 49%|████▉     | 244/497 [12:29<12:33,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2049.90 ms /    60 tokens\n",
      " 49%|████▉     | 245/497 [12:31<11:22,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2526.29 ms /    78 tokens\n",
      " 49%|████▉     | 246/497 [12:34<11:09,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3274.12 ms /    94 tokens\n",
      " 50%|████▉     | 247/497 [12:37<11:55,  2.86s/it]Llama.generate: 88 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4560.08 ms /   132 tokens\n",
      " 50%|████▉     | 248/497 [12:42<14:01,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.11 ms /    85 tokens\n",
      " 50%|█████     | 249/497 [12:45<13:33,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3511.87 ms /   110 tokens\n",
      " 50%|█████     | 250/497 [12:48<13:48,  3.36s/it]Llama.generate: 89 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1931.68 ms /    58 tokens\n",
      " 51%|█████     | 251/497 [12:50<12:03,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3723.01 ms /   123 tokens\n",
      " 51%|█████     | 252/497 [12:54<12:59,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3285.33 ms /   100 tokens\n",
      " 51%|█████     | 253/497 [12:57<13:05,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2833.73 ms /    86 tokens\n",
      " 51%|█████     | 254/497 [13:00<12:37,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2149.31 ms /    68 tokens\n",
      " 51%|█████▏    | 255/497 [13:02<11:25,  2.83s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4547.15 ms /   125 tokens\n",
      " 52%|█████▏    | 256/497 [13:07<13:28,  3.36s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1886.63 ms /    58 tokens\n",
      " 52%|█████▏    | 257/497 [13:09<11:41,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2840.84 ms /    89 tokens\n",
      " 52%|█████▏    | 258/497 [13:12<11:35,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2760.98 ms /    86 tokens\n",
      " 52%|█████▏    | 259/497 [13:14<11:23,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2425.65 ms /    75 tokens\n",
      " 52%|█████▏    | 260/497 [13:17<10:51,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3190.48 ms /    95 tokens\n",
      " 53%|█████▎    | 261/497 [13:20<11:21,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3957.17 ms /   113 tokens\n",
      " 53%|█████▎    | 262/497 [13:24<12:36,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2426.33 ms /    76 tokens\n",
      " 53%|█████▎    | 263/497 [13:26<11:39,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3405.92 ms /    96 tokens\n",
      " 53%|█████▎    | 264/497 [13:30<12:07,  3.12s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2427.59 ms /    77 tokens\n",
      " 53%|█████▎    | 265/497 [13:32<11:18,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2367.35 ms /    72 tokens\n",
      " 54%|█████▎    | 266/497 [13:35<10:39,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3478.92 ms /   103 tokens\n",
      " 54%|█████▎    | 267/497 [13:38<11:28,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3315.94 ms /   103 tokens\n",
      " 54%|█████▍    | 268/497 [13:42<11:50,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2646.03 ms /    77 tokens\n",
      " 54%|█████▍    | 269/497 [13:44<11:19,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3841.64 ms /   113 tokens\n",
      " 54%|█████▍    | 270/497 [13:48<12:17,  3.25s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2267.16 ms /    70 tokens\n",
      " 55%|█████▍    | 271/497 [13:51<11:09,  2.96s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2388.01 ms /    71 tokens\n",
      " 55%|█████▍    | 272/497 [13:53<10:29,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3585.74 ms /   105 tokens\n",
      " 55%|█████▍    | 273/497 [13:57<11:22,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3403.33 ms /   105 tokens\n",
      " 55%|█████▌    | 274/497 [14:00<11:44,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.58 ms /    89 tokens\n",
      " 55%|█████▌    | 275/497 [14:03<11:21,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1947.17 ms /    59 tokens\n",
      " 56%|█████▌    | 276/497 [14:05<10:06,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.52 ms /    86 tokens\n",
      " 56%|█████▌    | 277/497 [14:08<10:13,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2287.70 ms /    72 tokens\n",
      " 56%|█████▌    | 278/497 [14:10<09:39,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2042.55 ms /    64 tokens\n",
      " 56%|█████▌    | 279/497 [14:12<08:58,  2.47s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.26 ms /    91 tokens\n",
      " 56%|█████▋    | 280/497 [14:15<09:33,  2.64s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2323.12 ms /    72 tokens\n",
      " 57%|█████▋    | 281/497 [14:17<09:11,  2.55s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3353.44 ms /    99 tokens\n",
      " 57%|█████▋    | 282/497 [14:21<10:02,  2.80s/it]Llama.generate: 87 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1687.41 ms /    47 tokens\n",
      " 57%|█████▋    | 283/497 [14:23<08:50,  2.48s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2855.45 ms /    85 tokens\n",
      " 57%|█████▋    | 284/497 [14:25<09:14,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2183.15 ms /    65 tokens\n",
      " 57%|█████▋    | 285/497 [14:28<08:46,  2.48s/it]Llama.generate: 87 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4112.01 ms /   125 tokens\n",
      " 58%|█████▊    | 286/497 [14:32<10:28,  2.98s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2471.85 ms /    73 tokens\n",
      " 58%|█████▊    | 287/497 [14:34<09:55,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3726.68 ms /   117 tokens\n",
      " 58%|█████▊    | 288/497 [14:38<10:50,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3930.11 ms /   108 tokens\n",
      " 58%|█████▊    | 289/497 [14:42<11:41,  3.37s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2416.69 ms /    75 tokens\n",
      " 58%|█████▊    | 290/497 [14:45<10:40,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2162.29 ms /    67 tokens\n",
      " 59%|█████▊    | 291/497 [14:47<09:41,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2387.05 ms /    72 tokens\n",
      " 59%|█████▉    | 292/497 [14:49<09:13,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2868.07 ms /    88 tokens\n",
      " 59%|█████▉    | 293/497 [14:52<09:23,  2.76s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2292.44 ms /    72 tokens\n",
      " 59%|█████▉    | 294/497 [14:54<08:54,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2484.57 ms /    74 tokens\n",
      " 59%|█████▉    | 295/497 [14:57<08:44,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2258.14 ms /    70 tokens\n",
      " 60%|█████▉    | 296/497 [14:59<08:23,  2.50s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2865.77 ms /    84 tokens\n",
      " 60%|█████▉    | 297/497 [15:02<08:44,  2.62s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3267.89 ms /   104 tokens\n",
      " 60%|█████▉    | 298/497 [15:05<09:21,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2883.30 ms /    90 tokens\n",
      " 60%|██████    | 299/497 [15:08<09:25,  2.85s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3310.73 ms /   108 tokens\n",
      " 60%|██████    | 300/497 [15:12<09:50,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2970.21 ms /    88 tokens\n",
      " 61%|██████    | 301/497 [15:15<09:48,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3590.96 ms /   110 tokens\n",
      " 61%|██████    | 302/497 [15:18<10:21,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5454.58 ms /   163 tokens\n",
      " 61%|██████    | 303/497 [15:24<12:32,  3.88s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2214.29 ms /    69 tokens\n",
      " 61%|██████    | 304/497 [15:26<10:54,  3.39s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4494.07 ms /   130 tokens\n",
      " 61%|██████▏   | 305/497 [15:30<11:56,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4157.27 ms /   118 tokens\n",
      " 62%|██████▏   | 306/497 [15:35<12:18,  3.87s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2338.16 ms /    73 tokens\n",
      " 62%|██████▏   | 307/497 [15:37<10:48,  3.41s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2403.84 ms /    75 tokens\n",
      " 62%|██████▏   | 308/497 [15:39<09:49,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2724.78 ms /    83 tokens\n",
      " 62%|██████▏   | 309/497 [15:42<09:26,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1838.69 ms /    61 tokens\n",
      " 62%|██████▏   | 310/497 [15:44<08:18,  2.67s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2468.30 ms /    75 tokens\n",
      " 63%|██████▎   | 311/497 [15:47<08:07,  2.62s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.72 ms /    88 tokens\n",
      " 63%|██████▎   | 312/497 [15:49<08:18,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.68 ms /    89 tokens\n",
      " 63%|██████▎   | 313/497 [15:52<08:27,  2.76s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7205.27 ms /   199 tokens\n",
      " 63%|██████▎   | 314/497 [16:00<12:30,  4.10s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3444.53 ms /   108 tokens\n",
      " 63%|██████▎   | 315/497 [16:03<11:52,  3.92s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3151.57 ms /    89 tokens\n",
      " 64%|██████▎   | 316/497 [16:06<11:09,  3.70s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3542.22 ms /   109 tokens\n",
      " 64%|██████▍   | 317/497 [16:10<10:59,  3.66s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3193.25 ms /   100 tokens\n",
      " 64%|██████▍   | 318/497 [16:13<10:31,  3.53s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3725.84 ms /   107 tokens\n",
      " 64%|██████▍   | 319/497 [16:17<10:40,  3.60s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2052.27 ms /    65 tokens\n",
      " 64%|██████▍   | 320/497 [16:19<09:16,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4118.41 ms /   122 tokens\n",
      " 65%|██████▍   | 321/497 [16:23<10:07,  3.45s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2377.68 ms /    68 tokens\n",
      " 65%|██████▍   | 322/497 [16:25<09:08,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3363.40 ms /   102 tokens\n",
      " 65%|██████▍   | 323/497 [16:29<09:19,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1661.28 ms /    50 tokens\n",
      " 65%|██████▌   | 324/497 [16:31<07:57,  2.76s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3142.71 ms /    95 tokens\n",
      " 65%|██████▌   | 325/497 [16:34<08:15,  2.88s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4290.09 ms /   131 tokens\n",
      " 66%|██████▌   | 326/497 [16:38<09:26,  3.31s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4012.03 ms /   115 tokens\n",
      " 66%|██████▌   | 327/497 [16:42<10:01,  3.54s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2645.71 ms /    80 tokens\n",
      " 66%|██████▌   | 328/497 [16:45<09:13,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2338.91 ms /    68 tokens\n",
      " 66%|██████▌   | 329/497 [16:47<08:24,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2635.45 ms /    80 tokens\n",
      " 66%|██████▋   | 330/497 [16:50<08:04,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3746.29 ms /   113 tokens\n",
      " 67%|██████▋   | 331/497 [16:54<08:45,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2919.55 ms /    87 tokens\n",
      " 67%|██████▋   | 332/497 [16:57<08:32,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2952.84 ms /    89 tokens\n",
      " 67%|██████▋   | 333/497 [17:00<08:22,  3.06s/it]Llama.generate: 89 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2193.27 ms /    67 tokens\n",
      " 67%|██████▋   | 334/497 [17:02<07:39,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3400.96 ms /   105 tokens\n",
      " 67%|██████▋   | 335/497 [17:05<08:06,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.37 ms /    89 tokens\n",
      " 68%|██████▊   | 336/497 [17:08<07:59,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3721.62 ms /   109 tokens\n",
      " 68%|██████▊   | 337/497 [17:12<08:33,  3.21s/it]Llama.generate: 87 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2102.07 ms /    65 tokens\n",
      " 68%|██████▊   | 338/497 [17:14<07:39,  2.89s/it]Llama.generate: 87 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2382.21 ms /    72 tokens\n",
      " 68%|██████▊   | 339/497 [17:16<07:14,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2468.41 ms /    76 tokens\n",
      " 68%|██████▊   | 340/497 [17:19<06:59,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3519.40 ms /   113 tokens\n",
      " 69%|██████▊   | 341/497 [17:22<07:37,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2181.08 ms /    67 tokens\n",
      " 69%|██████▉   | 342/497 [17:25<07:01,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2968.24 ms /    92 tokens\n",
      " 69%|██████▉   | 343/497 [17:28<07:11,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2452.29 ms /    77 tokens\n",
      " 69%|██████▉   | 344/497 [17:30<06:53,  2.70s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1755.14 ms /    57 tokens\n",
      " 69%|██████▉   | 345/497 [17:32<06:09,  2.43s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2243.33 ms /    69 tokens\n",
      " 70%|██████▉   | 346/497 [17:34<05:59,  2.38s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2735.08 ms /    83 tokens\n",
      " 70%|██████▉   | 347/497 [17:37<06:14,  2.50s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1791.04 ms /    55 tokens\n",
      " 70%|███████   | 348/497 [17:39<05:41,  2.29s/it]Llama.generate: 88 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4208.77 ms /   123 tokens\n",
      " 70%|███████   | 349/497 [17:43<07:05,  2.88s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2194.94 ms /    63 tokens\n",
      " 70%|███████   | 350/497 [17:45<06:33,  2.68s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3479.23 ms /   110 tokens\n",
      " 71%|███████   | 351/497 [17:49<07:07,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2243.06 ms /    70 tokens\n",
      " 71%|███████   | 352/497 [17:51<06:36,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2430.40 ms /    74 tokens\n",
      " 71%|███████   | 353/497 [17:54<06:22,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2575.70 ms /    80 tokens\n",
      " 71%|███████   | 354/497 [17:56<06:17,  2.64s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2277.70 ms /    69 tokens\n",
      " 71%|███████▏  | 355/497 [17:58<06:00,  2.54s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3468.14 ms /   103 tokens\n",
      " 72%|███████▏  | 356/497 [18:02<06:38,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3354.95 ms /    96 tokens\n",
      " 72%|███████▏  | 357/497 [18:05<06:59,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4215.74 ms /   131 tokens\n",
      " 72%|███████▏  | 358/497 [18:10<07:48,  3.37s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2596.61 ms /    81 tokens\n",
      " 72%|███████▏  | 359/497 [18:12<07:14,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3076.15 ms /   103 tokens\n",
      " 72%|███████▏  | 360/497 [18:15<07:09,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3617.51 ms /   107 tokens\n",
      " 73%|███████▎  | 361/497 [18:19<07:27,  3.29s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6901.11 ms /   202 tokens\n",
      " 73%|███████▎  | 362/497 [18:26<09:51,  4.38s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2301.88 ms /    71 tokens\n",
      " 73%|███████▎  | 363/497 [18:28<08:24,  3.77s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4904.31 ms /   140 tokens\n",
      " 73%|███████▎  | 364/497 [18:33<09:07,  4.12s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3494.59 ms /   104 tokens\n",
      " 73%|███████▎  | 365/497 [18:37<08:40,  3.94s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2689.75 ms /    87 tokens\n",
      " 74%|███████▎  | 366/497 [18:39<07:47,  3.57s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3272.02 ms /    97 tokens\n",
      " 74%|███████▍  | 367/497 [18:43<07:34,  3.49s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2123.99 ms /    66 tokens\n",
      " 74%|███████▍  | 368/497 [18:45<06:38,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2663.78 ms /    79 tokens\n",
      " 74%|███████▍  | 369/497 [18:48<06:20,  2.97s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3641.08 ms /   104 tokens\n",
      " 74%|███████▍  | 370/497 [18:51<06:44,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3063.37 ms /    94 tokens\n",
      " 75%|███████▍  | 371/497 [18:54<06:37,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3822.94 ms /   117 tokens\n",
      " 75%|███████▍  | 372/497 [18:58<07:00,  3.36s/it]Llama.generate: 89 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1793.36 ms /    55 tokens\n",
      " 75%|███████▌  | 373/497 [19:00<06:00,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3976.75 ms /   123 tokens\n",
      " 75%|███████▌  | 374/497 [19:04<06:37,  3.23s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2092.99 ms /    62 tokens\n",
      " 75%|███████▌  | 375/497 [19:06<05:53,  2.90s/it]Llama.generate: 88 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3816.05 ms /   111 tokens\n",
      " 76%|███████▌  | 376/497 [19:10<06:25,  3.18s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2196.23 ms /    66 tokens\n",
      " 76%|███████▌  | 377/497 [19:12<05:47,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2498.57 ms /    81 tokens\n",
      " 76%|███████▌  | 378/497 [19:15<05:31,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3495.39 ms /   107 tokens\n",
      " 76%|███████▋  | 379/497 [19:18<05:54,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2925.65 ms /    89 tokens\n",
      " 76%|███████▋  | 380/497 [19:21<05:49,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3859.69 ms /   120 tokens\n",
      " 77%|███████▋  | 381/497 [19:25<06:18,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2351.82 ms /    74 tokens\n",
      " 77%|███████▋  | 382/497 [19:27<05:44,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3714.17 ms /   118 tokens\n",
      " 77%|███████▋  | 383/497 [19:31<06:07,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3174.25 ms /    97 tokens\n",
      " 77%|███████▋  | 384/497 [19:34<06:03,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3552.78 ms /    99 tokens\n",
      " 77%|███████▋  | 385/497 [19:38<06:12,  3.33s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4082.00 ms /   114 tokens\n",
      " 78%|███████▊  | 386/497 [19:42<06:35,  3.56s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3236.36 ms /    98 tokens\n",
      " 78%|███████▊  | 387/497 [19:45<06:22,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2650.32 ms /    83 tokens\n",
      " 78%|███████▊  | 388/497 [19:48<05:53,  3.24s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2579.87 ms /    74 tokens\n",
      " 78%|███████▊  | 389/497 [19:51<05:29,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1952.19 ms /    62 tokens\n",
      " 78%|███████▊  | 390/497 [19:53<04:52,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.11 ms /   103 tokens\n",
      " 79%|███████▊  | 391/497 [19:56<05:07,  2.90s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3291.99 ms /   103 tokens\n",
      " 79%|███████▉  | 392/497 [19:59<05:17,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3511.30 ms /   103 tokens\n",
      " 79%|███████▉  | 393/497 [20:03<05:31,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2764.64 ms /    86 tokens\n",
      " 79%|███████▉  | 394/497 [20:06<05:15,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1891.76 ms /    61 tokens\n",
      " 79%|███████▉  | 395/497 [20:08<04:37,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3688.55 ms /   110 tokens\n",
      " 80%|███████▉  | 396/497 [20:11<05:05,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4237.91 ms /   135 tokens\n",
      " 80%|███████▉  | 397/497 [20:16<05:39,  3.40s/it]Llama.generate: 87 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3610.81 ms /   110 tokens\n",
      " 80%|████████  | 398/497 [20:19<05:43,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3426.81 ms /   103 tokens\n",
      " 80%|████████  | 399/497 [20:23<05:40,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2235.84 ms /    65 tokens\n",
      " 80%|████████  | 400/497 [20:25<05:01,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2981.43 ms /    93 tokens\n",
      " 81%|████████  | 401/497 [20:28<04:55,  3.08s/it]Llama.generate: 89 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2118.40 ms /    65 tokens\n",
      " 81%|████████  | 402/497 [20:30<04:26,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2425.93 ms /    74 tokens\n",
      " 81%|████████  | 403/497 [20:33<04:13,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2346.81 ms /    71 tokens\n",
      " 81%|████████▏ | 404/497 [20:35<04:01,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2371.64 ms /    74 tokens\n",
      " 81%|████████▏ | 405/497 [20:37<03:54,  2.55s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2604.07 ms /    78 tokens\n",
      " 82%|████████▏ | 406/497 [20:40<03:54,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2925.30 ms /    91 tokens\n",
      " 82%|████████▏ | 407/497 [20:43<04:01,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3103.90 ms /    95 tokens\n",
      " 82%|████████▏ | 408/497 [20:46<04:10,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2022.70 ms /    64 tokens\n",
      " 82%|████████▏ | 409/497 [20:48<03:48,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3087.21 ms /    94 tokens\n",
      " 82%|████████▏ | 410/497 [20:51<03:58,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3289.51 ms /   106 tokens\n",
      " 83%|████████▎ | 411/497 [20:55<04:11,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2532.67 ms /    81 tokens\n",
      " 83%|████████▎ | 412/497 [20:57<03:59,  2.82s/it]Llama.generate: 87 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5342.20 ms /   155 tokens\n",
      " 83%|████████▎ | 413/497 [21:03<05:01,  3.58s/it]Llama.generate: 87 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4497.62 ms /   141 tokens\n",
      " 83%|████████▎ | 414/497 [21:07<05:20,  3.87s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2403.72 ms /    72 tokens\n",
      " 84%|████████▎ | 415/497 [21:09<04:41,  3.44s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2706.04 ms /    83 tokens\n",
      " 84%|████████▎ | 416/497 [21:12<04:21,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2486.41 ms /    72 tokens\n",
      " 84%|████████▍ | 417/497 [21:15<04:01,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2856.82 ms /    91 tokens\n",
      " 84%|████████▍ | 418/497 [21:18<03:55,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3652.59 ms /   104 tokens\n",
      " 84%|████████▍ | 419/497 [21:21<04:09,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2437.76 ms /    76 tokens\n",
      " 85%|████████▍ | 420/497 [21:24<03:48,  2.97s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3074.46 ms /    97 tokens\n",
      " 85%|████████▍ | 421/497 [21:27<03:49,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2434.84 ms /    76 tokens\n",
      " 85%|████████▍ | 422/497 [21:29<03:33,  2.85s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2679.52 ms /    80 tokens\n",
      " 85%|████████▌ | 423/497 [21:32<03:27,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6164.10 ms /   180 tokens\n",
      " 85%|████████▌ | 424/497 [21:38<04:39,  3.82s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3741.80 ms /   116 tokens\n",
      " 86%|████████▌ | 425/497 [21:42<04:34,  3.81s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2105.89 ms /    71 tokens\n",
      " 86%|████████▌ | 426/497 [21:44<03:54,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2634.45 ms /    81 tokens\n",
      " 86%|████████▌ | 427/497 [21:47<03:37,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2554.29 ms /    77 tokens\n",
      " 86%|████████▌ | 428/497 [21:49<03:23,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2509.98 ms /    77 tokens\n",
      " 86%|████████▋ | 429/497 [21:52<03:12,  2.83s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2141.27 ms /    64 tokens\n",
      " 87%|████████▋ | 430/497 [21:54<02:56,  2.63s/it]Llama.generate: 87 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2595.55 ms /    80 tokens\n",
      " 87%|████████▋ | 431/497 [21:57<02:53,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2915.02 ms /    87 tokens\n",
      " 87%|████████▋ | 432/497 [22:00<02:57,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2695.70 ms /    86 tokens\n",
      " 87%|████████▋ | 433/497 [22:02<02:54,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2139.37 ms /    66 tokens\n",
      " 87%|████████▋ | 434/497 [22:05<02:41,  2.56s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3985.49 ms /   121 tokens\n",
      " 88%|████████▊ | 435/497 [22:09<03:05,  3.00s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2628.94 ms /    82 tokens\n",
      " 88%|████████▊ | 436/497 [22:11<02:56,  2.89s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1947.98 ms /    60 tokens\n",
      " 88%|████████▊ | 437/497 [22:13<02:36,  2.62s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2233.24 ms /    73 tokens\n",
      " 88%|████████▊ | 438/497 [22:16<02:28,  2.51s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2023.49 ms /    63 tokens\n",
      " 88%|████████▊ | 439/497 [22:18<02:17,  2.38s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2115.45 ms /    65 tokens\n",
      " 89%|████████▊ | 440/497 [22:20<02:11,  2.31s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3168.44 ms /    96 tokens\n",
      " 89%|████████▊ | 441/497 [22:23<02:23,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2257.91 ms /    67 tokens\n",
      " 89%|████████▉ | 442/497 [22:25<02:16,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.27 ms /   113 tokens\n",
      " 89%|████████▉ | 443/497 [22:29<02:38,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2128.33 ms /    63 tokens\n",
      " 89%|████████▉ | 444/497 [22:31<02:23,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3045.53 ms /    90 tokens\n",
      " 90%|████████▉ | 445/497 [22:34<02:26,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2448.60 ms /    69 tokens\n",
      " 90%|████████▉ | 446/497 [22:37<02:18,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2179.95 ms /    67 tokens\n",
      " 90%|████████▉ | 447/497 [22:39<02:08,  2.56s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2226.86 ms /    72 tokens\n",
      " 90%|█████████ | 448/497 [22:41<02:01,  2.47s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2866.40 ms /    87 tokens\n",
      " 90%|█████████ | 449/497 [22:44<02:04,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4922.07 ms /   144 tokens\n",
      " 91%|█████████ | 450/497 [22:49<02:35,  3.31s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2228.09 ms /    73 tokens\n",
      " 91%|█████████ | 451/497 [22:51<02:17,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2564.24 ms /    80 tokens\n",
      " 91%|█████████ | 452/497 [22:54<02:09,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3477.63 ms /   102 tokens\n",
      " 91%|█████████ | 453/497 [22:58<02:14,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3099.17 ms /    99 tokens\n",
      " 91%|█████████▏| 454/497 [23:01<02:12,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3460.21 ms /   105 tokens\n",
      " 92%|█████████▏| 455/497 [23:04<02:14,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2706.89 ms /    84 tokens\n",
      " 92%|█████████▏| 456/497 [23:07<02:05,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3053.96 ms /    97 tokens\n",
      " 92%|█████████▏| 457/497 [23:10<02:02,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.42 ms /    97 tokens\n",
      " 92%|█████████▏| 458/497 [23:13<01:58,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2496.63 ms /    79 tokens\n",
      " 92%|█████████▏| 459/497 [23:16<01:49,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.40 ms /    96 tokens\n",
      " 93%|█████████▎| 460/497 [23:19<01:48,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2152.12 ms /    70 tokens\n",
      " 93%|█████████▎| 461/497 [23:21<01:37,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2064.37 ms /    62 tokens\n",
      " 93%|█████████▎| 462/497 [23:23<01:28,  2.53s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3072.25 ms /    95 tokens\n",
      " 93%|█████████▎| 463/497 [23:26<01:31,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3732.65 ms /   119 tokens\n",
      " 93%|█████████▎| 464/497 [23:30<01:39,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2605.36 ms /    81 tokens\n",
      " 94%|█████████▎| 465/497 [23:32<01:32,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3555.68 ms /   114 tokens\n",
      " 94%|█████████▍| 466/497 [23:36<01:36,  3.11s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3296.64 ms /   105 tokens\n",
      " 94%|█████████▍| 467/497 [23:39<01:35,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3039.19 ms /    95 tokens\n",
      " 94%|█████████▍| 468/497 [23:42<01:31,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2105.23 ms /    69 tokens\n",
      " 94%|█████████▍| 469/497 [23:45<01:19,  2.84s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2948.67 ms /    85 tokens\n",
      " 95%|█████████▍| 470/497 [23:47<01:17,  2.88s/it]Llama.generate: 88 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2163.39 ms /    70 tokens\n",
      " 95%|█████████▍| 471/497 [23:50<01:09,  2.68s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2028.75 ms /    63 tokens\n",
      " 95%|█████████▍| 472/497 [23:52<01:02,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2663.40 ms /    82 tokens\n",
      " 95%|█████████▌| 473/497 [23:54<01:01,  2.55s/it]Llama.generate: 89 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2579.83 ms /    79 tokens\n",
      " 95%|█████████▌| 474/497 [23:57<00:59,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2260.37 ms /    70 tokens\n",
      " 96%|█████████▌| 475/497 [23:59<00:54,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2335.61 ms /    74 tokens\n",
      " 96%|█████████▌| 476/497 [24:02<00:51,  2.45s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4845.48 ms /   145 tokens\n",
      " 96%|█████████▌| 477/497 [24:07<01:03,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2850.60 ms /    88 tokens\n",
      " 96%|█████████▌| 478/497 [24:09<00:58,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2538.68 ms /    80 tokens\n",
      " 96%|█████████▋| 479/497 [24:12<00:52,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2916.50 ms /    91 tokens\n",
      " 97%|█████████▋| 480/497 [24:15<00:49,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2155.60 ms /    70 tokens\n",
      " 97%|█████████▋| 481/497 [24:17<00:43,  2.71s/it]Llama.generate: 91 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1835.58 ms /    56 tokens\n",
      " 97%|█████████▋| 482/497 [24:19<00:36,  2.46s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1944.26 ms /    64 tokens\n",
      " 97%|█████████▋| 483/497 [24:21<00:32,  2.31s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3521.45 ms /   108 tokens\n",
      " 97%|█████████▋| 484/497 [24:25<00:34,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2240.72 ms /    71 tokens\n",
      " 98%|█████████▊| 485/497 [24:27<00:30,  2.56s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3968.12 ms /   123 tokens\n",
      " 98%|█████████▊| 486/497 [24:31<00:32,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1987.25 ms /    61 tokens\n",
      " 98%|█████████▊| 487/497 [24:33<00:26,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2797.16 ms /    88 tokens\n",
      " 98%|█████████▊| 488/497 [24:36<00:24,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4384.64 ms /   128 tokens\n",
      " 98%|█████████▊| 489/497 [24:40<00:25,  3.24s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2173.68 ms /    66 tokens\n",
      " 99%|█████████▊| 490/497 [24:42<00:20,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2555.62 ms /    80 tokens\n",
      " 99%|█████████▉| 491/497 [24:45<00:16,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2519.12 ms /    85 tokens\n",
      " 99%|█████████▉| 492/497 [24:47<00:13,  2.74s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1980.75 ms /    59 tokens\n",
      " 99%|█████████▉| 493/497 [24:49<00:10,  2.52s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4790.23 ms /   151 tokens\n",
      " 99%|█████████▉| 494/497 [24:54<00:09,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3117.81 ms /    99 tokens\n",
      "100%|█████████▉| 495/497 [24:57<00:06,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2509.94 ms /    78 tokens\n",
      "100%|█████████▉| 496/497 [25:00<00:02,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2571.86 ms /    79 tokens\n",
      "100%|██████████| 497/497 [25:03<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42458563535911603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Fusion\n",
    "\n",
    "The score is really bad here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized). And contrarily to the previous case, we have even more calls to the LLM hence the error propagates"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:34:58.560194Z",
     "start_time": "2025-01-13T14:08:53.432524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGFusion(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Mistral-Fusion\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Mistral-Fusion\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Storing documents embeddings (batch size is 1000): 0it [00:00, ?it/s]\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/497 [00:00<?, ?it/s]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3248.74 ms /    94 tokens\n",
      "  0%|          | 1/497 [00:03<27:02,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4184.17 ms /   126 tokens\n",
      "  0%|          | 2/497 [00:07<31:36,  3.83s/it]Llama.generate: 89 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1873.88 ms /    57 tokens\n",
      "  1%|          | 3/497 [00:09<24:19,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2379.95 ms /    71 tokens\n",
      "  1%|          | 4/497 [00:11<22:28,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4922.30 ms /   133 tokens\n",
      "  1%|          | 5/497 [00:16<29:01,  3.54s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3272.38 ms /   103 tokens\n",
      "  1%|          | 6/497 [00:20<28:17,  3.46s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2659.76 ms /    84 tokens\n",
      "  1%|▏         | 7/497 [00:22<26:11,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2373.75 ms /    75 tokens\n",
      "  2%|▏         | 8/497 [00:25<24:02,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2587.66 ms /    81 tokens\n",
      "  2%|▏         | 9/497 [00:27<23:09,  2.85s/it]Llama.generate: 89 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2671.95 ms /    79 tokens\n",
      "  2%|▏         | 10/497 [00:30<22:46,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.90 ms /    92 tokens\n",
      "  2%|▏         | 11/497 [00:33<23:12,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2807.53 ms /    86 tokens\n",
      "  2%|▏         | 12/497 [00:36<23:05,  2.86s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3633.42 ms /   108 tokens\n",
      "  3%|▎         | 13/497 [00:40<25:01,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2288.31 ms /    66 tokens\n",
      "  3%|▎         | 14/497 [00:42<23:03,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1936.80 ms /    59 tokens\n",
      "  3%|▎         | 15/497 [00:44<20:47,  2.59s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2352.31 ms /    71 tokens\n",
      "  3%|▎         | 16/497 [00:46<20:16,  2.53s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2053.25 ms /    63 tokens\n",
      "  3%|▎         | 17/497 [00:48<19:09,  2.39s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2850.66 ms /    91 tokens\n",
      "  4%|▎         | 18/497 [00:51<20:16,  2.54s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3677.67 ms /   116 tokens\n",
      "  4%|▍         | 19/497 [00:55<23:02,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2257.84 ms /    73 tokens\n",
      "  4%|▍         | 20/497 [00:57<21:32,  2.71s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4089.05 ms /   126 tokens\n",
      "  4%|▍         | 21/497 [01:01<24:53,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1571.68 ms /    50 tokens\n",
      "  4%|▍         | 22/497 [01:03<21:09,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4534.01 ms /   148 tokens\n",
      "  5%|▍         | 23/497 [01:07<25:37,  3.24s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3235.94 ms /   103 tokens\n",
      "  5%|▍         | 24/497 [01:11<25:36,  3.25s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2238.32 ms /    71 tokens\n",
      "  5%|▌         | 25/497 [01:13<23:14,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3374.04 ms /   103 tokens\n",
      "  5%|▌         | 26/497 [01:16<24:14,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2443.58 ms /    74 tokens\n",
      "  5%|▌         | 27/497 [01:19<22:46,  2.91s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3493.65 ms /   110 tokens\n",
      "  6%|▌         | 28/497 [01:22<24:09,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3734.32 ms /   118 tokens\n",
      "  6%|▌         | 29/497 [01:26<25:40,  3.29s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2315.15 ms /    75 tokens\n",
      "  6%|▌         | 30/497 [01:28<23:24,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3473.90 ms /   105 tokens\n",
      "  6%|▌         | 31/497 [01:32<24:32,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3668.73 ms /   113 tokens\n",
      "  6%|▋         | 32/497 [01:36<25:44,  3.32s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3769.69 ms /   117 tokens\n",
      "  7%|▋         | 33/497 [01:39<26:47,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2239.49 ms /    70 tokens\n",
      "  7%|▋         | 34/497 [01:42<23:57,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1499.36 ms /    47 tokens\n",
      "  7%|▋         | 35/497 [01:43<20:15,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2020.17 ms /    65 tokens\n",
      "  7%|▋         | 36/497 [01:45<18:53,  2.46s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2125.59 ms /    65 tokens\n",
      "  7%|▋         | 37/497 [01:47<18:07,  2.36s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2900.65 ms /    86 tokens\n",
      "  8%|▊         | 38/497 [01:50<19:24,  2.54s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2349.04 ms /    74 tokens\n",
      "  8%|▊         | 39/497 [01:53<19:00,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3376.41 ms /   110 tokens\n",
      "  8%|▊         | 40/497 [01:56<21:04,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4431.57 ms /   126 tokens\n",
      "  8%|▊         | 41/497 [02:01<24:52,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1988.65 ms /    65 tokens\n",
      "  8%|▊         | 42/497 [02:03<21:59,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 33 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    33 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5254.72 ms /   164 tokens\n",
      "  9%|▊         | 43/497 [02:08<27:23,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1974.86 ms /    62 tokens\n",
      "  9%|▉         | 44/497 [02:10<23:39,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2159.00 ms /    67 tokens\n",
      "  9%|▉         | 45/497 [02:12<21:27,  2.85s/it]Llama.generate: 89 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.26 ms /    90 tokens\n",
      "  9%|▉         | 46/497 [02:15<21:28,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4283.99 ms /   123 tokens\n",
      "  9%|▉         | 47/497 [02:19<24:42,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3437.93 ms /   107 tokens\n",
      " 10%|▉         | 48/497 [02:23<25:02,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.40 ms /    94 tokens\n",
      " 10%|▉         | 49/497 [02:26<24:01,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2496.17 ms /    75 tokens\n",
      " 10%|█         | 50/497 [02:28<22:26,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1941.16 ms /    60 tokens\n",
      " 10%|█         | 51/497 [02:30<20:04,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3264.69 ms /   103 tokens\n",
      " 10%|█         | 52/497 [02:34<21:20,  2.88s/it]Llama.generate: 87 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3586.46 ms /   108 tokens\n",
      " 11%|█         | 53/497 [02:37<22:55,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4476.80 ms /   134 tokens\n",
      " 11%|█         | 54/497 [02:42<25:59,  3.52s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2936.86 ms /    95 tokens\n",
      " 11%|█         | 55/497 [02:45<24:41,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4742.99 ms /   132 tokens\n",
      " 11%|█▏        | 56/497 [02:49<27:46,  3.78s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2800.73 ms /    91 tokens\n",
      " 11%|█▏        | 57/497 [02:52<25:38,  3.50s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3422.06 ms /   109 tokens\n",
      " 12%|█▏        | 58/497 [02:56<25:28,  3.48s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2289.97 ms /    77 tokens\n",
      " 12%|█▏        | 59/497 [02:58<22:50,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1875.30 ms /    58 tokens\n",
      " 12%|█▏        | 60/497 [03:00<20:06,  2.76s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4200.13 ms /   113 tokens\n",
      " 12%|█▏        | 61/497 [03:04<23:18,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3901.88 ms /   106 tokens\n",
      " 12%|█▏        | 62/497 [03:08<24:49,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2707.37 ms /    84 tokens\n",
      " 13%|█▎        | 63/497 [03:11<23:16,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3322.91 ms /    98 tokens\n",
      " 13%|█▎        | 64/497 [03:14<23:30,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2725.10 ms /    85 tokens\n",
      " 13%|█▎        | 65/497 [03:17<22:22,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1750.27 ms /    52 tokens\n",
      " 13%|█▎        | 66/497 [03:19<19:28,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2592.72 ms /    80 tokens\n",
      " 13%|█▎        | 67/497 [03:21<19:13,  2.68s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2193.24 ms /    70 tokens\n",
      " 14%|█▎        | 68/497 [03:24<18:13,  2.55s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1452.19 ms /    45 tokens\n",
      " 14%|█▍        | 69/497 [03:25<15:53,  2.23s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.56 ms /    82 tokens\n",
      " 14%|█▍        | 70/497 [03:28<16:51,  2.37s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3213.06 ms /   100 tokens\n",
      " 14%|█▍        | 71/497 [03:31<18:40,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2401.45 ms /    77 tokens\n",
      " 14%|█▍        | 72/497 [03:33<18:12,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2689.50 ms /    79 tokens\n",
      " 15%|█▍        | 73/497 [03:36<18:28,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3555.40 ms /   106 tokens\n",
      " 15%|█▍        | 74/497 [03:40<20:28,  2.91s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1708.21 ms /    53 tokens\n",
      " 15%|█▌        | 75/497 [03:41<17:57,  2.55s/it]Llama.generate: 89 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3786.77 ms /   122 tokens\n",
      " 15%|█▌        | 76/497 [03:45<20:35,  2.93s/it]Llama.generate: 89 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3172.92 ms /   101 tokens\n",
      " 15%|█▌        | 77/497 [03:48<21:06,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2846.94 ms /    89 tokens\n",
      " 16%|█▌        | 78/497 [03:51<20:45,  2.97s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1860.27 ms /    56 tokens\n",
      " 16%|█▌        | 79/497 [03:53<18:27,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2550.02 ms /    77 tokens\n",
      " 16%|█▌        | 80/497 [03:56<18:15,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3480.35 ms /    96 tokens\n",
      " 16%|█▋        | 81/497 [03:59<20:04,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2988.64 ms /    94 tokens\n",
      " 16%|█▋        | 82/497 [04:02<20:15,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2989.92 ms /    93 tokens\n",
      " 17%|█▋        | 83/497 [04:05<20:23,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3986.64 ms /   116 tokens\n",
      " 17%|█▋        | 84/497 [04:09<22:33,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3325.65 ms /   101 tokens\n",
      " 17%|█▋        | 85/497 [04:13<22:39,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3990.62 ms /   127 tokens\n",
      " 17%|█▋        | 86/497 [04:17<24:05,  3.52s/it]Llama.generate: 86 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.71 ms /   136 tokens\n",
      " 18%|█▊        | 87/497 [04:21<25:28,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2718.05 ms /    86 tokens\n",
      " 18%|█▊        | 88/497 [04:24<23:25,  3.44s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2539.85 ms /    78 tokens\n",
      " 18%|█▊        | 89/497 [04:26<21:36,  3.18s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1992.51 ms /    60 tokens\n",
      " 18%|█▊        | 90/497 [04:28<19:11,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2672.97 ms /    79 tokens\n",
      " 18%|█▊        | 91/497 [04:31<18:53,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3240.06 ms /    98 tokens\n",
      " 19%|█▊        | 92/497 [04:34<19:47,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3056.64 ms /    97 tokens\n",
      " 19%|█▊        | 93/497 [04:37<20:03,  2.98s/it]Llama.generate: 88 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2048.66 ms /    63 tokens\n",
      " 19%|█▉        | 94/497 [04:39<18:12,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.75 ms /   109 tokens\n",
      " 19%|█▉        | 95/497 [04:43<19:46,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1906.08 ms /    57 tokens\n",
      " 19%|█▉        | 96/497 [04:45<17:43,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3370.54 ms /   108 tokens\n",
      " 20%|█▉        | 97/497 [04:48<19:08,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4398.35 ms /   138 tokens\n",
      " 20%|█▉        | 98/497 [04:53<22:13,  3.34s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2583.36 ms /    80 tokens\n",
      " 20%|█▉        | 99/497 [04:55<20:44,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2381.90 ms /    74 tokens\n",
      " 20%|██        | 100/497 [04:58<19:16,  2.91s/it]Llama.generate: 91 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5002.68 ms /   139 tokens\n",
      " 20%|██        | 101/497 [05:03<23:26,  3.55s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4113.58 ms /   122 tokens\n",
      " 21%|██        | 102/497 [05:07<24:32,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2515.54 ms /    78 tokens\n",
      " 21%|██        | 103/497 [05:09<22:08,  3.37s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2679.43 ms /    84 tokens\n",
      " 21%|██        | 104/497 [05:12<20:49,  3.18s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1901.72 ms /    60 tokens\n",
      " 21%|██        | 105/497 [05:14<18:21,  2.81s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2238.10 ms /    70 tokens\n",
      " 21%|██▏       | 106/497 [05:16<17:13,  2.64s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1975.72 ms /    60 tokens\n",
      " 22%|██▏       | 107/497 [05:18<15:57,  2.45s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3128.36 ms /   102 tokens\n",
      " 22%|██▏       | 108/497 [05:22<17:16,  2.66s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2727.52 ms /    90 tokens\n",
      " 22%|██▏       | 109/497 [05:24<17:26,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4511.72 ms /   144 tokens\n",
      " 22%|██▏       | 110/497 [05:29<20:58,  3.25s/it]Llama.generate: 88 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3087.29 ms /    93 tokens\n",
      " 22%|██▏       | 111/497 [05:32<20:37,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3925.17 ms /   123 tokens\n",
      " 23%|██▎       | 112/497 [05:36<22:02,  3.43s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2847.33 ms /    90 tokens\n",
      " 23%|██▎       | 113/497 [05:39<20:55,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2387.40 ms /    72 tokens\n",
      " 23%|██▎       | 114/497 [05:41<19:13,  3.01s/it]Llama.generate: 87 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4544.54 ms /   143 tokens\n",
      " 23%|██▎       | 115/497 [05:46<22:09,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2728.86 ms /    87 tokens\n",
      " 23%|██▎       | 116/497 [05:49<20:43,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2239.07 ms /    71 tokens\n",
      " 24%|██▎       | 117/497 [05:51<18:48,  2.97s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3777.75 ms /   105 tokens\n",
      " 24%|██▎       | 118/497 [05:55<20:21,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.45 ms /    85 tokens\n",
      " 24%|██▍       | 119/497 [05:58<19:30,  3.10s/it]Llama.generate: 93 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2134.51 ms /    63 tokens\n",
      " 24%|██▍       | 120/497 [06:00<17:40,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2629.58 ms /    82 tokens\n",
      " 24%|██▍       | 121/497 [06:02<17:21,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2079.90 ms /    62 tokens\n",
      " 25%|██▍       | 122/497 [06:04<16:03,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3100.51 ms /    97 tokens\n",
      " 25%|██▍       | 123/497 [06:08<17:03,  2.74s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3222.14 ms /    98 tokens\n",
      " 25%|██▍       | 124/497 [06:11<17:58,  2.89s/it]Llama.generate: 88 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2511.01 ms /    78 tokens\n",
      " 25%|██▌       | 125/497 [06:13<17:16,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2356.80 ms /    71 tokens\n",
      " 25%|██▌       | 126/497 [06:16<16:30,  2.67s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2160.62 ms /    66 tokens\n",
      " 26%|██▌       | 127/497 [06:18<15:34,  2.53s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2471.72 ms /    76 tokens\n",
      " 26%|██▌       | 128/497 [06:20<15:29,  2.52s/it]Llama.generate: 89 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.14 ms /    94 tokens\n",
      " 26%|██▌       | 129/497 [06:24<16:54,  2.76s/it]Llama.generate: 89 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2350.18 ms /    72 tokens\n",
      " 26%|██▌       | 130/497 [06:26<16:09,  2.64s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2757.35 ms /    91 tokens\n",
      " 26%|██▋       | 131/497 [06:29<16:22,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4121.30 ms /   122 tokens\n",
      " 27%|██▋       | 132/497 [06:33<19:01,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2369.89 ms /    73 tokens\n",
      " 27%|██▋       | 133/497 [06:35<17:39,  2.91s/it]Llama.generate: 87 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2807.76 ms /    90 tokens\n",
      " 27%|██▋       | 134/497 [06:38<17:29,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2671.61 ms /    82 tokens\n",
      " 27%|██▋       | 135/497 [06:41<17:05,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4777.63 ms /   140 tokens\n",
      " 27%|██▋       | 136/497 [06:46<20:36,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2232.16 ms /    69 tokens\n",
      " 28%|██▊       | 137/497 [06:48<18:27,  3.08s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2361.15 ms /    77 tokens\n",
      " 28%|██▊       | 138/497 [06:50<17:10,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 27 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    27 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3743.62 ms /   122 tokens\n",
      " 28%|██▊       | 139/497 [06:54<18:44,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4673.90 ms /   134 tokens\n",
      " 28%|██▊       | 140/497 [06:59<21:28,  3.61s/it]Llama.generate: 89 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2945.88 ms /    92 tokens\n",
      " 28%|██▊       | 141/497 [07:02<20:17,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2119.93 ms /    65 tokens\n",
      " 29%|██▊       | 142/497 [07:04<17:59,  3.04s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4046.59 ms /   132 tokens\n",
      " 29%|██▉       | 143/497 [07:08<19:46,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2294.27 ms /    74 tokens\n",
      " 29%|██▉       | 144/497 [07:11<17:56,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2314.77 ms /    70 tokens\n",
      " 29%|██▉       | 145/497 [07:13<16:37,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3695.76 ms /   112 tokens\n",
      " 29%|██▉       | 146/497 [07:17<18:09,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2274.50 ms /    71 tokens\n",
      " 30%|██▉       | 147/497 [07:19<16:42,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1970.45 ms /    61 tokens\n",
      " 30%|██▉       | 148/497 [07:21<15:09,  2.60s/it]Llama.generate: 87 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3350.21 ms /   107 tokens\n",
      " 30%|██▉       | 149/497 [07:24<16:27,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2214.65 ms /    72 tokens\n",
      " 30%|███       | 150/497 [07:26<15:21,  2.66s/it]Llama.generate: 87 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2992.50 ms /   101 tokens\n",
      " 30%|███       | 151/497 [07:30<15:57,  2.77s/it]Llama.generate: 88 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2241.87 ms /    71 tokens\n",
      " 31%|███       | 152/497 [07:32<15:04,  2.62s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3770.83 ms /   114 tokens\n",
      " 31%|███       | 153/497 [07:36<17:03,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5129.14 ms /   172 tokens\n",
      " 31%|███       | 154/497 [07:41<20:45,  3.63s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3320.84 ms /   104 tokens\n",
      " 31%|███       | 155/497 [07:44<20:12,  3.55s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.07 ms /   103 tokens\n",
      " 31%|███▏      | 156/497 [07:47<19:33,  3.44s/it]Llama.generate: 87 prefix-match hit, remaining 3 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     3 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4205.26 ms /   116 tokens\n",
      " 32%|███▏      | 157/497 [07:52<20:51,  3.68s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2437.41 ms /    75 tokens\n",
      " 32%|███▏      | 158/497 [07:54<18:44,  3.32s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2333.41 ms /    71 tokens\n",
      " 32%|███▏      | 159/497 [07:56<17:04,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2138.67 ms /    67 tokens\n",
      " 32%|███▏      | 160/497 [07:59<15:33,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3071.29 ms /    98 tokens\n",
      " 32%|███▏      | 161/497 [08:02<16:03,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3069.07 ms /    96 tokens\n",
      " 33%|███▎      | 162/497 [08:05<16:24,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3532.84 ms /   110 tokens\n",
      " 33%|███▎      | 163/497 [08:08<17:23,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1984.60 ms /    62 tokens\n",
      " 33%|███▎      | 164/497 [08:10<15:29,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2058.41 ms /    63 tokens\n",
      " 33%|███▎      | 165/497 [08:12<14:17,  2.58s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2487.16 ms /    78 tokens\n",
      " 33%|███▎      | 166/497 [08:15<14:09,  2.57s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4308.09 ms /   120 tokens\n",
      " 34%|███▎      | 167/497 [08:19<17:02,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2966.78 ms /    92 tokens\n",
      " 34%|███▍      | 168/497 [08:22<16:48,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3710.88 ms /   117 tokens\n",
      " 34%|███▍      | 169/497 [08:26<17:52,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2889.41 ms /    91 tokens\n",
      " 34%|███▍      | 170/497 [08:29<17:15,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2392.91 ms /    73 tokens\n",
      " 34%|███▍      | 171/497 [08:31<15:58,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6214.71 ms /   171 tokens\n",
      " 35%|███▍      | 172/497 [08:38<21:17,  3.93s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2192.17 ms /    67 tokens\n",
      " 35%|███▍      | 173/497 [08:40<18:27,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 51 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7359.80 ms /   226 tokens\n",
      " 35%|███▌      | 174/497 [08:47<24:49,  4.61s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2026.38 ms /    66 tokens\n",
      " 35%|███▌      | 175/497 [08:49<20:37,  3.84s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3062.68 ms /    92 tokens\n",
      " 35%|███▌      | 176/497 [08:52<19:21,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2424.71 ms /    71 tokens\n",
      " 36%|███▌      | 177/497 [08:55<17:25,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1905.56 ms /    55 tokens\n",
      " 36%|███▌      | 178/497 [08:57<15:15,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2356.55 ms /    69 tokens\n",
      " 36%|███▌      | 179/497 [08:59<14:26,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2381.17 ms /    76 tokens\n",
      " 36%|███▌      | 180/497 [09:02<13:53,  2.63s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2078.81 ms /    64 tokens\n",
      " 36%|███▋      | 181/497 [09:04<13:03,  2.48s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2275.41 ms /    70 tokens\n",
      " 37%|███▋      | 182/497 [09:06<12:44,  2.43s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3356.33 ms /   102 tokens\n",
      " 37%|███▋      | 183/497 [09:09<14:12,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3305.98 ms /   107 tokens\n",
      " 37%|███▋      | 184/497 [09:13<15:08,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2264.57 ms /    66 tokens\n",
      " 37%|███▋      | 185/497 [09:15<14:08,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3084.81 ms /    95 tokens\n",
      " 37%|███▋      | 186/497 [09:18<14:42,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1904.17 ms /    58 tokens\n",
      " 38%|███▊      | 187/497 [09:20<13:16,  2.57s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2656.14 ms /    84 tokens\n",
      " 38%|███▊      | 188/497 [09:23<13:24,  2.60s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1591.46 ms /    52 tokens\n",
      " 38%|███▊      | 189/497 [09:24<11:51,  2.31s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2437.73 ms /    78 tokens\n",
      " 38%|███▊      | 190/497 [09:27<12:04,  2.36s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2209.48 ms /    69 tokens\n",
      " 38%|███▊      | 191/497 [09:29<11:51,  2.33s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.74 ms /    90 tokens\n",
      " 39%|███▊      | 192/497 [09:32<12:40,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4254.74 ms /   130 tokens\n",
      " 39%|███▉      | 193/497 [09:36<15:22,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2117.37 ms /    68 tokens\n",
      " 39%|███▉      | 194/497 [09:38<13:58,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3677.75 ms /   113 tokens\n",
      " 39%|███▉      | 195/497 [09:42<15:20,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3092.38 ms /    93 tokens\n",
      " 39%|███▉      | 196/497 [09:45<15:24,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4120.95 ms /   125 tokens\n",
      " 40%|███▉      | 197/497 [09:49<16:58,  3.39s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2963.61 ms /    91 tokens\n",
      " 40%|███▉      | 198/497 [09:52<16:20,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2327.89 ms /    72 tokens\n",
      " 40%|████      | 199/497 [09:55<14:53,  3.00s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2547.52 ms /    81 tokens\n",
      " 40%|████      | 200/497 [09:57<14:13,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2838.43 ms /    90 tokens\n",
      " 40%|████      | 201/497 [10:00<14:10,  2.87s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2225.17 ms /    69 tokens\n",
      " 41%|████      | 202/497 [10:02<13:11,  2.68s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2725.84 ms /    86 tokens\n",
      " 41%|████      | 203/497 [10:05<13:15,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2112.39 ms /    63 tokens\n",
      " 41%|████      | 204/497 [10:07<12:23,  2.54s/it]Llama.generate: 87 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2937.70 ms /    90 tokens\n",
      " 41%|████      | 205/497 [10:10<12:58,  2.67s/it]Llama.generate: 88 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2121.46 ms /    64 tokens\n",
      " 41%|████▏     | 206/497 [10:12<12:10,  2.51s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3772.70 ms /   120 tokens\n",
      " 42%|████▏     | 207/497 [10:16<14:00,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4161.07 ms /   126 tokens\n",
      " 42%|████▏     | 208/497 [10:20<15:51,  3.29s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.71 ms /    87 tokens\n",
      " 42%|████▏     | 209/497 [10:23<15:27,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3635.05 ms /   113 tokens\n",
      " 42%|████▏     | 210/497 [10:27<16:01,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2033.30 ms /    64 tokens\n",
      " 42%|████▏     | 211/497 [10:29<14:08,  2.97s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3712.14 ms /   109 tokens\n",
      " 43%|████▎     | 212/497 [10:33<15:12,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2596.67 ms /    81 tokens\n",
      " 43%|████▎     | 213/497 [10:36<14:18,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5890.43 ms /   166 tokens\n",
      " 43%|████▎     | 214/497 [10:42<18:22,  3.90s/it]Llama.generate: 87 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3323.56 ms /   108 tokens\n",
      " 43%|████▎     | 215/497 [10:45<17:32,  3.73s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3567.79 ms /   107 tokens\n",
      " 43%|████▎     | 216/497 [10:48<17:16,  3.69s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3188.41 ms /   103 tokens\n",
      " 44%|████▎     | 217/497 [10:52<16:33,  3.55s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3316.29 ms /   100 tokens\n",
      " 44%|████▍     | 218/497 [10:55<16:13,  3.49s/it]Llama.generate: 86 prefix-match hit, remaining 35 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5710.59 ms /   174 tokens\n",
      " 44%|████▍     | 219/497 [11:01<19:18,  4.17s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2242.65 ms /    65 tokens\n",
      " 44%|████▍     | 220/497 [11:03<16:36,  3.60s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2474.79 ms /    76 tokens\n",
      " 44%|████▍     | 221/497 [11:06<15:02,  3.27s/it]Llama.generate: 87 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5717.33 ms /   173 tokens\n",
      " 45%|████▍     | 222/497 [11:11<18:22,  4.01s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.90 ms /    98 tokens\n",
      " 45%|████▍     | 223/497 [11:14<17:07,  3.75s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2422.35 ms /    76 tokens\n",
      " 45%|████▌     | 224/497 [11:17<15:17,  3.36s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6104.98 ms /   169 tokens\n",
      " 45%|████▌     | 225/497 [11:23<19:00,  4.19s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3154.39 ms /   102 tokens\n",
      " 45%|████▌     | 226/497 [11:26<17:34,  3.89s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3441.18 ms /   104 tokens\n",
      " 46%|████▌     | 227/497 [11:30<16:56,  3.76s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3496.50 ms /   101 tokens\n",
      " 46%|████▌     | 228/497 [11:33<16:31,  3.69s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2510.02 ms /    79 tokens\n",
      " 46%|████▌     | 229/497 [11:36<14:57,  3.35s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3026.12 ms /    93 tokens\n",
      " 46%|████▋     | 230/497 [11:39<14:30,  3.26s/it]Llama.generate: 89 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3118.61 ms /   101 tokens\n",
      " 46%|████▋     | 231/497 [11:42<14:18,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2535.01 ms /    73 tokens\n",
      " 47%|████▋     | 232/497 [11:44<13:22,  3.03s/it]Llama.generate: 87 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3569.65 ms /   115 tokens\n",
      " 47%|████▋     | 233/497 [11:48<14:04,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2102.81 ms /    63 tokens\n",
      " 47%|████▋     | 234/497 [11:50<12:39,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2707.35 ms /    85 tokens\n",
      " 47%|████▋     | 235/497 [11:53<12:23,  2.84s/it]Llama.generate: 87 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3537.49 ms /   113 tokens\n",
      " 47%|████▋     | 236/497 [11:57<13:19,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2733.13 ms /    84 tokens\n",
      " 48%|████▊     | 237/497 [11:59<12:52,  2.97s/it]Llama.generate: 87 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5302.39 ms /   159 tokens\n",
      " 48%|████▊     | 238/497 [12:05<15:53,  3.68s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3264.26 ms /    95 tokens\n",
      " 48%|████▊     | 239/497 [12:08<15:21,  3.57s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3210.56 ms /   106 tokens\n",
      " 48%|████▊     | 240/497 [12:11<14:50,  3.47s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2601.31 ms /    78 tokens\n",
      " 48%|████▊     | 241/497 [12:14<13:43,  3.22s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2848.55 ms /    88 tokens\n",
      " 49%|████▊     | 242/497 [12:17<13:14,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1735.90 ms /    54 tokens\n",
      " 49%|████▉     | 243/497 [12:18<11:28,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2164.06 ms /    74 tokens\n",
      " 49%|████▉     | 244/497 [12:21<10:46,  2.55s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2800.31 ms /    80 tokens\n",
      " 49%|████▉     | 245/497 [12:23<11:04,  2.64s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2517.96 ms /    78 tokens\n",
      " 49%|████▉     | 246/497 [12:26<10:56,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3048.96 ms /    87 tokens\n",
      " 50%|████▉     | 247/497 [12:29<11:28,  2.75s/it]Llama.generate: 88 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2782.43 ms /    88 tokens\n",
      " 50%|████▉     | 248/497 [12:32<11:30,  2.77s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3509.53 ms /   102 tokens\n",
      " 50%|█████     | 249/497 [12:35<12:24,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3917.78 ms /   123 tokens\n",
      " 50%|█████     | 250/497 [12:39<13:32,  3.29s/it]Llama.generate: 89 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2284.42 ms /    68 tokens\n",
      " 51%|█████     | 251/497 [12:42<12:16,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4571.11 ms /   147 tokens\n",
      " 51%|█████     | 252/497 [12:46<14:12,  3.48s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3111.00 ms /    99 tokens\n",
      " 51%|█████     | 253/497 [12:49<13:42,  3.37s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2698.80 ms /    82 tokens\n",
      " 51%|█████     | 254/497 [12:52<12:54,  3.19s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2094.56 ms /    66 tokens\n",
      " 51%|█████▏    | 255/497 [12:54<11:34,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3499.78 ms /   100 tokens\n",
      " 52%|█████▏    | 256/497 [12:58<12:20,  3.07s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1763.15 ms /    54 tokens\n",
      " 52%|█████▏    | 257/497 [13:00<10:44,  2.69s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2800.55 ms /    88 tokens\n",
      " 52%|█████▏    | 258/497 [13:03<10:51,  2.73s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2988.58 ms /    90 tokens\n",
      " 52%|█████▏    | 259/497 [13:06<11:10,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2752.76 ms /    80 tokens\n",
      " 52%|█████▏    | 260/497 [13:08<11:05,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2409.62 ms /    76 tokens\n",
      " 53%|█████▎    | 261/497 [13:11<10:37,  2.70s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.82 ms /    90 tokens\n",
      " 53%|█████▎    | 262/497 [13:14<10:57,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2275.53 ms /    73 tokens\n",
      " 53%|█████▎    | 263/497 [13:16<10:19,  2.65s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2010.64 ms /    61 tokens\n",
      " 53%|█████▎    | 264/497 [13:18<09:34,  2.47s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2483.99 ms /    77 tokens\n",
      " 53%|█████▎    | 265/497 [13:21<09:35,  2.48s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2448.01 ms /    76 tokens\n",
      " 54%|█████▎    | 266/497 [13:23<09:34,  2.49s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4447.93 ms /   131 tokens\n",
      " 54%|█████▎    | 267/497 [13:28<11:48,  3.08s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3191.15 ms /   101 tokens\n",
      " 54%|█████▍    | 268/497 [13:31<11:55,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2152.25 ms /    65 tokens\n",
      " 54%|█████▍    | 269/497 [13:33<10:47,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3274.65 ms /    99 tokens\n",
      " 54%|█████▍    | 270/497 [13:36<11:17,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2419.86 ms /    74 tokens\n",
      " 55%|█████▍    | 271/497 [13:39<10:37,  2.82s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3804.86 ms /   108 tokens\n",
      " 55%|█████▍    | 272/497 [13:43<11:43,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2473.27 ms /    76 tokens\n",
      " 55%|█████▍    | 273/497 [13:45<10:58,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3124.34 ms /    97 tokens\n",
      " 55%|█████▌    | 274/497 [13:48<11:09,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2906.46 ms /    87 tokens\n",
      " 55%|█████▌    | 275/497 [13:51<11:01,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1700.79 ms /    51 tokens\n",
      " 56%|█████▌    | 276/497 [13:53<09:36,  2.61s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2292.23 ms /    70 tokens\n",
      " 56%|█████▌    | 277/497 [13:55<09:15,  2.52s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2947.04 ms /    83 tokens\n",
      " 56%|█████▌    | 278/497 [13:58<09:42,  2.66s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2640.12 ms /    80 tokens\n",
      " 56%|█████▌    | 279/497 [14:01<09:40,  2.66s/it]Llama.generate: 89 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2695.89 ms /    81 tokens\n",
      " 56%|█████▋    | 280/497 [14:04<09:42,  2.69s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2292.88 ms /    70 tokens\n",
      " 57%|█████▋    | 281/497 [14:06<09:16,  2.58s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2203.54 ms /    68 tokens\n",
      " 57%|█████▋    | 282/497 [14:08<08:51,  2.47s/it]Llama.generate: 87 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    43 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1575.15 ms /    48 tokens\n",
      " 57%|█████▋    | 283/497 [14:10<07:53,  2.21s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2814.73 ms /    86 tokens\n",
      " 57%|█████▋    | 284/497 [14:13<08:31,  2.40s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2071.02 ms /    66 tokens\n",
      " 57%|█████▋    | 285/497 [14:15<08:09,  2.31s/it]Llama.generate: 87 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3443.85 ms /   112 tokens\n",
      " 58%|█████▊    | 286/497 [14:18<09:22,  2.67s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1812.06 ms /    58 tokens\n",
      " 58%|█████▊    | 287/497 [14:20<08:27,  2.42s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3764.64 ms /   121 tokens\n",
      " 58%|█████▊    | 288/497 [14:24<09:52,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1999.94 ms /    61 tokens\n",
      " 58%|█████▊    | 289/497 [14:26<08:58,  2.59s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2316.44 ms /    70 tokens\n",
      " 58%|█████▊    | 290/497 [14:28<08:41,  2.52s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2044.84 ms /    65 tokens\n",
      " 59%|█████▊    | 291/497 [14:30<08:11,  2.38s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2306.75 ms /    70 tokens\n",
      " 59%|█████▉    | 292/497 [14:33<08:05,  2.37s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4059.80 ms /   123 tokens\n",
      " 59%|█████▉    | 293/497 [14:37<09:48,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3200.54 ms /    98 tokens\n",
      " 59%|█████▉    | 294/497 [14:40<10:06,  2.99s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2354.68 ms /    72 tokens\n",
      " 59%|█████▉    | 295/497 [14:42<09:28,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2690.58 ms /    83 tokens\n",
      " 60%|█████▉    | 296/497 [14:45<09:19,  2.78s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1998.33 ms /    61 tokens\n",
      " 60%|█████▉    | 297/497 [14:47<08:32,  2.56s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3343.43 ms /   106 tokens\n",
      " 60%|█████▉    | 298/497 [14:51<09:17,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3293.61 ms /   104 tokens\n",
      " 60%|██████    | 299/497 [14:54<09:46,  2.96s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4138.61 ms /   128 tokens\n",
      " 60%|██████    | 300/497 [14:58<10:55,  3.32s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2261.64 ms /    71 tokens\n",
      " 61%|██████    | 301/497 [15:00<09:51,  3.02s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3228.19 ms /    95 tokens\n",
      " 61%|██████    | 302/497 [15:04<10:01,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 30 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5330.63 ms /   159 tokens\n",
      " 61%|██████    | 303/497 [15:09<12:11,  3.77s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2516.47 ms /    74 tokens\n",
      " 61%|██████    | 304/497 [15:11<10:57,  3.40s/it]Llama.generate: 87 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3300.36 ms /   100 tokens\n",
      " 61%|██████▏   | 305/497 [15:15<10:49,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2269.45 ms /    71 tokens\n",
      " 62%|██████▏   | 306/497 [15:17<09:44,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2387.70 ms /    75 tokens\n",
      " 62%|██████▏   | 307/497 [15:20<09:04,  2.87s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2791.76 ms /    83 tokens\n",
      " 62%|██████▏   | 308/497 [15:22<08:58,  2.85s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2753.03 ms /    85 tokens\n",
      " 62%|██████▏   | 309/497 [15:25<08:52,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2306.63 ms /    73 tokens\n",
      " 62%|██████▏   | 310/497 [15:27<08:21,  2.68s/it]Llama.generate: 87 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3031.86 ms /    90 tokens\n",
      " 63%|██████▎   | 311/497 [15:31<08:40,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2808.71 ms /    87 tokens\n",
      " 63%|██████▎   | 312/497 [15:33<08:39,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2754.38 ms /    88 tokens\n",
      " 63%|██████▎   | 313/497 [15:36<08:35,  2.80s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4093.41 ms /   129 tokens\n",
      " 63%|██████▎   | 314/497 [15:40<09:46,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3865.44 ms /   120 tokens\n",
      " 63%|██████▎   | 315/497 [15:44<10:20,  3.41s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2456.64 ms /    73 tokens\n",
      " 64%|██████▎   | 316/497 [15:47<09:27,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.91 ms /   108 tokens\n",
      " 64%|██████▍   | 317/497 [15:50<09:41,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3084.05 ms /    99 tokens\n",
      " 64%|██████▍   | 318/497 [15:53<09:32,  3.20s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3849.36 ms /   111 tokens\n",
      " 64%|██████▍   | 319/497 [15:57<10:05,  3.40s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4189.29 ms /   123 tokens\n",
      " 64%|██████▍   | 320/497 [16:01<10:45,  3.65s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3392.06 ms /   106 tokens\n",
      " 65%|██████▍   | 321/497 [16:05<10:30,  3.58s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3561.32 ms /   102 tokens\n",
      " 65%|██████▍   | 322/497 [16:08<10:27,  3.58s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2877.19 ms /    91 tokens\n",
      " 65%|██████▍   | 323/497 [16:11<09:47,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2461.61 ms /    73 tokens\n",
      " 65%|██████▌   | 324/497 [16:14<08:58,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3086.59 ms /    96 tokens\n",
      " 65%|██████▌   | 325/497 [16:17<08:55,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3771.99 ms /   121 tokens\n",
      " 66%|██████▌   | 326/497 [16:21<09:28,  3.32s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4375.00 ms /   126 tokens\n",
      " 66%|██████▌   | 327/497 [16:25<10:20,  3.65s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2873.41 ms /    90 tokens\n",
      " 66%|██████▌   | 328/497 [16:28<09:38,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1932.74 ms /    60 tokens\n",
      " 66%|██████▌   | 329/497 [16:30<08:21,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2478.89 ms /    79 tokens\n",
      " 66%|██████▋   | 330/497 [16:32<07:54,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3194.68 ms /   101 tokens\n",
      " 67%|██████▋   | 331/497 [16:36<08:10,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2520.20 ms /    79 tokens\n",
      " 67%|██████▋   | 332/497 [16:38<07:47,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2495.13 ms /    78 tokens\n",
      " 67%|██████▋   | 333/497 [16:41<07:29,  2.74s/it]Llama.generate: 89 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3719.53 ms /    98 tokens\n",
      " 67%|██████▋   | 334/497 [16:45<08:16,  3.04s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3744.55 ms /   100 tokens\n",
      " 67%|██████▋   | 335/497 [16:48<08:48,  3.26s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3393.47 ms /    86 tokens\n",
      " 68%|██████▊   | 336/497 [16:52<08:53,  3.32s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4372.08 ms /   115 tokens\n",
      " 68%|██████▊   | 337/497 [16:56<09:42,  3.64s/it]Llama.generate: 87 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3228.83 ms /    85 tokens\n",
      " 68%|██████▊   | 338/497 [16:59<09:20,  3.53s/it]Llama.generate: 87 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3411.97 ms /    91 tokens\n",
      " 68%|██████▊   | 339/497 [17:03<09:13,  3.50s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3061.21 ms /    81 tokens\n",
      " 68%|██████▊   | 340/497 [17:06<08:50,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4478.88 ms /   120 tokens\n",
      " 69%|██████▊   | 341/497 [17:10<09:39,  3.72s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2684.04 ms /    75 tokens\n",
      " 69%|██████▉   | 342/497 [17:13<08:50,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3855.85 ms /   102 tokens\n",
      " 69%|██████▉   | 343/497 [17:17<09:08,  3.56s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2450.88 ms /    70 tokens\n",
      " 69%|██████▉   | 344/497 [17:20<08:15,  3.24s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2532.91 ms /    69 tokens\n",
      " 69%|██████▉   | 345/497 [17:22<07:41,  3.04s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3212.21 ms /    87 tokens\n",
      " 70%|██████▉   | 346/497 [17:25<07:47,  3.10s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2578.63 ms /    72 tokens\n",
      " 70%|██████▉   | 347/497 [17:28<07:22,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1803.33 ms /    52 tokens\n",
      " 70%|███████   | 348/497 [17:30<06:30,  2.62s/it]Llama.generate: 88 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3800.36 ms /   104 tokens\n",
      " 70%|███████   | 349/497 [17:34<07:21,  2.98s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2601.38 ms /    74 tokens\n",
      " 70%|███████   | 350/497 [17:36<07:02,  2.88s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4331.54 ms /   114 tokens\n",
      " 71%|███████   | 351/497 [17:41<08:05,  3.32s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2314.58 ms /    62 tokens\n",
      " 71%|███████   | 352/497 [17:43<07:19,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2488.45 ms /    68 tokens\n",
      " 71%|███████   | 353/497 [17:46<06:54,  2.88s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3256.37 ms /    89 tokens\n",
      " 71%|███████   | 354/497 [17:49<07:09,  3.00s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2491.36 ms /    67 tokens\n",
      " 71%|███████▏  | 355/497 [17:51<06:45,  2.86s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3393.12 ms /    89 tokens\n",
      " 72%|███████▏  | 356/497 [17:55<07:06,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1770.74 ms /    47 tokens\n",
      " 72%|███████▏  | 357/497 [17:57<06:12,  2.66s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4649.36 ms /   123 tokens\n",
      " 72%|███████▏  | 358/497 [18:01<07:34,  3.27s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3189.84 ms /    85 tokens\n",
      " 72%|███████▏  | 359/497 [18:04<07:29,  3.25s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4486.46 ms /   115 tokens\n",
      " 72%|███████▏  | 360/497 [18:09<08:17,  3.63s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2636.82 ms /    70 tokens\n",
      " 73%|███████▎  | 361/497 [18:12<07:34,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 32 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    32 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8122.97 ms /   204 tokens\n",
      " 73%|███████▎  | 362/497 [18:20<10:46,  4.79s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2347.52 ms /    64 tokens\n",
      " 73%|███████▎  | 363/497 [18:22<09:05,  4.07s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3598.22 ms /   100 tokens\n",
      " 73%|███████▎  | 364/497 [18:26<08:43,  3.94s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2839.81 ms /    78 tokens\n",
      " 73%|███████▎  | 365/497 [18:29<07:57,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5097.08 ms /   134 tokens\n",
      " 74%|███████▎  | 366/497 [18:34<08:53,  4.07s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4258.71 ms /   111 tokens\n",
      " 74%|███████▍  | 367/497 [18:38<08:57,  4.14s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2374.69 ms /    65 tokens\n",
      " 74%|███████▍  | 368/497 [18:41<07:46,  3.61s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2611.78 ms /    68 tokens\n",
      " 74%|███████▍  | 369/497 [18:43<07:05,  3.33s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2862.93 ms /    77 tokens\n",
      " 74%|███████▍  | 370/497 [18:46<06:46,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3811.64 ms /    96 tokens\n",
      " 75%|███████▍  | 371/497 [18:50<07:06,  3.39s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3609.72 ms /    97 tokens\n",
      " 75%|███████▍  | 372/497 [18:54<07:13,  3.47s/it]Llama.generate: 89 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2297.47 ms /    62 tokens\n",
      " 75%|███████▌  | 373/497 [18:56<06:27,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 26 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    26 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4005.20 ms /   106 tokens\n",
      " 75%|███████▌  | 374/497 [19:00<06:58,  3.40s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2726.25 ms /    80 tokens\n",
      " 75%|███████▌  | 375/497 [19:03<06:31,  3.21s/it]Llama.generate: 88 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3437.63 ms /    87 tokens\n",
      " 76%|███████▌  | 376/497 [19:06<06:38,  3.29s/it]Llama.generate: 87 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1945.38 ms /    53 tokens\n",
      " 76%|███████▌  | 377/497 [19:08<05:47,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3330.20 ms /    84 tokens\n",
      " 76%|███████▌  | 378/497 [19:12<06:01,  3.04s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4061.00 ms /   107 tokens\n",
      " 76%|███████▋  | 379/497 [19:16<06:35,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3846.35 ms /   104 tokens\n",
      " 76%|███████▋  | 380/497 [19:19<06:50,  3.51s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4227.26 ms /   110 tokens\n",
      " 77%|███████▋  | 381/497 [19:24<07:13,  3.73s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3019.68 ms /    77 tokens\n",
      " 77%|███████▋  | 382/497 [19:27<06:45,  3.53s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4342.31 ms /   120 tokens\n",
      " 77%|███████▋  | 383/497 [19:31<07:11,  3.78s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3222.94 ms /    85 tokens\n",
      " 77%|███████▋  | 384/497 [19:34<06:49,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2094.90 ms /    56 tokens\n",
      " 77%|███████▋  | 385/497 [19:37<05:55,  3.17s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2214.39 ms /    60 tokens\n",
      " 78%|███████▊  | 386/497 [19:39<05:21,  2.90s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3365.28 ms /    87 tokens\n",
      " 78%|███████▊  | 387/497 [19:42<05:35,  3.05s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3705.02 ms /   104 tokens\n",
      " 78%|███████▊  | 388/497 [19:46<05:54,  3.25s/it]Llama.generate: 86 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1845.81 ms /    53 tokens\n",
      " 78%|███████▊  | 389/497 [19:48<05:06,  2.84s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2426.65 ms /    69 tokens\n",
      " 78%|███████▊  | 390/497 [19:50<04:51,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3716.11 ms /   103 tokens\n",
      " 79%|███████▊  | 391/497 [19:54<05:21,  3.03s/it]Llama.generate: 87 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3397.87 ms /    90 tokens\n",
      " 79%|███████▉  | 392/497 [19:57<05:30,  3.15s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4343.46 ms /   117 tokens\n",
      " 79%|███████▉  | 393/497 [20:02<06:06,  3.52s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2697.69 ms /    76 tokens\n",
      " 79%|███████▉  | 394/497 [20:05<05:37,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2119.08 ms /    60 tokens\n",
      " 79%|███████▉  | 395/497 [20:07<05:00,  2.94s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3786.32 ms /   106 tokens\n",
      " 80%|███████▉  | 396/497 [20:10<05:23,  3.20s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4993.88 ms /   135 tokens\n",
      " 80%|███████▉  | 397/497 [20:16<06:15,  3.75s/it]Llama.generate: 87 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4818.77 ms /   126 tokens\n",
      " 80%|████████  | 398/497 [20:20<06:43,  4.08s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3829.99 ms /   101 tokens\n",
      " 80%|████████  | 399/497 [20:24<06:33,  4.01s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2255.34 ms /    62 tokens\n",
      " 80%|████████  | 400/497 [20:27<05:39,  3.49s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4240.27 ms /   116 tokens\n",
      " 81%|████████  | 401/497 [20:31<05:57,  3.73s/it]Llama.generate: 89 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2393.00 ms /    65 tokens\n",
      " 81%|████████  | 402/497 [20:33<05:16,  3.34s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2397.03 ms /    66 tokens\n",
      " 81%|████████  | 403/497 [20:36<04:47,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4587.40 ms /   122 tokens\n",
      " 81%|████████▏ | 404/497 [20:40<05:28,  3.53s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2480.33 ms /    69 tokens\n",
      " 81%|████████▏ | 405/497 [20:43<04:56,  3.23s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.02 ms /    83 tokens\n",
      " 82%|████████▏ | 406/497 [20:46<04:47,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2720.85 ms /    74 tokens\n",
      " 82%|████████▏ | 407/497 [20:49<04:33,  3.03s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2730.61 ms /    74 tokens\n",
      " 82%|████████▏ | 408/497 [20:51<04:22,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4290.46 ms /   116 tokens\n",
      " 82%|████████▏ | 409/497 [20:56<04:55,  3.36s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3346.04 ms /    97 tokens\n",
      " 82%|████████▏ | 410/497 [20:59<04:52,  3.37s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4163.95 ms /   115 tokens\n",
      " 83%|████████▎ | 411/497 [21:03<05:11,  3.62s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2671.59 ms /    70 tokens\n",
      " 83%|████████▎ | 412/497 [21:06<04:43,  3.34s/it]Llama.generate: 87 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3949.06 ms /   103 tokens\n",
      " 83%|████████▎ | 413/497 [21:10<04:56,  3.53s/it]Llama.generate: 87 prefix-match hit, remaining 29 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5465.71 ms /   141 tokens\n",
      " 83%|████████▎ | 414/497 [21:15<05:42,  4.12s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2229.59 ms /    55 tokens\n",
      " 84%|████████▎ | 415/497 [21:18<04:52,  3.56s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3640.16 ms /   102 tokens\n",
      " 84%|████████▎ | 416/497 [21:21<04:51,  3.59s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3992.43 ms /   109 tokens\n",
      " 84%|████████▍ | 417/497 [21:25<04:57,  3.72s/it]Llama.generate: 86 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    21 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5076.96 ms /   140 tokens\n",
      " 84%|████████▍ | 418/497 [21:30<05:27,  4.14s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3969.47 ms /   112 tokens\n",
      " 84%|████████▍ | 419/497 [21:34<05:19,  4.10s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2430.55 ms /    70 tokens\n",
      " 85%|████████▍ | 420/497 [21:37<04:37,  3.61s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3758.76 ms /    99 tokens\n",
      " 85%|████████▍ | 421/497 [21:41<04:38,  3.66s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2686.26 ms /    74 tokens\n",
      " 85%|████████▍ | 422/497 [21:43<04:13,  3.38s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2351.69 ms /    63 tokens\n",
      " 85%|████████▌ | 423/497 [21:46<03:48,  3.08s/it]Llama.generate: 86 prefix-match hit, remaining 24 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    24 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7393.87 ms /   190 tokens\n",
      " 85%|████████▌ | 424/497 [21:53<05:20,  4.39s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4273.38 ms /   113 tokens\n",
      " 86%|████████▌ | 425/497 [21:58<05:14,  4.36s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2793.25 ms /    71 tokens\n",
      " 86%|████████▌ | 426/497 [22:00<04:37,  3.90s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2562.69 ms /    69 tokens\n",
      " 86%|████████▌ | 427/497 [22:03<04:05,  3.51s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2345.43 ms /    63 tokens\n",
      " 86%|████████▌ | 428/497 [22:05<03:38,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2474.04 ms /    66 tokens\n",
      " 86%|████████▋ | 429/497 [22:08<03:21,  2.97s/it]Llama.generate: 87 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2449.66 ms /    66 tokens\n",
      " 87%|████████▋ | 430/497 [22:10<03:09,  2.82s/it]Llama.generate: 87 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.83 ms /    85 tokens\n",
      " 87%|████████▋ | 431/497 [22:13<03:12,  2.92s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3287.73 ms /    91 tokens\n",
      " 87%|████████▋ | 432/497 [22:17<03:17,  3.04s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2897.97 ms /    80 tokens\n",
      " 87%|████████▋ | 433/497 [22:20<03:12,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2127.18 ms /    60 tokens\n",
      " 87%|████████▋ | 434/497 [22:22<02:53,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4573.54 ms /   125 tokens\n",
      " 88%|████████▊ | 435/497 [22:26<03:25,  3.31s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2722.40 ms /    77 tokens\n",
      " 88%|████████▊ | 436/497 [22:29<03:11,  3.14s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2429.44 ms /    67 tokens\n",
      " 88%|████████▊ | 437/497 [22:32<02:56,  2.94s/it]Llama.generate: 87 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2765.57 ms /    81 tokens\n",
      " 88%|████████▊ | 438/497 [22:34<02:50,  2.90s/it]Llama.generate: 87 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2240.69 ms /    59 tokens\n",
      " 88%|████████▊ | 439/497 [22:37<02:37,  2.71s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4338.99 ms /   113 tokens\n",
      " 89%|████████▊ | 440/497 [22:41<03:02,  3.21s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3777.35 ms /   101 tokens\n",
      " 89%|████████▊ | 441/497 [22:45<03:09,  3.39s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3537.39 ms /    95 tokens\n",
      " 89%|████████▉ | 442/497 [22:48<03:09,  3.44s/it]Llama.generate: 86 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3130.10 ms /    80 tokens\n",
      " 89%|████████▉ | 443/497 [22:52<03:01,  3.36s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2530.01 ms /    68 tokens\n",
      " 89%|████████▉ | 444/497 [22:54<02:45,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3075.77 ms /    83 tokens\n",
      " 90%|████████▉ | 445/497 [22:57<02:41,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     6 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2203.81 ms /    62 tokens\n",
      " 90%|████████▉ | 446/497 [23:00<02:25,  2.85s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2623.24 ms /    70 tokens\n",
      " 90%|████████▉ | 447/497 [23:02<02:19,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2512.70 ms /    71 tokens\n",
      " 90%|█████████ | 448/497 [23:05<02:13,  2.72s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2986.55 ms /    85 tokens\n",
      " 90%|█████████ | 449/497 [23:08<02:14,  2.81s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2596.80 ms /    76 tokens\n",
      " 91%|█████████ | 450/497 [23:10<02:09,  2.75s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3314.21 ms /    96 tokens\n",
      " 91%|█████████ | 451/497 [23:14<02:14,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2546.01 ms /    73 tokens\n",
      " 91%|█████████ | 452/497 [23:16<02:07,  2.82s/it]Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     8 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2676.54 ms /    74 tokens\n",
      " 91%|█████████ | 453/497 [23:19<02:02,  2.79s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3885.00 ms /   107 tokens\n",
      " 91%|█████████▏| 454/497 [23:23<02:14,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3130.95 ms /    89 tokens\n",
      " 92%|█████████▏| 455/497 [23:26<02:11,  3.14s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2856.37 ms /    81 tokens\n",
      " 92%|█████████▏| 456/497 [23:29<02:05,  3.06s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3400.23 ms /    90 tokens\n",
      " 92%|█████████▏| 457/497 [23:32<02:06,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4839.30 ms /   123 tokens\n",
      " 92%|█████████▏| 458/497 [23:37<02:23,  3.68s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2495.32 ms /    70 tokens\n",
      " 92%|█████████▏| 459/497 [23:40<02:06,  3.33s/it]Llama.generate: 86 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    18 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3571.96 ms /    97 tokens\n",
      " 93%|█████████▎| 460/497 [23:43<02:06,  3.41s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4868.16 ms /   134 tokens\n",
      " 93%|█████████▎| 461/497 [23:48<02:18,  3.86s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.01 ms /    77 tokens\n",
      " 93%|█████████▎| 462/497 [23:51<02:06,  3.61s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4603.70 ms /   117 tokens\n",
      " 93%|█████████▎| 463/497 [23:56<02:13,  3.92s/it]Llama.generate: 86 prefix-match hit, remaining 25 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    25 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4857.06 ms /   122 tokens\n",
      " 93%|█████████▎| 464/497 [24:01<02:18,  4.21s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2587.15 ms /    72 tokens\n",
      " 94%|█████████▎| 465/497 [24:03<01:59,  3.74s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5199.81 ms /   143 tokens\n",
      " 94%|█████████▍| 466/497 [24:09<02:09,  4.19s/it]Llama.generate: 87 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    20 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4768.55 ms /   126 tokens\n",
      " 94%|█████████▍| 467/497 [24:13<02:11,  4.37s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3610.11 ms /    92 tokens\n",
      " 94%|█████████▍| 468/497 [24:17<02:00,  4.15s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2174.09 ms /    64 tokens\n",
      " 94%|█████████▍| 469/497 [24:19<01:39,  3.57s/it]Llama.generate: 88 prefix-match hit, remaining 5 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     5 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2106.75 ms /    62 tokens\n",
      " 95%|█████████▍| 470/497 [24:21<01:24,  3.14s/it]Llama.generate: 88 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3091.34 ms /    78 tokens\n",
      " 95%|█████████▍| 471/497 [24:25<01:21,  3.13s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2281.45 ms /    62 tokens\n",
      " 95%|█████████▍| 472/497 [24:27<01:12,  2.89s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2803.17 ms /    78 tokens\n",
      " 95%|█████████▌| 473/497 [24:30<01:08,  2.87s/it]Llama.generate: 89 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3186.26 ms /    88 tokens\n",
      " 95%|█████████▌| 474/497 [24:33<01:08,  2.98s/it]Llama.generate: 86 prefix-match hit, remaining 11 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.41 ms /    76 tokens\n",
      " 96%|█████████▌| 475/497 [24:36<01:04,  2.93s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2571.72 ms /    71 tokens\n",
      " 96%|█████████▌| 476/497 [24:38<00:59,  2.83s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3936.30 ms /   108 tokens\n",
      " 96%|█████████▌| 477/497 [24:42<01:03,  3.17s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2604.49 ms /    71 tokens\n",
      " 96%|█████████▌| 478/497 [24:45<00:57,  3.01s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3335.95 ms /    91 tokens\n",
      " 96%|█████████▋| 479/497 [24:48<00:56,  3.12s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3209.92 ms /    91 tokens\n",
      " 97%|█████████▋| 480/497 [24:52<00:53,  3.16s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3194.21 ms /    89 tokens\n",
      " 97%|█████████▋| 481/497 [24:55<00:50,  3.18s/it]Llama.generate: 91 prefix-match hit, remaining 7 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     7 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3960.91 ms /   107 tokens\n",
      " 97%|█████████▋| 482/497 [24:59<00:51,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4258.77 ms /   113 tokens\n",
      " 97%|█████████▋| 483/497 [25:03<00:51,  3.68s/it]Llama.generate: 86 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4261.19 ms /   109 tokens\n",
      " 97%|█████████▋| 484/497 [25:07<00:50,  3.87s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2349.43 ms /    64 tokens\n",
      " 98%|█████████▊| 485/497 [25:10<00:41,  3.42s/it]Llama.generate: 86 prefix-match hit, remaining 22 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3461.37 ms /    95 tokens\n",
      " 98%|█████████▊| 486/497 [25:13<00:37,  3.44s/it]Llama.generate: 86 prefix-match hit, remaining 9 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     9 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2299.27 ms /    60 tokens\n",
      " 98%|█████████▊| 487/497 [25:16<00:31,  3.11s/it]Llama.generate: 86 prefix-match hit, remaining 14 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    14 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.89 ms /    82 tokens\n",
      " 98%|█████████▊| 488/497 [25:19<00:27,  3.09s/it]Llama.generate: 86 prefix-match hit, remaining 12 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    12 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2573.45 ms /    70 tokens\n",
      " 98%|█████████▊| 489/497 [25:21<00:23,  2.95s/it]Llama.generate: 86 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4031.81 ms /   107 tokens\n",
      " 99%|█████████▊| 490/497 [25:25<00:22,  3.28s/it]Llama.generate: 86 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3330.83 ms /    94 tokens\n",
      " 99%|█████████▉| 491/497 [25:29<00:19,  3.30s/it]Llama.generate: 86 prefix-match hit, remaining 23 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    23 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4193.94 ms /   115 tokens\n",
      " 99%|█████████▉| 492/497 [25:33<00:17,  3.58s/it]Llama.generate: 87 prefix-match hit, remaining 10 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    10 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2777.54 ms /    75 tokens\n",
      " 99%|█████████▉| 493/497 [25:36<00:13,  3.35s/it]Llama.generate: 86 prefix-match hit, remaining 31 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    31 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5524.52 ms /   148 tokens\n",
      " 99%|█████████▉| 494/497 [25:41<00:12,  4.01s/it]Llama.generate: 86 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3984.21 ms /   104 tokens\n",
      "100%|█████████▉| 495/497 [25:45<00:08,  4.01s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4477.04 ms /   119 tokens\n",
      "100%|█████████▉| 496/497 [25:50<00:04,  4.16s/it]Llama.generate: 86 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     799.57 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    13 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3632.39 ms /    98 tokens\n",
      "100%|██████████| 497/497 [25:53<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011401743796109993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## All results\n",
    "\n",
    "It's important here to note that actually for Fusion and Decomposition we should evaluate the completion rather than the retrieval because here the only difference between those two, is that the fusion version only keep the best documents, whereas the other one keeps all the documents."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:35:00.407529Z",
     "start_time": "2025-01-13T14:34:58.689690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_scores = pd.DataFrame(scores.items(), columns=[\"Method\", \"Score\"])\n",
    "print(df_scores)\n",
    "\n",
    "plt.bar(scores.keys(), scores.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Method     Score\n",
      "0                  MiniLM-NaiveChunking-Gemma  0.503018\n",
      "1                     Jina-LateChunking-Gemma  0.398390\n",
      "2    MiniLM-NaiveChunking-Gemma-Decomposition  0.239113\n",
      "3           MiniLM-NaiveChunking-Gemma-Fusion  0.010731\n",
      "4  MiniLM-NaiveChunking-Mistral-Decomposition  0.424586\n",
      "5         MiniLM-NaiveChunking-Mistral-Fusion  0.011402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
