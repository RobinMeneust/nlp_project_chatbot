{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:42:30.712135Z",
     "start_time": "2025-01-10T14:42:25.669529Z"
    }
   },
   "source": [
    "from nlp_chat_bot.rag.classic_rag import ClassicRAG\n",
    "from nlp_chat_bot.model.late_chunking_embedding import LateChunkingEmbedding\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from nlp_chat_bot.vector_store.late_chunking_chroma_vector_store_builder import LateChunkingChromaVectorStoreBuilder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:42:30.728230Z",
     "start_time": "2025-01-10T14:42:30.712135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:42:54.333424Z",
     "start_time": "2025-01-10T14:42:30.964049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"../data\"\n",
    "model_download_path = \"../models\"\n",
    "vector_store_path = \"../chromadb\"\n",
    "embedding_function = LateChunkingEmbedding(model_download_path)\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "vector_store = LateChunkingChromaVectorStoreBuilder(dataset_path,\n",
    "                                        embedding_function,\n",
    "                                        vector_store_path,\n",
    "                                        splitter=None).build()\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm_gemini)\n",
    "print(\"LENGTH\", rag.get_num_docs())\n",
    "docs_retrieved = rag.retrieve(state = {\"question\": \"What is my conclusion in my project report on image inpainting?\", \"context\": []})\n",
    "\n",
    "print(\"Num docs:\", len(docs_retrieved[\"context\"]))\n",
    "\n",
    "for i in range(len(docs_retrieved[\"context\"])):\n",
    "    doc = docs_retrieved[\"context\"][i]\n",
    "    print(\"\\n\\n\", \"#\"*30,\"\\n\")\n",
    "    print(f\"doc {i}: (score: {doc.metadata['score']})\")\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "0it [00:00, ?it/s]\n",
      "Storing 6 documents embeddings (batch size is 10): 10it [00:12,  1.29s/it]              \n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are now loaded\n",
      "LENGTH 56\n",
      "Num docs: 3\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 0: (score: 39.987790474214655)\n",
      "Image Inpainting with Basic Convolutional Networks\n",
      "Robin Meneust, Ethan Pinto\n",
      "December 2024\n",
      "1 Introduction\n",
      "In the context of our ”AI-Based Image Processing”\n",
      "course, we worked on this project, in which we repro-\n",
      "duced and tested a specific image inpainting approach,\n",
      "defined by the paper ”Context Encoders: Feature Learn-\n",
      "ing by Inpainting”(Pathak et al., 2016)[1].\n",
      "Image inpainting consists of filling hole(s) in an im-\n",
      "age. There exist different methods to do so (e.g. they\n",
      "compared their results with Photoshop). In this paper,\n",
      "they used a context encoder trained in an adversarial\n",
      "way. Basically there is a generator, this is our context\n",
      "encoder (here an encoder and a decoder) that given an\n",
      "image of size 128x128 with a dropout region (a ”hole”,\n",
      "with values set to 0) tries to predict what should be inside\n",
      "the hole. We focused on the simplest case here for the\n",
      "dropout region: a square in the center of size 64x64 (i.e.\n",
      "half of the image). This is a large section of the image,\n",
      "so the task is complex. There is also a discriminator that\n",
      "tries to predict if an image is false (generated) or true\n",
      "(the real region that was dropped). It’s also important\n",
      "to note here that there are no pooling layers, because it’s\n",
      "”detrimental for reconstruction-based training” as they\n",
      "mentioned in their paper. So we only have convolution\n",
      "layers with batch normalization and activation functions.\n",
      "The authors use two loss functions: reconstruction loss\n",
      "and adversarial loss that they combine using a weighted\n",
      "sum (of parameters λrec and λadv): this is the joint\n",
      "loss. The reconstruction loss is basically a MSE (Mean\n",
      "Squared Error) loss. A model using only this loss will\n",
      "output blurry images because it minimizes the mean\n",
      "pixel-wise error. It will be clearly visible in the section 4\n",
      "and in our GitHub 1 in the results images. That’s what\n",
      "the adversarial loss will fix. Indeed this loss will ”en-\n",
      "courage the entire output of the context encoder to look\n",
      "realistic, not just the missing regions”. So it will be\n",
      "sharper than with the reconstruction loss, as we can see\n",
      "in their paper in the figure 1. It’s basically BCE (Bi-\n",
      "nary Cross Entropy) loss. Note here that the encoder\n",
      "only considers the masked region (dropout) and not the\n",
      "whole image. This isn’t really clear in the loss section of\n",
      "the paper, but it is on their model architecture image,\n",
      "here in figure 2, since the discriminator input is 64x64\n",
      "while the full image is 128x128.\n",
      "In this report, we will first explain how we imple-\n",
      "mented the model as it’s defined in the paper in section 2.\n",
      "1https://github.com/Hanabi-TheFox/\n",
      "Image-Inpainting-with-Basic-Convolutional-Networks\n",
      "Figure 1: Comparison of Inpainting Results Using Rec,\n",
      "Adv or Joint Loss (Results From the Paper)\n",
      "Then we will describe our experiments setup (datasets,\n",
      "hyper-parameters...)3. Finally, we will present and in-\n",
      "terpret our results for the tests that we conducted in the\n",
      "section 4. Those tests include dataset variations and dif-\n",
      "ferent λ values. Our code will be available at a later date\n",
      "on GitHub (some time after the report deadline).\n",
      "Figure 2: Model Architecture of Pathak et al. (2016).\n",
      "2 Methodology\n",
      "In this section we will focus on the coding environment\n",
      "and implementation details.\n",
      "2.1 Development Environment\n",
      "This project was developed on Python using PyTorch\n",
      "Lightning. This library simplifies the creation and use\n",
      "of PyTorch models by providing a high level abstraction\n",
      "and separating the model logic from the training loop.\n",
      "We used both PyCharm and Visual Studio Code for the\n",
      "IDE. We first set up a Conda environment to manage de-\n",
      "pendencies and simplify the installation process. The en-\n",
      "vironment setup was automated using a YAML configu-\n",
      "ration file (environment.yaml) at the root of this project.\n",
      "1\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 1: (score: 40.397865157169804)\n",
      "• Batch size: 64 or 512 (the results didn’t change\n",
      "much)\n",
      "• λrec = 0.999 and λadv = 0.001\n",
      "• Adam betas coefficients: 0.5 and 0.9\n",
      "4 Results\n",
      "In this last section, we will finally present our experi-\n",
      "ments results. We will first compare the test PSNR val-\n",
      "ues in section 4.1 and then we will look at the generated\n",
      "images and discuss the results4.2.\n",
      "4.1 Test PSNR\n",
      "Table 1: Variation of The λrec\n",
      "λadv\n",
      "Ratio\n",
      "Data (approach) Size Ratio×1 ×100×200×500×1000\n",
      "Tiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\n",
      "The results in 1 first indicate that our implementation\n",
      "has worse performance compared to the initial paper.\n",
      "However, we need to note that the dataset is different\n",
      "(they didn’t provide PSNR results for ImageNet). Addi-\n",
      "tionally, using their recommended parameters (especially\n",
      "λrec and λadv) is not always the best choice, as our tests\n",
      "on ImageNet-1k have shown. It will be especially vis-\n",
      "ible in 4.2. We can also note that using a 64x64 im-\n",
      "age and rescaling it to 128x128 gives better performance\n",
      "for ImageNet. This can be because PSNR considers the\n",
      "difference in quality between the real and reconstructed\n",
      "images, so if the real one is of poorer quality the differ-\n",
      "ence is lower. It might also be because it doesn’t have to\n",
      "be as precise, there are fewer sharp details to reproduce.\n",
      "It’s very important to note that the results here are for\n",
      "different epochs. We just picked the model with the best\n",
      "validation PSNR value across all epochs and we tested\n",
      "it to get those results.\n",
      "4.2 Generated Images\n",
      "Note that you can see in our GitHub animated im-\n",
      "ages showing the evolution of the model outputs across\n",
      "epochs. There are also more results in the notebooks,\n",
      "since we can’t be exhaustive in this report.\n",
      "Figure 3 shows that with default parameters the gen-\n",
      "erated image is quite good considering half of the image\n",
      "is missing. However, it’s too blurry, so we tried increas-\n",
      "ing the weight of the adversarial loss. The results are in\n",
      "figure 4. Here there is almost no blur and the results are\n",
      "very similar to the ones in the paper, even though it’s\n",
      "not perfect.\n",
      "The two others are on Tiny ImageNet. We tried set-\n",
      "ting the same weight for adversarial and reconstruction\n",
      "Figure 3: Experiment on ImageNet 128x128 with Stan-\n",
      "dard Parameters (×1000 Ratio)\n",
      "Figure 4: Experiment on ImageNet 128x128 with ×200\n",
      "Ratio\n",
      "loss for once in figure 5. The results were to be expected,\n",
      "and are aligned with the original paper results (on adver-\n",
      "sarial loss only). The context doesn’t seem to be taken\n",
      "into account, the results are not blurry but are totally\n",
      "off compared to what we want. Note that compared to\n",
      "ImageNet-1k, we obtained quite good results with the\n",
      "parameters of the paper in figure 6.\n",
      "No matter the experiment, we always observed some\n",
      "errors on especially difficult images. That is for example\n",
      "those with many colors, small items, lots of details...\n",
      "4\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 2: (score: 40.91784833810618)\n",
      "Figure 5: Experiment on Tiny ImageNet with ×1 Ratio\n",
      "Figure 6: Experiment on Tiny ImageNet with Standard\n",
      "Parameters (×1000 Ratio)\n",
      "5 Conclusion\n",
      "Our results are not as good as the initial paper, that can\n",
      "be due to the difference in dataset. So we might want to\n",
      "consider other simpler datasets. We should also consider\n",
      "changing other parameters such as the learning rate. We\n",
      "can then add noise, as they suggest themselves. We only\n",
      "considered the simplest case as asked for this project.\n",
      "But using pre-trained models as the paper did with Alex-\n",
      "Net (when the dropout region is not a square) might also\n",
      "improve our results. In this project we provided a Py-\n",
      "Torch Lightning implementation of a context encoder in\n",
      "a simple Python package to facilitate the understanding\n",
      "of the paper and run experiments on its model architec-\n",
      "ture. We also added tools to make the visualization of\n",
      "the results easier. We typically added an option to save\n",
      "images per epoch and create an animated image out of\n",
      "it.\n",
      "References\n",
      "[1] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell,\n",
      "and A. A. Efros, “Context encoders: Feature\n",
      "learning by inpainting,” 2016. [Online]. Available:\n",
      "https://arxiv.org/abs/1604.07379\n",
      "[2] A. Radford, L. Metz, and S. Chintala, “Unsupervised\n",
      "representation learning with deep convolutional\n",
      "generative adversarial networks,” 2016. [Online].\n",
      "Available: https://arxiv.org/abs/1511.06434\n",
      "5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T14:42:55.361832Z",
     "start_time": "2025-01-10T14:42:54.336424Z"
    }
   },
   "cell_type": "code",
   "source": "rag.invoke(query={\"question\":\"What is my conclusion in my project report on image inpainting?\"})[\"answer\"]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The project's results were not as good as the original paper's, possibly due to dataset differences.  Further improvements could involve using simpler datasets, adjusting parameters like learning rate, adding noise, and utilizing pre-trained models.  The project delivered a PyTorch Lightning implementation of a context encoder for easier understanding and experimentation.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
