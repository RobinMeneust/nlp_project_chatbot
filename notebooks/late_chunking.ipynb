{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:08:43.103671Z",
     "start_time": "2025-01-11T14:08:38.192919Z"
    }
   },
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.rag.classic_rag import ClassicRAG\n",
    "from nlp_chat_bot.model.embedding.late_chunking_embedding import LateChunkingEmbedding\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from nlp_chat_bot.vector_store.late_chunking_chroma_vector_store_builder import LateChunkingChromaVectorStoreBuilder"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:08:43.119747Z",
     "start_time": "2025-01-11T14:08:43.104712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:09:01.064032Z",
     "start_time": "2025-01-11T14:08:43.483700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"../data\"\n",
    "model_download_path = \"../models\"\n",
    "vector_store_path = \"../chromadb\"\n",
    "embedding_function = LateChunkingEmbedding(model_download_path)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=50,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "vector_store = LateChunkingChromaVectorStoreBuilder(dataset_path,\n",
    "                                        embedding_function,\n",
    "                                        vector_store_path,\n",
    "                                        splitter=splitter).build()\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm_gemini)\n",
    "print(\"LENGTH\", rag.get_num_docs())\n",
    "docs_retrieved = rag.retrieve(state = {\"question\": \"What is my conclusion in my project report on image inpainting?\", \"context\": []})\n",
    "\n",
    "print(\"Num docs:\", len(docs_retrieved[\"context\"]))\n",
    "\n",
    "for i in range(len(docs_retrieved[\"context\"])):\n",
    "    doc = docs_retrieved[\"context\"][i]\n",
    "    print(\"\\n\\n\", \"#\"*30,\"\\n\")\n",
    "    print(f\"doc {i}: (score: {doc.metadata['score']})\")\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "0it [00:00, ?it/s]\n",
      "Filtering existing documents: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "Storing 6 documents embeddings (batch size is 10): 10it [00:12,  1.23s/it]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 35 total chunks len, 35 chunks with 35 embeddings\n",
      "Documents are now loaded\n",
      "LENGTH 35\n",
      "Num docs: 3\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 0: (score: 35.03460781059225)\n",
      "Image Inpainting with Basic Convolutional Networks\n",
      "Robin Meneust, Ethan Pinto\n",
      "December 2024\n",
      "1 Introduction\n",
      "In the context of our ”AI-Based Image Processing”\n",
      "course, we worked on this project, in which we repro-\n",
      "duced and tested a specific image inpainting approach,\n",
      "defined by the paper ”Context Encoders: Feature Learn-\n",
      "ing by Inpainting”(Pathak et al., 2016)[1].\n",
      "Image inpainting consists of filling hole(s) in an im-\n",
      "age. There exist different methods to do so (e.g. they\n",
      "compared their results with Photoshop). In this paper,\n",
      "they used a context encoder trained in an adversarial\n",
      "way. Basically there is a generator, this is our context\n",
      "encoder (here an encoder and a decoder) that given an\n",
      "image of size 128x128 with a dropout region (a ”hole”,\n",
      "with values set to 0) tries to predict what should be inside\n",
      "the hole. We focused on the simplest case here for the\n",
      "dropout region: a square in the center of size 64x64 (i.e.\n",
      "half of the image). This is a large section of the image,\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 1: (score: 39.124720999629965)\n",
      "Figure 5: Experiment on Tiny ImageNet with ×1 Ratio\n",
      "Figure 6: Experiment on Tiny ImageNet with Standard\n",
      "Parameters (×1000 Ratio)\n",
      "5 Conclusion\n",
      "Our results are not as good as the initial paper, that can\n",
      "be due to the difference in dataset. So we might want to\n",
      "consider other simpler datasets. We should also consider\n",
      "changing other parameters such as the learning rate. We\n",
      "can then add noise, as they suggest themselves. We only\n",
      "considered the simplest case as asked for this project.\n",
      "But using pre-trained models as the paper did with Alex-\n",
      "Net (when the dropout region is not a square) might also\n",
      "improve our results. In this project we provided a Py-\n",
      "Torch Lightning implementation of a context encoder in\n",
      "a simple Python package to facilitate the understanding\n",
      "of the paper and run experiments on its model architec-\n",
      "ture. We also added tools to make the visualization of\n",
      "the results easier. We typically added an option to save\n",
      "images per epoch and create an animated image out of\n",
      "it.\n",
      "References\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 2: (score: 40.64239421765705)\n",
      "• Batch size: 64 or 512 (the results didn’t change\n",
      "much)\n",
      "• λrec = 0.999 and λadv = 0.001\n",
      "• Adam betas coefficients: 0.5 and 0.9\n",
      "4 Results\n",
      "In this last section, we will finally present our experi-\n",
      "ments results. We will first compare the test PSNR val-\n",
      "ues in section 4.1 and then we will look at the generated\n",
      "images and discuss the results4.2.\n",
      "4.1 Test PSNR\n",
      "Table 1: Variation of The λrec\n",
      "λadv\n",
      "Ratio\n",
      "Data (approach) Size Ratio×1 ×100×200×500×1000\n",
      "Tiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\n",
      "The results in 1 first indicate that our implementation\n",
      "has worse performance compared to the initial paper.\n",
      "However, we need to note that the dataset is different\n",
      "(they didn’t provide PSNR results for ImageNet). Addi-\n",
      "tionally, using their recommended parameters (especially\n",
      "λrec and λadv) is not always the best choice, as our tests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:09:02.300465Z",
     "start_time": "2025-01-11T14:09:01.076060Z"
    }
   },
   "cell_type": "code",
   "source": "rag.invoke(query={\"question\":\"What is my conclusion in my project report on image inpainting?\"})[\"answer\"]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The project's results were not as good as the original paper's, possibly due to dataset differences.  Improvements could involve using simpler datasets, adjusting parameters like the learning rate, adding noise, and using pre-trained models.  A PyTorch Lightning implementation of a context encoder was created to facilitate understanding and experimentation.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
