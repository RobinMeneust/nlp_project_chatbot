{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nlp_chat_bot.rag import RAG\n",
    "from nlp_chat_bot.model.late_chunking_embedding import LateChunkingEmbedding\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T23:45:12.991943Z",
     "start_time": "2025-01-02T23:45:12.975930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T23:55:40.966970Z",
     "start_time": "2025-01-02T23:45:13.214214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"../data\"\n",
    "model_download_path = \"../models\"\n",
    "vector_store_path = \"../chromadb\"\n",
    "embedding_function = LateChunkingEmbedding(model_download_path)\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "rag = RAG(dataset_path, embedding_function, vector_store_path, late_chunking=True, llm=llm_gemini)\n",
    "print(\"LENGTH\", rag.get_num_docs())\n",
    "docs_retrieved = rag.retrieve(state = {\"question\": \"What is my conclusion in my project report on image inpainting?\", \"context\": []})\n",
    "\n",
    "print(\"Num docs:\", len(docs_retrieved[\"context\"]))\n",
    "\n",
    "for i in range(len(docs_retrieved[\"context\"])):\n",
    "    doc = docs_retrieved[\"context\"][i]\n",
    "    print(\"\\n\\n\", \"#\"*30,\"\\n\")\n",
    "    print(f\"doc {i}: (score: {doc.metadata['score']})\")\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 87 0 (offset 0)\n",
      "Ignoring wrong pointing object 98 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n",
      "Ignoring wrong pointing object 136 0 (offset 0)\n",
      "Ignoring wrong pointing object 141 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 171 0 (offset 0)\n",
      "Ignoring wrong pointing object 177 0 (offset 0)\n",
      "Ignoring wrong pointing object 183 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 199 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 214 0 (offset 0)\n",
      "Ignoring wrong pointing object 220 0 (offset 0)\n",
      "Ignoring wrong pointing object 226 0 (offset 0)\n",
      "Ignoring wrong pointing object 231 0 (offset 0)\n",
      "Ignoring wrong pointing object 236 0 (offset 0)\n",
      "Ignoring wrong pointing object 241 0 (offset 0)\n",
      "Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 251 0 (offset 0)\n",
      "Ignoring wrong pointing object 256 0 (offset 0)\n",
      "Ignoring wrong pointing object 261 0 (offset 0)\n",
      "Ignoring wrong pointing object 267 0 (offset 0)\n",
      "Ignoring wrong pointing object 272 0 (offset 0)\n",
      "Ignoring wrong pointing object 277 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 287 0 (offset 0)\n",
      "Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 297 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 310 0 (offset 0)\n",
      "Ignoring wrong pointing object 316 0 (offset 0)\n",
      "Ignoring wrong pointing object 321 0 (offset 0)\n",
      "Ignoring wrong pointing object 326 0 (offset 0)\n",
      "Ignoring wrong pointing object 336 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 348 0 (offset 0)\n",
      "Ignoring wrong pointing object 353 0 (offset 0)\n",
      "Ignoring wrong pointing object 359 0 (offset 0)\n",
      "Ignoring wrong pointing object 364 0 (offset 0)\n",
      "Ignoring wrong pointing object 369 0 (offset 0)\n",
      "Ignoring wrong pointing object 374 0 (offset 0)\n",
      "Ignoring wrong pointing object 381 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 398 0 (offset 0)\n",
      "Ignoring wrong pointing object 406 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n",
      "Ignoring wrong pointing object 416 0 (offset 0)\n",
      "Ignoring wrong pointing object 421 0 (offset 0)\n",
      "Ignoring wrong pointing object 428 0 (offset 0)\n",
      "Ignoring wrong pointing object 435 0 (offset 0)\n",
      "Ignoring wrong pointing object 442 0 (offset 0)\n",
      "Ignoring wrong pointing object 447 0 (offset 0)\n",
      "Ignoring wrong pointing object 455 0 (offset 0)\n",
      "Ignoring wrong pointing object 463 0 (offset 0)\n",
      "100%|██████████| 7/7 [00:02<00:00,  3.13it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and storing 311 new documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [10:20<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH 361\n",
      "Num docs: 3\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 0: (score: 39.98778533935547)\n",
      "Image Inpainting with Basic Convolutional Networks\n",
      "Robin Meneust, Ethan Pinto\n",
      "December 2024\n",
      "1 Introduction\n",
      "In the context of our ”AI-Based Image Processing”\n",
      "course, we worked on this project, in which we repro-\n",
      "duced and tested a specific image inpainting approach,\n",
      "defined by the paper ”Context Encoders: Feature Learn-\n",
      "ing by Inpainting”(Pathak et al., 2016)[1].\n",
      "Image inpainting consists of filling hole(s) in an im-\n",
      "age. There exist different methods to do so (e.g. they\n",
      "compared their results with Photoshop). In this paper,\n",
      "they used a context encoder trained in an adversarial\n",
      "way. Basically there is a generator, this is our context\n",
      "encoder (here an encoder and a decoder) that given an\n",
      "image of size 128x128 with a dropout region (a ”hole”,\n",
      "with values set to 0) tries to predict what should be inside\n",
      "the hole. We focused on the simplest case here for the\n",
      "dropout region: a square in the center of size 64x64 (i.e.\n",
      "half of the image). This is a large section of the image,\n",
      "so the task is complex. There is also a discriminator that\n",
      "tries to predict if an image is false (generated) or true\n",
      "(the real region that was dropped). It’s also important\n",
      "to note here that there are no pooling layers, because it’s\n",
      "”detrimental for reconstruction-based training” as they\n",
      "mentioned in their paper. So we only have convolution\n",
      "layers with batch normalization and activation functions.\n",
      "The authors use two loss functions: reconstruction loss\n",
      "and adversarial loss that they combine using a weighted\n",
      "sum (of parameters λrec and λadv): this is the joint\n",
      "loss. The reconstruction loss is basically a MSE (Mean\n",
      "Squared Error) loss. A model using only this loss will\n",
      "output blurry images because it minimizes the mean\n",
      "pixel-wise error. It will be clearly visible in the section 4\n",
      "and in our GitHub 1 in the results images. That’s what\n",
      "the adversarial loss will fix. Indeed this loss will ”en-\n",
      "courage the entire output of the context encoder to look\n",
      "realistic, not just the missing regions”. So it will be\n",
      "sharper than with the reconstruction loss, as we can see\n",
      "in their paper in the figure 1. It’s basically BCE (Bi-\n",
      "nary Cross Entropy) loss. Note here that the encoder\n",
      "only considers the masked region (dropout) and not the\n",
      "whole image. This isn’t really clear in the loss section of\n",
      "the paper, but it is on their model architecture image,\n",
      "here in figure 2, since the discriminator input is 64x64\n",
      "while the full image is 128x128.\n",
      "In this report, we will first explain how we imple-\n",
      "mented the model as it’s defined in the paper in section 2.\n",
      "1https://github.com/Hanabi-TheFox/\n",
      "Image-Inpainting-with-Basic-Convolutional-Networks\n",
      "Figure 1: Comparison of Inpainting Results Using Rec,\n",
      "Adv or Joint Loss (Results From the Paper)\n",
      "Then we will describe our experiments setup (datasets,\n",
      "hyper-parameters...)3. Finally, we will present and in-\n",
      "terpret our results for the tests that we conducted in the\n",
      "section 4. Those tests include dataset variations and dif-\n",
      "ferent λ values. Our code will be available at a later date\n",
      "on GitHub (some time after the report deadline).\n",
      "Figure 2: Model Architecture of Pathak et al. (2016).\n",
      "2 Methodology\n",
      "In this section we will focus on the coding environment\n",
      "and implementation details.\n",
      "2.1 Development Environment\n",
      "This project was developed on Python using PyTorch\n",
      "Lightning. This library simplifies the creation and use\n",
      "of PyTorch models by providing a high level abstraction\n",
      "and separating the model logic from the training loop.\n",
      "We used both PyCharm and Visual Studio Code for the\n",
      "IDE. We first set up a Conda environment to manage de-\n",
      "pendencies and simplify the installation process. The en-\n",
      "vironment setup was automated using a YAML configu-\n",
      "ration file (environment.yaml) at the root of this project.\n",
      "1\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 1: (score: 40.39786911010742)\n",
      "• Batch size: 64 or 512 (the results didn’t change\n",
      "much)\n",
      "• λrec = 0.999 and λadv = 0.001\n",
      "• Adam betas coefficients: 0.5 and 0.9\n",
      "4 Results\n",
      "In this last section, we will finally present our experi-\n",
      "ments results. We will first compare the test PSNR val-\n",
      "ues in section 4.1 and then we will look at the generated\n",
      "images and discuss the results4.2.\n",
      "4.1 Test PSNR\n",
      "Table 1: Variation of The λrec\n",
      "λadv\n",
      "Ratio\n",
      "Data (approach) Size Ratio×1 ×100×200×500×1000\n",
      "Tiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\n",
      "The results in 1 first indicate that our implementation\n",
      "has worse performance compared to the initial paper.\n",
      "However, we need to note that the dataset is different\n",
      "(they didn’t provide PSNR results for ImageNet). Addi-\n",
      "tionally, using their recommended parameters (especially\n",
      "λrec and λadv) is not always the best choice, as our tests\n",
      "on ImageNet-1k have shown. It will be especially vis-\n",
      "ible in 4.2. We can also note that using a 64x64 im-\n",
      "age and rescaling it to 128x128 gives better performance\n",
      "for ImageNet. This can be because PSNR considers the\n",
      "difference in quality between the real and reconstructed\n",
      "images, so if the real one is of poorer quality the differ-\n",
      "ence is lower. It might also be because it doesn’t have to\n",
      "be as precise, there are fewer sharp details to reproduce.\n",
      "It’s very important to note that the results here are for\n",
      "different epochs. We just picked the model with the best\n",
      "validation PSNR value across all epochs and we tested\n",
      "it to get those results.\n",
      "4.2 Generated Images\n",
      "Note that you can see in our GitHub animated im-\n",
      "ages showing the evolution of the model outputs across\n",
      "epochs. There are also more results in the notebooks,\n",
      "since we can’t be exhaustive in this report.\n",
      "Figure 3 shows that with default parameters the gen-\n",
      "erated image is quite good considering half of the image\n",
      "is missing. However, it’s too blurry, so we tried increas-\n",
      "ing the weight of the adversarial loss. The results are in\n",
      "figure 4. Here there is almost no blur and the results are\n",
      "very similar to the ones in the paper, even though it’s\n",
      "not perfect.\n",
      "The two others are on Tiny ImageNet. We tried set-\n",
      "ting the same weight for adversarial and reconstruction\n",
      "Figure 3: Experiment on ImageNet 128x128 with Stan-\n",
      "dard Parameters (×1000 Ratio)\n",
      "Figure 4: Experiment on ImageNet 128x128 with ×200\n",
      "Ratio\n",
      "loss for once in figure 5. The results were to be expected,\n",
      "and are aligned with the original paper results (on adver-\n",
      "sarial loss only). The context doesn’t seem to be taken\n",
      "into account, the results are not blurry but are totally\n",
      "off compared to what we want. Note that compared to\n",
      "ImageNet-1k, we obtained quite good results with the\n",
      "parameters of the paper in figure 6.\n",
      "No matter the experiment, we always observed some\n",
      "errors on especially difficult images. That is for example\n",
      "those with many colors, small items, lots of details...\n",
      "4\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 2: (score: 40.43163299560547)\n",
      "02\n",
      "Panorama Eco-système\n",
      "Hadoop\n",
      "04/11/2024 45 / 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T23:55:42.545190Z",
     "start_time": "2025-01-02T23:55:41.029477Z"
    }
   },
   "cell_type": "code",
   "source": "rag.invoke(query={\"question\":\"What is my conclusion in my project report on image inpainting?\"})",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my conclusion in my project report on image inpainting?',\n",
       " 'context': (Document(metadata={'score': 39.98778533935547}, page_content='Image Inpainting with Basic Convolutional Networks\\nRobin Meneust, Ethan Pinto\\nDecember 2024\\n1 Introduction\\nIn the context of our ”AI-Based Image Processing”\\ncourse, we worked on this project, in which we repro-\\nduced and tested a specific image inpainting approach,\\ndefined by the paper ”Context Encoders: Feature Learn-\\ning by Inpainting”(Pathak et al., 2016)[1].\\nImage inpainting consists of filling hole(s) in an im-\\nage. There exist different methods to do so (e.g. they\\ncompared their results with Photoshop). In this paper,\\nthey used a context encoder trained in an adversarial\\nway. Basically there is a generator, this is our context\\nencoder (here an encoder and a decoder) that given an\\nimage of size 128x128 with a dropout region (a ”hole”,\\nwith values set to 0) tries to predict what should be inside\\nthe hole. We focused on the simplest case here for the\\ndropout region: a square in the center of size 64x64 (i.e.\\nhalf of the image). This is a large section of the image,\\nso the task is complex. There is also a discriminator that\\ntries to predict if an image is false (generated) or true\\n(the real region that was dropped). It’s also important\\nto note here that there are no pooling layers, because it’s\\n”detrimental for reconstruction-based training” as they\\nmentioned in their paper. So we only have convolution\\nlayers with batch normalization and activation functions.\\nThe authors use two loss functions: reconstruction loss\\nand adversarial loss that they combine using a weighted\\nsum (of parameters λrec and λadv): this is the joint\\nloss. The reconstruction loss is basically a MSE (Mean\\nSquared Error) loss. A model using only this loss will\\noutput blurry images because it minimizes the mean\\npixel-wise error. It will be clearly visible in the section 4\\nand in our GitHub 1 in the results images. That’s what\\nthe adversarial loss will fix. Indeed this loss will ”en-\\ncourage the entire output of the context encoder to look\\nrealistic, not just the missing regions”. So it will be\\nsharper than with the reconstruction loss, as we can see\\nin their paper in the figure 1. It’s basically BCE (Bi-\\nnary Cross Entropy) loss. Note here that the encoder\\nonly considers the masked region (dropout) and not the\\nwhole image. This isn’t really clear in the loss section of\\nthe paper, but it is on their model architecture image,\\nhere in figure 2, since the discriminator input is 64x64\\nwhile the full image is 128x128.\\nIn this report, we will first explain how we imple-\\nmented the model as it’s defined in the paper in section 2.\\n1https://github.com/Hanabi-TheFox/\\nImage-Inpainting-with-Basic-Convolutional-Networks\\nFigure 1: Comparison of Inpainting Results Using Rec,\\nAdv or Joint Loss (Results From the Paper)\\nThen we will describe our experiments setup (datasets,\\nhyper-parameters...)3. Finally, we will present and in-\\nterpret our results for the tests that we conducted in the\\nsection 4. Those tests include dataset variations and dif-\\nferent λ values. Our code will be available at a later date\\non GitHub (some time after the report deadline).\\nFigure 2: Model Architecture of Pathak et al. (2016).\\n2 Methodology\\nIn this section we will focus on the coding environment\\nand implementation details.\\n2.1 Development Environment\\nThis project was developed on Python using PyTorch\\nLightning. This library simplifies the creation and use\\nof PyTorch models by providing a high level abstraction\\nand separating the model logic from the training loop.\\nWe used both PyCharm and Visual Studio Code for the\\nIDE. We first set up a Conda environment to manage de-\\npendencies and simplify the installation process. The en-\\nvironment setup was automated using a YAML configu-\\nration file (environment.yaml) at the root of this project.\\n1'),\n",
       "  Document(metadata={'score': 40.39786911010742}, page_content='• Batch size: 64 or 512 (the results didn’t change\\nmuch)\\n• λrec = 0.999 and λadv = 0.001\\n• Adam betas coefficients: 0.5 and 0.9\\n4 Results\\nIn this last section, we will finally present our experi-\\nments results. We will first compare the test PSNR val-\\nues in section 4.1 and then we will look at the generated\\nimages and discuss the results4.2.\\n4.1 Test PSNR\\nTable 1: Variation of The λrec\\nλadv\\nRatio\\nData (approach) Size Ratio×1 ×100×200×500×1000\\nTiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\\nThe results in 1 first indicate that our implementation\\nhas worse performance compared to the initial paper.\\nHowever, we need to note that the dataset is different\\n(they didn’t provide PSNR results for ImageNet). Addi-\\ntionally, using their recommended parameters (especially\\nλrec and λadv) is not always the best choice, as our tests\\non ImageNet-1k have shown. It will be especially vis-\\nible in 4.2. We can also note that using a 64x64 im-\\nage and rescaling it to 128x128 gives better performance\\nfor ImageNet. This can be because PSNR considers the\\ndifference in quality between the real and reconstructed\\nimages, so if the real one is of poorer quality the differ-\\nence is lower. It might also be because it doesn’t have to\\nbe as precise, there are fewer sharp details to reproduce.\\nIt’s very important to note that the results here are for\\ndifferent epochs. We just picked the model with the best\\nvalidation PSNR value across all epochs and we tested\\nit to get those results.\\n4.2 Generated Images\\nNote that you can see in our GitHub animated im-\\nages showing the evolution of the model outputs across\\nepochs. There are also more results in the notebooks,\\nsince we can’t be exhaustive in this report.\\nFigure 3 shows that with default parameters the gen-\\nerated image is quite good considering half of the image\\nis missing. However, it’s too blurry, so we tried increas-\\ning the weight of the adversarial loss. The results are in\\nfigure 4. Here there is almost no blur and the results are\\nvery similar to the ones in the paper, even though it’s\\nnot perfect.\\nThe two others are on Tiny ImageNet. We tried set-\\nting the same weight for adversarial and reconstruction\\nFigure 3: Experiment on ImageNet 128x128 with Stan-\\ndard Parameters (×1000 Ratio)\\nFigure 4: Experiment on ImageNet 128x128 with ×200\\nRatio\\nloss for once in figure 5. The results were to be expected,\\nand are aligned with the original paper results (on adver-\\nsarial loss only). The context doesn’t seem to be taken\\ninto account, the results are not blurry but are totally\\noff compared to what we want. Note that compared to\\nImageNet-1k, we obtained quite good results with the\\nparameters of the paper in figure 6.\\nNo matter the experiment, we always observed some\\nerrors on especially difficult images. That is for example\\nthose with many colors, small items, lots of details...\\n4'),\n",
       "  Document(metadata={'score': 40.43163299560547}, page_content='02\\nPanorama Eco-système\\nHadoop\\n04/11/2024 45 / 79')),\n",
       " 'answer': \"The project reproduced and tested an image inpainting approach using a context encoder.  The results showed that while the implementation achieved reasonable inpainting, its performance was worse than the original paper's, likely due to dataset differences and hyperparameter choices.  Adjusting the adversarial loss weight improved results, yielding outputs similar to the original paper, though some errors persisted on complex images.\\n\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
