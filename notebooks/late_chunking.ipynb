{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:54:32.838973Z",
     "start_time": "2025-01-02T15:54:32.823343Z"
    }
   },
   "source": [
    "from nlp_chat_bot.rag import RAG\n",
    "from nlp_chat_bot.model.late_chunking_embedding import LateChunkingEmbedding\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:54:32.870224Z",
     "start_time": "2025-01-02T15:54:32.854598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:54:57.490320Z",
     "start_time": "2025-01-02T15:54:32.881561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"../data\"\n",
    "model_download_path = \"../models\"\n",
    "vector_store_path = \"../chromadb\"\n",
    "embedding_function = LateChunkingEmbedding(model_download_path)\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "rag = RAG(dataset_path, embedding_function, vector_store_path, late_chunking=True, llm=llm_gemini)\n",
    "print(\"LENGTH\", rag.get_num_docs())\n",
    "docs_retrieved = rag.retrieve(state = {\"question\": \"What is my conclusion in my project report on image inpainting?\", \"context\": []})\n",
    "\n",
    "print(\"Num docs:\", len(docs_retrieved[\"context\"]))\n",
    "\n",
    "for i in range(len(docs_retrieved[\"context\"])):\n",
    "    doc = docs_retrieved[\"context\"][i]\n",
    "    print(\"\\n\\n\", \"#\"*30,\"\\n\")\n",
    "    print(f\"doc {i}: (score: {doc.metadata['score']})\")\n",
    "    print(doc.page_content)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and storing 6 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:21<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENGTH 144\n",
      "Num docs: 3\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 0: (score: 39.987790474214655)\n",
      "Image Inpainting with Basic Convolutional Networks\n",
      "Robin Meneust, Ethan Pinto\n",
      "December 2024\n",
      "1 Introduction\n",
      "In the context of our ”AI-Based Image Processing”\n",
      "course, we worked on this project, in which we repro-\n",
      "duced and tested a specific image inpainting approach,\n",
      "defined by the paper ”Context Encoders: Feature Learn-\n",
      "ing by Inpainting”(Pathak et al., 2016)[1].\n",
      "Image inpainting consists of filling hole(s) in an im-\n",
      "age. There exist different methods to do so (e.g. they\n",
      "compared their results with Photoshop). In this paper,\n",
      "they used a context encoder trained in an adversarial\n",
      "way. Basically there is a generator, this is our context\n",
      "encoder (here an encoder and a decoder) that given an\n",
      "image of size 128x128 with a dropout region (a ”hole”,\n",
      "with values set to 0) tries to predict what should be inside\n",
      "the hole. We focused on the simplest case here for the\n",
      "dropout region: a square in the center of size 64x64 (i.e.\n",
      "half of the image). This is a large section of the image,\n",
      "so the task is complex. There is also a discriminator that\n",
      "tries to predict if an image is false (generated) or true\n",
      "(the real region that was dropped). It’s also important\n",
      "to note here that there are no pooling layers, because it’s\n",
      "”detrimental for reconstruction-based training” as they\n",
      "mentioned in their paper. So we only have convolution\n",
      "layers with batch normalization and activation functions.\n",
      "The authors use two loss functions: reconstruction loss\n",
      "and adversarial loss that they combine using a weighted\n",
      "sum (of parameters λrec and λadv): this is the joint\n",
      "loss. The reconstruction loss is basically a MSE (Mean\n",
      "Squared Error) loss. A model using only this loss will\n",
      "output blurry images because it minimizes the mean\n",
      "pixel-wise error. It will be clearly visible in the section 4\n",
      "and in our GitHub 1 in the results images. That’s what\n",
      "the adversarial loss will fix. Indeed this loss will ”en-\n",
      "courage the entire output of the context encoder to look\n",
      "realistic, not just the missing regions”. So it will be\n",
      "sharper than with the reconstruction loss, as we can see\n",
      "in their paper in the figure 1. It’s basically BCE (Bi-\n",
      "nary Cross Entropy) loss. Note here that the encoder\n",
      "only considers the masked region (dropout) and not the\n",
      "whole image. This isn’t really clear in the loss section of\n",
      "the paper, but it is on their model architecture image,\n",
      "here in figure 2, since the discriminator input is 64x64\n",
      "while the full image is 128x128.\n",
      "In this report, we will first explain how we imple-\n",
      "mented the model as it’s defined in the paper in section 2.\n",
      "1https://github.com/Hanabi-TheFox/\n",
      "Image-Inpainting-with-Basic-Convolutional-Networks\n",
      "Figure 1: Comparison of Inpainting Results Using Rec,\n",
      "Adv or Joint Loss (Results From the Paper)\n",
      "Then we will describe our experiments setup (datasets,\n",
      "hyper-parameters...)3. Finally, we will present and in-\n",
      "terpret our results for the tests that we conducted in the\n",
      "section 4. Those tests include dataset variations and dif-\n",
      "ferent λ values. Our code will be available at a later date\n",
      "on GitHub (some time after the report deadline).\n",
      "Figure 2: Model Architecture of Pathak et al. (2016).\n",
      "2 Methodology\n",
      "In this section we will focus on the coding environment\n",
      "and implementation details.\n",
      "2.1 Development Environment\n",
      "This project was developed on Python using PyTorch\n",
      "Lightning. This library simplifies the creation and use\n",
      "of PyTorch models by providing a high level abstraction\n",
      "and separating the model logic from the training loop.\n",
      "We used both PyCharm and Visual Studio Code for the\n",
      "IDE. We first set up a Conda environment to manage de-\n",
      "pendencies and simplify the installation process. The en-\n",
      "vironment setup was automated using a YAML configu-\n",
      "ration file (environment.yaml) at the root of this project.\n",
      "1\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 1: (score: 40.397865157169804)\n",
      "• Batch size: 64 or 512 (the results didn’t change\n",
      "much)\n",
      "• λrec = 0.999 and λadv = 0.001\n",
      "• Adam betas coefficients: 0.5 and 0.9\n",
      "4 Results\n",
      "In this last section, we will finally present our experi-\n",
      "ments results. We will first compare the test PSNR val-\n",
      "ues in section 4.1 and then we will look at the generated\n",
      "images and discuss the results4.2.\n",
      "4.1 Test PSNR\n",
      "Table 1: Variation of The λrec\n",
      "λadv\n",
      "Ratio\n",
      "Data (approach) Size Ratio×1 ×100×200×500×1000\n",
      "Tiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\n",
      "The results in 1 first indicate that our implementation\n",
      "has worse performance compared to the initial paper.\n",
      "However, we need to note that the dataset is different\n",
      "(they didn’t provide PSNR results for ImageNet). Addi-\n",
      "tionally, using their recommended parameters (especially\n",
      "λrec and λadv) is not always the best choice, as our tests\n",
      "on ImageNet-1k have shown. It will be especially vis-\n",
      "ible in 4.2. We can also note that using a 64x64 im-\n",
      "age and rescaling it to 128x128 gives better performance\n",
      "for ImageNet. This can be because PSNR considers the\n",
      "difference in quality between the real and reconstructed\n",
      "images, so if the real one is of poorer quality the differ-\n",
      "ence is lower. It might also be because it doesn’t have to\n",
      "be as precise, there are fewer sharp details to reproduce.\n",
      "It’s very important to note that the results here are for\n",
      "different epochs. We just picked the model with the best\n",
      "validation PSNR value across all epochs and we tested\n",
      "it to get those results.\n",
      "4.2 Generated Images\n",
      "Note that you can see in our GitHub animated im-\n",
      "ages showing the evolution of the model outputs across\n",
      "epochs. There are also more results in the notebooks,\n",
      "since we can’t be exhaustive in this report.\n",
      "Figure 3 shows that with default parameters the gen-\n",
      "erated image is quite good considering half of the image\n",
      "is missing. However, it’s too blurry, so we tried increas-\n",
      "ing the weight of the adversarial loss. The results are in\n",
      "figure 4. Here there is almost no blur and the results are\n",
      "very similar to the ones in the paper, even though it’s\n",
      "not perfect.\n",
      "The two others are on Tiny ImageNet. We tried set-\n",
      "ting the same weight for adversarial and reconstruction\n",
      "Figure 3: Experiment on ImageNet 128x128 with Stan-\n",
      "dard Parameters (×1000 Ratio)\n",
      "Figure 4: Experiment on ImageNet 128x128 with ×200\n",
      "Ratio\n",
      "loss for once in figure 5. The results were to be expected,\n",
      "and are aligned with the original paper results (on adver-\n",
      "sarial loss only). The context doesn’t seem to be taken\n",
      "into account, the results are not blurry but are totally\n",
      "off compared to what we want. Note that compared to\n",
      "ImageNet-1k, we obtained quite good results with the\n",
      "parameters of the paper in figure 6.\n",
      "No matter the experiment, we always observed some\n",
      "errors on especially difficult images. That is for example\n",
      "those with many colors, small items, lots of details...\n",
      "4\n",
      "\n",
      "\n",
      " ############################## \n",
      "\n",
      "doc 2: (score: 40.91784833810618)\n",
      "Figure 5: Experiment on Tiny ImageNet with ×1 Ratio\n",
      "Figure 6: Experiment on Tiny ImageNet with Standard\n",
      "Parameters (×1000 Ratio)\n",
      "5 Conclusion\n",
      "Our results are not as good as the initial paper, that can\n",
      "be due to the difference in dataset. So we might want to\n",
      "consider other simpler datasets. We should also consider\n",
      "changing other parameters such as the learning rate. We\n",
      "can then add noise, as they suggest themselves. We only\n",
      "considered the simplest case as asked for this project.\n",
      "But using pre-trained models as the paper did with Alex-\n",
      "Net (when the dropout region is not a square) might also\n",
      "improve our results. In this project we provided a Py-\n",
      "Torch Lightning implementation of a context encoder in\n",
      "a simple Python package to facilitate the understanding\n",
      "of the paper and run experiments on its model architec-\n",
      "ture. We also added tools to make the visualization of\n",
      "the results easier. We typically added an option to save\n",
      "images per epoch and create an animated image out of\n",
      "it.\n",
      "References\n",
      "[1] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell,\n",
      "and A. A. Efros, “Context encoders: Feature\n",
      "learning by inpainting,” 2016. [Online]. Available:\n",
      "https://arxiv.org/abs/1604.07379\n",
      "[2] A. Radford, L. Metz, and S. Chintala, “Unsupervised\n",
      "representation learning with deep convolutional\n",
      "generative adversarial networks,” 2016. [Online].\n",
      "Available: https://arxiv.org/abs/1511.06434\n",
      "5\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:54:59.000344Z",
     "start_time": "2025-01-02T15:54:57.584463Z"
    }
   },
   "cell_type": "code",
   "source": "rag.invoke(query={\"question\":\"What is my conclusion in my project report on image inpainting?\"})",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is my conclusion in my project report on image inpainting?',\n",
       " 'context': (Document(metadata={'score': 39.987790474214655}, page_content='Image Inpainting with Basic Convolutional Networks\\nRobin Meneust, Ethan Pinto\\nDecember 2024\\n1 Introduction\\nIn the context of our ”AI-Based Image Processing”\\ncourse, we worked on this project, in which we repro-\\nduced and tested a specific image inpainting approach,\\ndefined by the paper ”Context Encoders: Feature Learn-\\ning by Inpainting”(Pathak et al., 2016)[1].\\nImage inpainting consists of filling hole(s) in an im-\\nage. There exist different methods to do so (e.g. they\\ncompared their results with Photoshop). In this paper,\\nthey used a context encoder trained in an adversarial\\nway. Basically there is a generator, this is our context\\nencoder (here an encoder and a decoder) that given an\\nimage of size 128x128 with a dropout region (a ”hole”,\\nwith values set to 0) tries to predict what should be inside\\nthe hole. We focused on the simplest case here for the\\ndropout region: a square in the center of size 64x64 (i.e.\\nhalf of the image). This is a large section of the image,\\nso the task is complex. There is also a discriminator that\\ntries to predict if an image is false (generated) or true\\n(the real region that was dropped). It’s also important\\nto note here that there are no pooling layers, because it’s\\n”detrimental for reconstruction-based training” as they\\nmentioned in their paper. So we only have convolution\\nlayers with batch normalization and activation functions.\\nThe authors use two loss functions: reconstruction loss\\nand adversarial loss that they combine using a weighted\\nsum (of parameters λrec and λadv): this is the joint\\nloss. The reconstruction loss is basically a MSE (Mean\\nSquared Error) loss. A model using only this loss will\\noutput blurry images because it minimizes the mean\\npixel-wise error. It will be clearly visible in the section 4\\nand in our GitHub 1 in the results images. That’s what\\nthe adversarial loss will fix. Indeed this loss will ”en-\\ncourage the entire output of the context encoder to look\\nrealistic, not just the missing regions”. So it will be\\nsharper than with the reconstruction loss, as we can see\\nin their paper in the figure 1. It’s basically BCE (Bi-\\nnary Cross Entropy) loss. Note here that the encoder\\nonly considers the masked region (dropout) and not the\\nwhole image. This isn’t really clear in the loss section of\\nthe paper, but it is on their model architecture image,\\nhere in figure 2, since the discriminator input is 64x64\\nwhile the full image is 128x128.\\nIn this report, we will first explain how we imple-\\nmented the model as it’s defined in the paper in section 2.\\n1https://github.com/Hanabi-TheFox/\\nImage-Inpainting-with-Basic-Convolutional-Networks\\nFigure 1: Comparison of Inpainting Results Using Rec,\\nAdv or Joint Loss (Results From the Paper)\\nThen we will describe our experiments setup (datasets,\\nhyper-parameters...)3. Finally, we will present and in-\\nterpret our results for the tests that we conducted in the\\nsection 4. Those tests include dataset variations and dif-\\nferent λ values. Our code will be available at a later date\\non GitHub (some time after the report deadline).\\nFigure 2: Model Architecture of Pathak et al. (2016).\\n2 Methodology\\nIn this section we will focus on the coding environment\\nand implementation details.\\n2.1 Development Environment\\nThis project was developed on Python using PyTorch\\nLightning. This library simplifies the creation and use\\nof PyTorch models by providing a high level abstraction\\nand separating the model logic from the training loop.\\nWe used both PyCharm and Visual Studio Code for the\\nIDE. We first set up a Conda environment to manage de-\\npendencies and simplify the installation process. The en-\\nvironment setup was automated using a YAML configu-\\nration file (environment.yaml) at the root of this project.\\n1'),\n",
       "  Document(metadata={'score': 40.397865157169804}, page_content='• Batch size: 64 or 512 (the results didn’t change\\nmuch)\\n• λrec = 0.999 and λadv = 0.001\\n• Adam betas coefficients: 0.5 and 0.9\\n4 Results\\nIn this last section, we will finally present our experi-\\nments results. We will first compare the test PSNR val-\\nues in section 4.1 and then we will look at the generated\\nimages and discuss the results4.2.\\n4.1 Test PSNR\\nTable 1: Variation of The λrec\\nλadv\\nRatio\\nData (approach) Size Ratio×1 ×100×200×500×1000\\nTiny ImageNet (Ours) 64×64 11.56 14.34 14.76ImageNet-1k 64 (Ours) 64×64 17.20ImageNet-1k 128 (Ours) 128×128 13.17 13.4015.3914.70Paris StreetView (Original paper) 128×128 18.58\\nThe results in 1 first indicate that our implementation\\nhas worse performance compared to the initial paper.\\nHowever, we need to note that the dataset is different\\n(they didn’t provide PSNR results for ImageNet). Addi-\\ntionally, using their recommended parameters (especially\\nλrec and λadv) is not always the best choice, as our tests\\non ImageNet-1k have shown. It will be especially vis-\\nible in 4.2. We can also note that using a 64x64 im-\\nage and rescaling it to 128x128 gives better performance\\nfor ImageNet. This can be because PSNR considers the\\ndifference in quality between the real and reconstructed\\nimages, so if the real one is of poorer quality the differ-\\nence is lower. It might also be because it doesn’t have to\\nbe as precise, there are fewer sharp details to reproduce.\\nIt’s very important to note that the results here are for\\ndifferent epochs. We just picked the model with the best\\nvalidation PSNR value across all epochs and we tested\\nit to get those results.\\n4.2 Generated Images\\nNote that you can see in our GitHub animated im-\\nages showing the evolution of the model outputs across\\nepochs. There are also more results in the notebooks,\\nsince we can’t be exhaustive in this report.\\nFigure 3 shows that with default parameters the gen-\\nerated image is quite good considering half of the image\\nis missing. However, it’s too blurry, so we tried increas-\\ning the weight of the adversarial loss. The results are in\\nfigure 4. Here there is almost no blur and the results are\\nvery similar to the ones in the paper, even though it’s\\nnot perfect.\\nThe two others are on Tiny ImageNet. We tried set-\\nting the same weight for adversarial and reconstruction\\nFigure 3: Experiment on ImageNet 128x128 with Stan-\\ndard Parameters (×1000 Ratio)\\nFigure 4: Experiment on ImageNet 128x128 with ×200\\nRatio\\nloss for once in figure 5. The results were to be expected,\\nand are aligned with the original paper results (on adver-\\nsarial loss only). The context doesn’t seem to be taken\\ninto account, the results are not blurry but are totally\\noff compared to what we want. Note that compared to\\nImageNet-1k, we obtained quite good results with the\\nparameters of the paper in figure 6.\\nNo matter the experiment, we always observed some\\nerrors on especially difficult images. That is for example\\nthose with many colors, small items, lots of details...\\n4'),\n",
       "  Document(metadata={'score': 40.91784833810618}, page_content='Figure 5: Experiment on Tiny ImageNet with ×1 Ratio\\nFigure 6: Experiment on Tiny ImageNet with Standard\\nParameters (×1000 Ratio)\\n5 Conclusion\\nOur results are not as good as the initial paper, that can\\nbe due to the difference in dataset. So we might want to\\nconsider other simpler datasets. We should also consider\\nchanging other parameters such as the learning rate. We\\ncan then add noise, as they suggest themselves. We only\\nconsidered the simplest case as asked for this project.\\nBut using pre-trained models as the paper did with Alex-\\nNet (when the dropout region is not a square) might also\\nimprove our results. In this project we provided a Py-\\nTorch Lightning implementation of a context encoder in\\na simple Python package to facilitate the understanding\\nof the paper and run experiments on its model architec-\\nture. We also added tools to make the visualization of\\nthe results easier. We typically added an option to save\\nimages per epoch and create an animated image out of\\nit.\\nReferences\\n[1] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell,\\nand A. A. Efros, “Context encoders: Feature\\nlearning by inpainting,” 2016. [Online]. Available:\\nhttps://arxiv.org/abs/1604.07379\\n[2] A. Radford, L. Metz, and S. Chintala, “Unsupervised\\nrepresentation learning with deep convolutional\\ngenerative adversarial networks,” 2016. [Online].\\nAvailable: https://arxiv.org/abs/1511.06434\\n5')),\n",
       " 'answer': \"The project's results were not as good as the original paper's, possibly due to dataset differences.  Further improvements could involve exploring simpler datasets, adjusting parameters like learning rate, adding noise, and using pre-trained models.  The project provided a PyTorch Lightning implementation for easier understanding and experimentation.\\n\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
