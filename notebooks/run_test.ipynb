{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Notebook used to test our RAG chatbot package on the `rag-mini-bioasq` dataset\n",
    "\n",
    "## Setup and dataset download"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.doc_loader.test_data_csv_loader import TestDataCSVLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.model.embedding.minilm import MiniLM\n",
    "from nlp_chat_bot.model.llm.gemma import Gemma\n",
    "from nlp_chat_bot.model.llm.mistral import Mistral\n",
    "from nlp_chat_bot.rag.classic_rag import ClassicRAG\n",
    "from nlp_chat_bot.rag.query_translation_rag_decomposition import QueryTranslationRAGDecomposition\n",
    "from nlp_chat_bot.rag.query_translation_rag_fusion import QueryTranslationRAGFusion\n",
    "from nlp_chat_bot.vector_store.late_chunking_chroma_vector_store_builder import LateChunkingChromaVectorStoreBuilder\n",
    "from nlp_chat_bot.vector_store.naive_chunking_chroma_vector_store_builder import NaiveChunkingChromaVectorStoreBuilder\n",
    "from datasets import load_dataset, tqdm\n",
    "from nlp_chat_bot.model.embedding.late_chunking_embedding import LateChunkingEmbedding\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds_qa = load_dataset(\"enelpol/rag-mini-bioasq\", \"question-answer-passages\")[\"test\"]\n",
    "ds_corpus = load_dataset(\"enelpol/rag-mini-bioasq\", \"text-corpus\")[\"test\"]\n",
    "\n",
    "# only keep ds_qa rows with 3 items or more because we use 3 by default in our case\n",
    "ds_qa = ds_qa.filter(lambda x: len(x[\"relevant_passage_ids\"]) >= 3)\n",
    "\n",
    "ds_qa.to_csv(\"../test_datasets/rag-mini-bioasq/qa/qa.csv\")\n",
    "ds_corpus.to_csv(\"../test_datasets/rag-mini-bioasq/corpus/corpus.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Datasets sizes:\")\n",
    "print(f\"ds_qa: {len(ds_qa)}\")\n",
    "print(f\"ds_corpus: {len(ds_corpus)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(ds_qa.to_pandas().head(1))\n",
    "print(ds_corpus.to_pandas().head(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test if our Llama installation supports GPU\n",
    "\n",
    "# import os\n",
    "# from llama_cpp.llama_cpp import load_shared_library\n",
    "# import llama_cpp\n",
    "# \n",
    "# llama_root_path_module = os.path.dirname(llama_cpp.__file__)\n",
    "# import pathlib\n",
    "# \n",
    "# def is_gpu_available_v3() -> bool:\n",
    "# \n",
    "#     lib = load_shared_library('llama',pathlib.Path(llama_root_path_module+'/lib'))\n",
    "#     return bool(lib.llama_supports_gpu_offload())\n",
    "# \n",
    "# print(is_gpu_available_v3())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_retrieval(ds_qa, retrieve_function):\n",
    "    total_num_documents_considered = 0\n",
    "    num_valid_docs = 0\n",
    "    for test_item in tqdm(ds_qa):\n",
    "        question = test_item[\"question\"]\n",
    "        expected_documents_ids = test_item[\"relevant_passage_ids\"]\n",
    "        \n",
    "        response = retrieve_function(state = {\"question\": question, \"context\": []})\n",
    "        docs_retrieved = response[\"context\"]        \n",
    "        \n",
    "        # print(\"Question:\",question)\n",
    "        # print(\"Retrieved:\",docs_retrieved)\n",
    "        # print(\"#########################\\n\\n\")\n",
    "        # if it's a dict of docs (e.g. with QueryTranslationRAGDecomposition)\n",
    "        if isinstance(docs_retrieved, dict):\n",
    "            num_docs_retrieved = 0\n",
    "            for _, docs in docs_retrieved.items():\n",
    "                num_docs_retrieved += len(docs)\n",
    "                for doc in docs:\n",
    "                    if int(doc.metadata[\"id\"]) in expected_documents_ids:\n",
    "                        num_valid_docs += 1\n",
    "            total_num_documents_considered += min(len(expected_documents_ids), num_docs_retrieved)\n",
    "        else:\n",
    "            num_documents_considered = min(len(expected_documents_ids), len(docs_retrieved))\n",
    "            total_num_documents_considered += num_documents_considered\n",
    "            # print(\"Expected:\",expected_documents_ids,\"Got:\",[doc.metadata[\"id\"] for doc in docs_retrieved])\n",
    "            # print(\"Expected:\",expected_documents_ids)\n",
    "            for doc in docs_retrieved:\n",
    "                # print(\"Got:\",doc.metadata[\"id\"])\n",
    "                if int(doc.metadata[\"id\"]) in expected_documents_ids:\n",
    "                    num_valid_docs += 1\n",
    "        \n",
    "        \n",
    "    return num_valid_docs / total_num_documents_considered"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tests"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "scores = {}",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus_path = \"../test_datasets/rag-mini-bioasq/corpus\"\n",
    "vector_store_path = \"../test_chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "reload_vector_store = True # Add non existing documents\n",
    "reset_vector_store = False # Remove previous documents\n",
    "\n",
    "# splitter = None\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    ")\n",
    "\n",
    "llm = Gemma(model_download_path=model_download_path)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MiniLM, Naive Chunking"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Jina, Late Chunking\n",
    "\n",
    "Here we should note that we use Late Chunking on small documents, which is not the best use case for Late Chunking, so we can't really evaluate the performance of Late Chunking here."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = LateChunkingEmbedding(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = LateChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = ClassicRAG(vector_store, llm=llm)\n",
    "\n",
    "scores[\"Jina-LateChunking-Gemma\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"Jina-LateChunking-Gemma\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Decomposition\n",
    "\n",
    "The score is worse here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGDecomposition(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma-Decomposition\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma-Decomposition\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Fusion\n",
    "\n",
    "The score is really bad here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized). And contrarily to the previous case, we have even more calls to the LLM hence the error propagates"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGFusion(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Gemma-Fusion\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Gemma-Fusion\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## With Mistral Instruct instead of Gemma"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus_path = \"../test_datasets/rag-mini-bioasq/corpus\"\n",
    "vector_store_path = \"../test_chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "reload_vector_store = True # Add non existing documents\n",
    "reset_vector_store = False # Remove previous documents\n",
    "\n",
    "# splitter = None\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=0,  # chunk overlap (characters)\n",
    ")\n",
    "\n",
    "llm = Mistral(model_download_path=model_download_path) # reset your notebook if it crashes (it probably means you don't have enough memory)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Decomposition\n",
    "\n",
    "The score is worse here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGDecomposition(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Mistral-Decomposition\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Mistral-Decomposition\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MiniLM, Naive Chunking, Query Translation RAG Fusion\n",
    "\n",
    "The score is really bad here, it's likely because of the LLM used (Gemma), that is not big enough (and quantized). And contrarily to the previous case, we have even more calls to the LLM hence the error propagates"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_function = MiniLM(model_download_path=model_download_path)\n",
    "\n",
    "vector_store = NaiveChunkingChromaVectorStoreBuilder(corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "\n",
    "rag = QueryTranslationRAGFusion(vector_store, llm=llm)\n",
    "\n",
    "scores[\"MiniLM-NaiveChunking-Mistral-Fusion\"] = test_retrieval(ds_qa, rag.retrieve)\n",
    "print(scores[\"MiniLM-NaiveChunking-Mistral-Fusion\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## All results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_scores = pd.DataFrame(scores.items(), columns=[\"Method\", \"Score\"])\n",
    "print(df_scores)\n",
    "\n",
    "plt.bar(scores.keys(), scores.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
