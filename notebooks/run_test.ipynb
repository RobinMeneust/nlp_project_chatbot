{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052Z",
     "start_time": "2025-01-11T13:09:15.875143Z"
    }
   },
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.doc_loader.test_data_csv_loader import TestDataCSVLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from nlp_chat_bot.model.embedding.minilm import MiniLM\n",
    "from nlp_chat_bot.model.llm.gemma import Gemma\n",
    "from nlp_chat_bot.rag.classic_rag import ClassicRAG\n",
    "from nlp_chat_bot.rag.query_translation_rag_decomposition import QueryTranslationRAGDecomposition\n",
    "from nlp_chat_bot.rag.query_translation_rag_fusion import QueryTranslationRAGFusion\n",
    "from nlp_chat_bot.vector_store.naive_chunking_chroma_vector_store_builder import NaiveChunkingChromaVectorStoreBuilder\n",
    "from datasets import load_dataset, tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_google_genai\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChatGoogleGenerativeAI\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_text_splitters\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp_chat_bot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membedding\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mminilm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MiniLM\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp_chat_bot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllm\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgemma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Gemma\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp_chat_bot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassic_rag\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ClassicRAG\n",
      "File \u001B[1;32mC:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\model\\embedding\\minilm.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mMiniLM\u001B[39;00m:\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\sentence_transformers\\__init__.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     export_dynamic_quantized_onnx_model,\n\u001B[0;32m     11\u001B[0m     export_optimized_onnx_model,\n\u001B[0;32m     12\u001B[0m     export_static_quantized_openvino_model,\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_encoder\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCrossEncoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\sentence_transformers\\backend.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Callable, Literal\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m disable_datasets_caching, is_datasets_available\n\u001B[0;32m     13\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\sentence_transformers\\util.py:15\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mimportlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetadata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PackageNotFoundError, metadata\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any, Callable, Literal, overload\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\__init__.py:127\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _distributor_init\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__config__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m show_config\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    129\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mError importing numpy: you should not try to import numpy from\u001B[39m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001B[39m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124m    your python interpreter from there.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\__config__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This file is generated by numpy's build process\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# It contains system_info results at the time of building this package.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01menum\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Enum\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     __cpu_features__,\n\u001B[0;32m      6\u001B[0m     __cpu_baseline__,\n\u001B[0;32m      7\u001B[0m     __cpu_dispatch__,\n\u001B[0;32m      8\u001B[0m )\n\u001B[0;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshow_config\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     11\u001B[0m _built_with_meson \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\__init__.py:23\u001B[0m\n\u001B[0;32m     20\u001B[0m         env_added\u001B[38;5;241m.\u001B[39mappend(envkey)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multiarray\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\multiarray.py:10\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mCreate the numpy._core.multiarray namespace for backward compatibility.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mIn v1.16 the multiarray and umath c-extension modules were merged into\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m overrides\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _multiarray_umath\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\overrides.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_module\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inspect\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getargspec\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001B[0;32m     11\u001B[0m ARRAY_FUNCTIONS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m     13\u001B[0m array_function_like_doc \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     14\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"like : array_like, optional\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m        Reference object to allow the creation of arrays which are not\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03m        compatible with that passed in via this argument.\"\"\"\u001B[39;00m\n\u001B[0;32m     20\u001B[0m )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CPU dispatcher tracer already initlized"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-10T21:00:09.008891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_qa = load_dataset(\"enelpol/rag-mini-bioasq\", \"question-answer-passages\")[\"test\"]\n",
    "ds_corpus = load_dataset(\"enelpol/rag-mini-bioasq\", \"text-corpus\")[\"test\"]\n",
    "\n",
    "# only keep ds_qa rows with 3 items or more because we use 3 by default in our case\n",
    "ds_qa = ds_qa.filter(lambda x: len(x[\"relevant_passage_ids\"]) >= 3)\n",
    "\n",
    "ds_qa.to_csv(\"../test_datasets/rag-mini-bioasq/qa/qa.csv\")\n",
    "ds_corpus.to_csv(\"../test_datasets/rag-mini-bioasq/corpus/corpus.csv\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  6.75ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 41/41 [00:00<00:00, 50.39ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60209989"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-10T21:00:15.023926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Datasets sizes:\")\n",
    "print(f\"ds_qa: {len(ds_qa)}\")\n",
    "print(f\"ds_corpus: {len(ds_corpus)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets sizes:\n",
      "ds_qa: 497\n",
      "ds_corpus: 40181\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-10T21:00:15.050592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(ds_qa.to_pandas().head(1))\n",
    "print(ds_corpus.to_pandas().head(1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          question  \\\n",
      "0  Describe the mechanism of action of ibalizumab.   \n",
      "\n",
      "                                              answer    id  \\\n",
      "0  Ibalizumab is a humanized monoclonal antibody ...  2835   \n",
      "\n",
      "                                relevant_passage_ids  \n",
      "0  [29675744, 24853313, 29689540, 21289125, 20698...  \n",
      "                                             passage    id\n",
      "0  New data on viruses isolated from patients wit...  9797\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-10T21:00:15.213959Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-10T21:00:15.244135Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:09:21.897052800Z",
     "start_time": "2025-01-11T13:09:12.378188Z"
    }
   },
   "source": [
    "from nlp_chat_bot.model.embedding.late_chunking_embedding import LateChunkingEmbedding\n",
    "\n",
    "\n",
    "corpus_path = \"../test_datasets/rag-mini-bioasq/corpus\"\n",
    "vector_store_path = \"../test_chromadb\"\n",
    "model_download_path = \"../models\"\n",
    "reload_vector_store = True\n",
    "reset_vector_store = True\n",
    "\n",
    "test_params = {\n",
    "    \"splitter\": {\n",
    "        \"class\": RecursiveCharacterTextSplitter,\n",
    "        \"params\": {\n",
    "            \"chunk_size\": 1000,\n",
    "            \"chunk_overlap\": 50,\n",
    "            \"add_start_index\": True,\n",
    "        }\n",
    "    },\n",
    "    \"embedding_function\": {\n",
    "        \"class\": MiniLM\n",
    "    },\n",
    "    \"llm\": {\n",
    "        # \"class\": ChatGoogleGenerativeAI,\n",
    "        # \"params\": {\n",
    "        #     \"model\": \"gemini-1.5-flash\"\n",
    "        # }\n",
    "        \"class\": Gemma,\n",
    "        \"params\": {\n",
    "            \"model_download_path\": model_download_path\n",
    "        }\n",
    "    },\n",
    "    \"rag\": {\n",
    "        \"class\": QueryTranslationRAGFusion\n",
    "    },\n",
    "    \"vector_store_builder\": {\n",
    "        \"class\": LateChunkingEmbedding\n",
    "    }\n",
    "}\n",
    "splitter = None\n",
    "# splitter = test_params[\"splitter\"][\"class\"](\n",
    "#     chunk_size=test_params[\"splitter\"][\"params\"][\"chunk_size\"],  # chunk size (characters)\n",
    "#     chunk_overlap=test_params[\"splitter\"][\"params\"][\"chunk_overlap\"],  # chunk overlap (characters)\n",
    "#     add_start_index=test_params[\"splitter\"][\"params\"][\"add_start_index\"],  # track index in original document\n",
    "# )\n",
    "\n",
    "embedding_function = test_params[\"embedding_function\"][\"class\"](model_download_path=model_download_path)\n",
    "vector_store = test_params[\"vector_store_builder\"][\"class\"](corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(reload_vector_store, reset_vector_store)\n",
    "# vector_store = test_params[\"vector_store_builder\"][\"class\"](corpus_path, embedding_function, vector_store_path, splitter, document_loader=TestDataCSVLoader()).build(False, False)\n",
    "llm = model_name=test_params[\"llm\"][\"class\"](**(test_params[\"llm\"][\"params\"]))\n",
    "rag = test_params[\"rag\"][\"class\"](vector_store, llm=llm)"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp_chat_bot\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membedding\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlate_chunking_embedding\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LateChunkingEmbedding\n\u001B[0;32m      4\u001B[0m corpus_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../test_datasets/rag-mini-bioasq/corpus\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m vector_store_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../test_chromadb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mC:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\model\\embedding\\late_chunking_embedding.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgc\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_text_splitters\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharacterTextSplitter\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\__init__.py:127\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _distributor_init\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__config__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m show_config\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    129\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mError importing numpy: you should not try to import numpy from\u001B[39m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124m    its source directory; please exit the numpy source tree, and relaunch\u001B[39m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;124m    your python interpreter from there.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\__config__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This file is generated by numpy's build process\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# It contains system_info results at the time of building this package.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01menum\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Enum\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      5\u001B[0m     __cpu_features__,\n\u001B[0;32m      6\u001B[0m     __cpu_baseline__,\n\u001B[0;32m      7\u001B[0m     __cpu_dispatch__,\n\u001B[0;32m      8\u001B[0m )\n\u001B[0;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshow_config\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     11\u001B[0m _built_with_meson \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\__init__.py:23\u001B[0m\n\u001B[0;32m     20\u001B[0m         env_added\u001B[38;5;241m.\u001B[39mappend(envkey)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multiarray\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\multiarray.py:10\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mCreate the numpy._core.multiarray namespace for backward compatibility.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mIn v1.16 the multiarray and umath c-extension modules were merged into\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m overrides\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _multiarray_umath\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# noqa: F403\u001B[39;00m\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\numpy\\_core\\overrides.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_module\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inspect\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m getargspec\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_multiarray_umath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      8\u001B[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001B[0;32m     11\u001B[0m ARRAY_FUNCTIONS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m     13\u001B[0m array_function_like_doc \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     14\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"like : array_like, optional\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m        Reference object to allow the creation of arrays which are not\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03m        compatible with that passed in via this argument.\"\"\"\u001B[39;00m\n\u001B[0;32m     20\u001B[0m )\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1022\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:00:22.276328Z",
     "start_time": "2025-01-10T21:00:22.260774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test if our Llama installation supports GPU\n",
    "\n",
    "# import os\n",
    "# from llama_cpp.llama_cpp import load_shared_library\n",
    "# import llama_cpp\n",
    "# \n",
    "# llama_root_path_module = os.path.dirname(llama_cpp.__file__)\n",
    "# import pathlib\n",
    "# \n",
    "# def is_gpu_available_v3() -> bool:\n",
    "# \n",
    "#     lib = load_shared_library('llama',pathlib.Path(llama_root_path_module+'/lib'))\n",
    "#     return bool(lib.llama_supports_gpu_offload())\n",
    "# \n",
    "# print(is_gpu_available_v3())"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:00:22.308283Z",
     "start_time": "2025-01-10T21:00:22.292409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_retrieval(ds_qa, retrieve_function):\n",
    "    total_num_documents_considered = 0\n",
    "    num_valid_docs = 0\n",
    "    for test_item in tqdm(ds_qa):\n",
    "        question = test_item[\"question\"]\n",
    "        expected_documents_ids = test_item[\"relevant_passage_ids\"]\n",
    "        \n",
    "        response = retrieve_function(state = {\"question\": question, \"context\": []})\n",
    "        docs_retrieved = response[\"context\"]\n",
    "        num_documents_considered = max(len(expected_documents_ids), len(docs_retrieved))\n",
    "        total_num_documents_considered += num_documents_considered\n",
    "        \n",
    "        # print(\"Question:\",question)\n",
    "        \n",
    "        for doc in docs_retrieved:\n",
    "            # print(\"Got:\",doc)\n",
    "            if doc.metadata[\"id\"] in expected_documents_ids:\n",
    "                num_valid_docs += 1\n",
    "        \n",
    "        \n",
    "    return num_valid_docs / total_num_documents_considered"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T21:06:23.389352Z",
     "start_time": "2025-01-10T21:00:22.324023Z"
    }
   },
   "cell_type": "code",
   "source": "score = test_retrieval(ds_qa, rag.retrieve)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/497 [00:00<?, ?it/s]C:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\model\\llm\\gemma.py:25: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return AIMessage(self._llm(prompt.messages[0].content))\n",
      " 34%|███▎      | 167/497 [06:00<11:52,  2.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mtest_retrieval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds_qa\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrag\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 8\u001B[0m, in \u001B[0;36mtest_retrieval\u001B[1;34m(ds_qa, retrieve_function)\u001B[0m\n\u001B[0;32m      5\u001B[0m question \u001B[38;5;241m=\u001B[39m test_item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      6\u001B[0m expected_documents_ids \u001B[38;5;241m=\u001B[39m test_item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevant_passage_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 8\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mretrieve_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m docs_retrieved \u001B[38;5;241m=\u001B[39m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     10\u001B[0m num_documents_considered \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mlen\u001B[39m(expected_documents_ids), \u001B[38;5;28mlen\u001B[39m(docs_retrieved))\n",
      "File \u001B[1;32mC:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\rag\\query_translation_rag_fusion.py:11\u001B[0m, in \u001B[0;36mQueryTranslationRAGFusion.retrieve\u001B[1;34m(self, state, k)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mretrieve\u001B[39m(\u001B[38;5;28mself\u001B[39m, state: State, k: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m):\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 11\u001B[0m         docs_per_question \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retrieve_docs_multiple_questions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m         docs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m question, doc_list \u001B[38;5;129;01min\u001B[39;00m docs_per_question\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32mC:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\rag\\abstract_query_translation_rag.py:18\u001B[0m, in \u001B[0;36mAbstractQueryTranslationRAG._retrieve_docs_multiple_questions\u001B[1;34m(self, initial_question, k)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_retrieve_docs_multiple_questions\u001B[39m(\u001B[38;5;28mself\u001B[39m, initial_question, k):\n\u001B[0;32m     17\u001B[0m     questions_gen_prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prompt_perspectives\u001B[38;5;241m.\u001B[39minvoke({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: initial_question})\n\u001B[1;32m---> 18\u001B[0m     questions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestions_gen_prompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     docs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m question \u001B[38;5;129;01min\u001B[39;00m questions\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\CYTechNVME\\nlp_project_chatbot\\src\\nlp_chat_bot\\model\\llm\\gemma.py:25\u001B[0m, in \u001B[0;36mGemma.invoke\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\u001B[38;5;28mself\u001B[39m, prompt):\n\u001B[1;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m AIMessage(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    181\u001B[0m     emit_warning()\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1277\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[0;32m   1270\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1271\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1272\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1273\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1274\u001B[0m     )\n\u001B[0;32m   1275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1276\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m-> 1277\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m   1278\u001B[0m         [prompt],\n\u001B[0;32m   1279\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m   1280\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m   1281\u001B[0m         tags\u001B[38;5;241m=\u001B[39mtags,\n\u001B[0;32m   1282\u001B[0m         metadata\u001B[38;5;241m=\u001B[39mmetadata,\n\u001B[0;32m   1283\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1284\u001B[0m     )\n\u001B[0;32m   1285\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1286\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m   1287\u001B[0m )\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py:950\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    935\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    936\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    937\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    938\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    948\u001B[0m         )\n\u001B[0;32m    949\u001B[0m     ]\n\u001B[1;32m--> 950\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[0;32m    951\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    952\u001B[0m     )\n\u001B[0;32m    953\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[0;32m    954\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    790\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[0;32m    791\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 792\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    793\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    794\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py:779\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    769\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    770\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    771\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    775\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    776\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    777\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    778\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 779\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    780\u001B[0m                 prompts,\n\u001B[0;32m    781\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    782\u001B[0m                 \u001B[38;5;66;03m# TODO: support multiple run managers\u001B[39;00m\n\u001B[0;32m    783\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    784\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    785\u001B[0m             )\n\u001B[0;32m    786\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    787\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    788\u001B[0m         )\n\u001B[0;32m    789\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1502\u001B[0m, in \u001B[0;36mLLM._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1499\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1500\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[0;32m   1501\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m-> 1502\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1503\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m   1504\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1505\u001B[0m     )\n\u001B[0;32m   1506\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[0;32m   1507\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:286\u001B[0m, in \u001B[0;36mLlamaCpp._call\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming:\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;66;03m# If streaming is enabled, we use the stream\u001B[39;00m\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;66;03m# method that yields as they are generated\u001B[39;00m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;66;03m# and return the combined strings from the first choices's text:\u001B[39;00m\n\u001B[0;32m    285\u001B[0m     combined_text_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 286\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(\n\u001B[0;32m    287\u001B[0m         prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m    288\u001B[0m         stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    289\u001B[0m         run_manager\u001B[38;5;241m=\u001B[39mrun_manager,\n\u001B[0;32m    290\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    291\u001B[0m     ):\n\u001B[0;32m    292\u001B[0m         combined_text_output \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m chunk\u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m combined_text_output\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\langchain_community\\llms\\llamacpp.py:339\u001B[0m, in \u001B[0;36mLlamaCpp._stream\u001B[1;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    337\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_parameters(stop), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m    338\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient(prompt\u001B[38;5;241m=\u001B[39mprompt, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[1;32m--> 339\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m result:\n\u001B[0;32m    340\u001B[0m     logprobs \u001B[38;5;241m=\u001B[39m part[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    341\u001B[0m     chunk \u001B[38;5;241m=\u001B[39m GenerationChunk(\n\u001B[0;32m    342\u001B[0m         text\u001B[38;5;241m=\u001B[39mpart[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    343\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs},\n\u001B[0;32m    344\u001B[0m     )\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\llama_cpp\\llama.py:1317\u001B[0m, in \u001B[0;36mLlama._create_completion\u001B[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[0m\n\u001B[0;32m   1315\u001B[0m finish_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlength\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1316\u001B[0m multibyte_fix \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m-> 1317\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(\n\u001B[0;32m   1318\u001B[0m     prompt_tokens,\n\u001B[0;32m   1319\u001B[0m     top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[0;32m   1320\u001B[0m     top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[0;32m   1321\u001B[0m     min_p\u001B[38;5;241m=\u001B[39mmin_p,\n\u001B[0;32m   1322\u001B[0m     typical_p\u001B[38;5;241m=\u001B[39mtypical_p,\n\u001B[0;32m   1323\u001B[0m     temp\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[0;32m   1324\u001B[0m     tfs_z\u001B[38;5;241m=\u001B[39mtfs_z,\n\u001B[0;32m   1325\u001B[0m     mirostat_mode\u001B[38;5;241m=\u001B[39mmirostat_mode,\n\u001B[0;32m   1326\u001B[0m     mirostat_tau\u001B[38;5;241m=\u001B[39mmirostat_tau,\n\u001B[0;32m   1327\u001B[0m     mirostat_eta\u001B[38;5;241m=\u001B[39mmirostat_eta,\n\u001B[0;32m   1328\u001B[0m     frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[0;32m   1329\u001B[0m     presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[0;32m   1330\u001B[0m     repeat_penalty\u001B[38;5;241m=\u001B[39mrepeat_penalty,\n\u001B[0;32m   1331\u001B[0m     stopping_criteria\u001B[38;5;241m=\u001B[39mstopping_criteria,\n\u001B[0;32m   1332\u001B[0m     logits_processor\u001B[38;5;241m=\u001B[39mlogits_processor,\n\u001B[0;32m   1333\u001B[0m     grammar\u001B[38;5;241m=\u001B[39mgrammar,\n\u001B[0;32m   1334\u001B[0m ):\n\u001B[0;32m   1335\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m llama_cpp\u001B[38;5;241m.\u001B[39mllama_token_is_eog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mmodel, token):\n\u001B[0;32m   1336\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdetokenize(completion_tokens, prev_tokens\u001B[38;5;241m=\u001B[39mprompt_tokens)\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\llama_cpp\\llama.py:909\u001B[0m, in \u001B[0;36mLlama.generate\u001B[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001B[0m\n\u001B[0;32m    907\u001B[0m \u001B[38;5;66;03m# Eval and sample\u001B[39;00m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 909\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m sample_idx \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_tokens:\n\u001B[0;32m    911\u001B[0m         token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msample(\n\u001B[0;32m    912\u001B[0m             top_k\u001B[38;5;241m=\u001B[39mtop_k,\n\u001B[0;32m    913\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    927\u001B[0m             idx\u001B[38;5;241m=\u001B[39msample_idx,\n\u001B[0;32m    928\u001B[0m         )\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\llama_cpp\\llama.py:643\u001B[0m, in \u001B[0;36mLlama.eval\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m    639\u001B[0m n_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[0;32m    640\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch\u001B[38;5;241m.\u001B[39mset_batch(\n\u001B[0;32m    641\u001B[0m     batch\u001B[38;5;241m=\u001B[39mbatch, n_past\u001B[38;5;241m=\u001B[39mn_past, logits_all\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_params\u001B[38;5;241m.\u001B[39mlogits_all\n\u001B[0;32m    642\u001B[0m )\n\u001B[1;32m--> 643\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ids[n_past : n_past \u001B[38;5;241m+\u001B[39m n_tokens] \u001B[38;5;241m=\u001B[39m batch\n",
      "File \u001B[1;32md:\\Programs\\Anaconda\\envs\\nlp_project_chatbot\\lib\\site-packages\\llama_cpp\\_internals.py:300\u001B[0m, in \u001B[0;36mLlamaContext.decode\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: LlamaBatch):\n\u001B[1;32m--> 300\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m \u001B[43mllama_cpp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllama_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    303\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mllama_decode returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"RAG score: {score}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
